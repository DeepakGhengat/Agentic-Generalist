{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3442fbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM clients initialized\n",
      "‚úÖ Deep Search Advanced System initialized!\n",
      "üî¨ Deep Search Advanced System Status\n",
      "==================================================\n",
      "üìÅ Workspace: d:\\Workspace\\LLMs_projects\\agents\\DeepSearch\\workspace\n",
      "üíæ Data Dir: d:\\Workspace\\LLMs_projects\\agents\\DeepSearch\\workspace\\data\n",
      "üìä Results Dir: d:\\Workspace\\LLMs_projects\\agents\\DeepSearch\\workspace\\results\n",
      "ü§ñ LLM Status: Available\n",
      "üìÑ Research Files: 23\n",
      "  üîß unknown: 10\n",
      "  üîß explorer_only: 3\n",
      "  üîß combined_deep_search: 9\n",
      "  üîß combined: 1\n",
      "\n",
      "üöÄ Ready to launch!\n",
      "\n",
      "üí° Available Commands:\n",
      "  ‚Ä¢ await test_complete_system() - Test all functionality\n",
      "  ‚Ä¢ await run_research('your query') - Complete research\n",
      "  ‚Ä¢ display_research_summary() - Show file summary\n",
      "  ‚Ä¢ launch_gradio_interface() - Launch web interface\n",
      "  ‚Ä¢ demo.launch() - Quick launch Gradio\n",
      "\n",
      "‚úÖ Deep Search Advanced System Ready!\n",
      "üöÄ To start: Run launch_gradio_interface() or demo.launch()\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Deep Search Advanced System - Complete Single Code Implementation\n",
    "Multi-Engine Search ‚Ä¢ Agentic AI Research ‚Ä¢ Dual-Model Evaluation ‚Ä¢ Gradio Interface\n",
    "\"\"\"\n",
    "\n",
    "# ============================================\n",
    "# IMPORTS & INITIALIZATION\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import random\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "from html.parser import HTMLParser\n",
    "import concurrent.futures\n",
    "\n",
    "# Fix asyncio for Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# AI Agent imports\n",
    "from agents import Agent, WebSearchTool, trace, Runner, gen_trace_id, function_tool\n",
    "from agents.model_settings import ModelSettings\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LLM clients\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# UI imports\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Initialize environment\n",
    "load_dotenv(override=True)\n",
    "\n",
    "try:\n",
    "    openai_client = OpenAI()\n",
    "    claude_client = Anthropic()\n",
    "    LLM_AVAILABLE = True\n",
    "    print(\"‚úÖ LLM clients initialized\")\n",
    "except Exception as e:\n",
    "    LLM_AVAILABLE = False\n",
    "    print(f\"‚ö†Ô∏è LLM clients not available: {e}\")\n",
    "\n",
    "# Setup directories\n",
    "BASE_DIR = os.getcwd()\n",
    "WORKSPACE_DIR = os.path.join(BASE_DIR, \"workspace\")\n",
    "RESULTS_DIR = os.path.join(WORKSPACE_DIR, \"results\")\n",
    "DATA_DIR = os.path.join(WORKSPACE_DIR, \"data\")\n",
    "\n",
    "for dir_path in [WORKSPACE_DIR, RESULTS_DIR, DATA_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Deep Search Advanced System initialized!\")\n",
    "\n",
    "# ============================================\n",
    "# PYDANTIC MODELS\n",
    "# ============================================\n",
    "\n",
    "class WebSearchItem(BaseModel):\n",
    "    reason: str = Field(description=\"Reasoning for this search\")\n",
    "    query: str = Field(description=\"Search term to use\")\n",
    "\n",
    "class WebSearchPlan(BaseModel):\n",
    "    searches: List[WebSearchItem] = Field(description=\"List of strategic searches\")\n",
    "\n",
    "class ReportData(BaseModel):\n",
    "    short_summary: str = Field(description=\"Short summary\")\n",
    "    markdown_report: str = Field(description=\"Detailed report\")\n",
    "    follow_up_questions: List[str] = Field(description=\"Follow-up questions\")\n",
    "\n",
    "class ResearchSubquery(BaseModel):\n",
    "    subquery: str = Field(description=\"Specific research subquery\")\n",
    "    reasoning: str = Field(description=\"Why this subquery is needed\")\n",
    "    priority: int = Field(description=\"Priority level (1-5, 5 being highest)\")\n",
    "    search_type: str = Field(description=\"Type of search: web, academic, news, technical\")\n",
    "\n",
    "class ResearchPlan(BaseModel):\n",
    "    main_query: str = Field(description=\"Original research query\")\n",
    "    subqueries: List[ResearchSubquery] = Field(description=\"List of research subqueries\")\n",
    "    research_strategy: str = Field(description=\"Overall research approach\")\n",
    "    estimated_depth: int = Field(description=\"Expected research depth (1-5)\")\n",
    "\n",
    "class ResearchEvidence(BaseModel):\n",
    "    source_url: str = Field(description=\"Source URL\")\n",
    "    content_snippet: str = Field(description=\"Relevant content snippet\")\n",
    "    credibility_score: float = Field(description=\"Source credibility (0-1)\")\n",
    "    relevance_score: float = Field(description=\"Content relevance (0-1)\")\n",
    "    evidence_type: str = Field(description=\"Type: factual, opinion, analysis, data\")\n",
    "\n",
    "class ResearchFindings(BaseModel):\n",
    "    subquery: str = Field(description=\"The subquery this answers\")\n",
    "    key_findings: List[str] = Field(description=\"Main findings\")\n",
    "    evidence: List[ResearchEvidence] = Field(description=\"Supporting evidence\")\n",
    "    confidence_level: float = Field(description=\"Confidence in findings (0-1)\")\n",
    "    gaps_identified: List[str] = Field(description=\"Information gaps found\")\n",
    "\n",
    "class DeepResearchReport(BaseModel):\n",
    "    query: str = Field(description=\"Original research question\")\n",
    "    executive_summary: str = Field(description=\"High-level summary\")\n",
    "    detailed_analysis: str = Field(description=\"Comprehensive analysis\")\n",
    "    key_insights: List[str] = Field(description=\"Main insights discovered\")\n",
    "    evidence_strength: str = Field(description=\"Overall evidence quality\")\n",
    "    research_limitations: List[str] = Field(description=\"Limitations and gaps\")\n",
    "    follow_up_questions: List[str] = Field(description=\"Questions for further research\")\n",
    "    methodology_notes: str = Field(description=\"Research methodology used\")\n",
    "\n",
    "class EvaluationCriteria(BaseModel):\n",
    "    accuracy_score: float = Field(description=\"Factual correctness (0-10)\")\n",
    "    completeness_score: float = Field(description=\"Comprehensive coverage (0-10)\")\n",
    "    relevance_score: float = Field(description=\"Query relevance (0-10)\")\n",
    "    clarity_score: float = Field(description=\"Organization and clarity (0-10)\")\n",
    "    depth_score: float = Field(description=\"Insight and analysis depth (0-10)\")\n",
    "\n",
    "class DetailedEvaluation(BaseModel):\n",
    "    criteria_scores: EvaluationCriteria\n",
    "    overall_score: float\n",
    "    strengths: List[str]\n",
    "    weaknesses: List[str]\n",
    "    missing_aspects: List[str]\n",
    "    recommendations: List[str]\n",
    "    confidence_level: str\n",
    "\n",
    "class UniversalEvaluation(BaseModel):\n",
    "    query: str\n",
    "    content_type: str\n",
    "    gpt_evaluation: DetailedEvaluation\n",
    "    claude_evaluation: DetailedEvaluation\n",
    "    consensus_score: float\n",
    "    final_recommendations: List[str]\n",
    "    evaluation_summary: str\n",
    "\n",
    "# ============================================\n",
    "# EXPLORER SEARCH FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "class LinkExtractor(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.links = []\n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'a':\n",
    "            for attr_name, attr_value in attrs:\n",
    "                if attr_name == 'href' and attr_value and attr_value.startswith('http'):\n",
    "                    self.links.append(attr_value)\n",
    "\n",
    "def simple_html_parse(html_content, max_links=20):\n",
    "    parser = LinkExtractor()\n",
    "    try:\n",
    "        parser.feed(html_content)\n",
    "        return parser.links[:max_links]\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing HTML: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_duckduckgo_links(query, results=10):\n",
    "    endpoint = \"https://duckduckgo.com/html/\"\n",
    "    params = {\"q\": query}\n",
    "    url = endpoint + \"?\" + urllib.parse.urlencode(params)\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(url)\n",
    "        req.add_header('User-Agent', random.choice(USER_AGENTS))\n",
    "        \n",
    "        with urllib.request.urlopen(req, timeout=10) as response:\n",
    "            html_content = response.read().decode('utf-8')\n",
    "        \n",
    "        links = simple_html_parse(html_content, results)\n",
    "        clean_links = [link for link in links if 'duckduckgo.com' not in link]\n",
    "        return clean_links[:results]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching DuckDuckGo results: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_brave_links(query, results=10):\n",
    "    endpoint = \"https://search.brave.com/search\"\n",
    "    params = {\"q\": query}\n",
    "    url = endpoint + \"?\" + urllib.parse.urlencode(params)\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(url)\n",
    "        req.add_header('User-Agent', random.choice(USER_AGENTS))\n",
    "        req.add_header('Accept-Encoding', 'identity')\n",
    "        \n",
    "        with urllib.request.urlopen(req, timeout=10) as response:\n",
    "            html_content = response.read().decode('utf-8')\n",
    "        \n",
    "        links = simple_html_parse(html_content, results)\n",
    "        clean_links = [link for link in links if 'search.brave.com' not in link]\n",
    "        return clean_links[:results]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Brave results: {e}\")\n",
    "        return []\n",
    "\n",
    "def scrape_pages_text(urls, max_urls=5):\n",
    "    collected = []\n",
    "    \n",
    "    for url in urls[:max_urls]:\n",
    "        print(f\"Scraping: {url}\")\n",
    "        try:\n",
    "            req = urllib.request.Request(url)\n",
    "            req.add_header('User-Agent', random.choice(USER_AGENTS))\n",
    "            \n",
    "            with urllib.request.urlopen(req, timeout=15) as response:\n",
    "                html_content = response.read().decode('utf-8', errors='ignore')\n",
    "            \n",
    "            import re\n",
    "            text = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)\n",
    "            text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "            text = re.sub(r'<[^>]+>', ' ', text)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "            collected.append({\n",
    "                \"url\": url,\n",
    "                \"text\": text[:3000],\n",
    "                \"length\": len(text),\n",
    "                \"scraped_at\": datetime.now().isoformat()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            collected.append({\"url\": url, \"text\": f\"Error: {str(e)}\", \"error\": True})\n",
    "    \n",
    "    return collected\n",
    "\n",
    "# ============================================\n",
    "# AI AGENTS SETUP\n",
    "# ============================================\n",
    "\n",
    "# Research Planner Agent\n",
    "RESEARCH_PLANNER_INSTRUCTIONS = \"\"\"You are an expert research strategist. Given a research query, create a comprehensive research plan by:\n",
    "\n",
    "1. Breaking down the main query into 3-5 focused subqueries\n",
    "2. Prioritizing subqueries by importance \n",
    "3. Determining the best search strategy for each\n",
    "4. Estimating the research depth needed\n",
    "\n",
    "Consider different angles: factual, analytical, historical, future implications, expert opinions, and data-driven insights.\n",
    "Make your plan systematic and thorough.\"\"\"\n",
    "\n",
    "research_planner = Agent(\n",
    "    name=\"ResearchPlanner\",\n",
    "    instructions=RESEARCH_PLANNER_INSTRUCTIONS,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=ResearchPlan,\n",
    ")\n",
    "\n",
    "# Deep Search Agent\n",
    "DEEP_SEARCH_INSTRUCTIONS = \"\"\"You are a specialized deep research agent. For each subquery:\n",
    "\n",
    "1. Conduct thorough web searches\n",
    "2. Extract the most relevant and credible information\n",
    "3. Evaluate source credibility and content relevance  \n",
    "4. Identify key findings and supporting evidence\n",
    "5. Note any information gaps or limitations\n",
    "\n",
    "Focus on factual accuracy and provide specific, actionable findings. Always include source credibility assessment.\"\"\"\n",
    "\n",
    "deep_search_agent = Agent(\n",
    "    name=\"DeepSearchAgent\", \n",
    "    instructions=DEEP_SEARCH_INSTRUCTIONS,\n",
    "    tools=[WebSearchTool(search_context_size=\"high\")],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    model_settings=ModelSettings(tool_choice=\"required\"),\n",
    "    output_type=ResearchFindings,\n",
    ")\n",
    "\n",
    "# Research Synthesizer Agent\n",
    "SYNTHESIS_INSTRUCTIONS = \"\"\"You are a research synthesis expert. Combine multiple research findings into a comprehensive report:\n",
    "\n",
    "1. Create an executive summary of all findings\n",
    "2. Develop detailed analysis connecting all insights\n",
    "3. Identify the strongest evidence and key insights\n",
    "4. Note research limitations and gaps\n",
    "5. Suggest follow-up research questions\n",
    "6. Document the methodology used\n",
    "\n",
    "Ensure the report is structured, evidence-based, and provides clear value to the reader.\"\"\"\n",
    "\n",
    "synthesis_agent = Agent(\n",
    "    name=\"ResearchSynthesizer\",\n",
    "    instructions=SYNTHESIS_INSTRUCTIONS,\n",
    "    model=\"gpt-4o-mini\", \n",
    "    output_type=DeepResearchReport,\n",
    ")\n",
    "\n",
    "# Basic agents for compatibility\n",
    "search_agent = Agent(\n",
    "    name=\"SearchAgent\",\n",
    "    instructions=\"You are a research assistant. Search the web and provide a concise 200-word summary of key findings.\",\n",
    "    tools=[WebSearchTool(search_context_size=\"medium\")],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    model_settings=ModelSettings(tool_choice=\"required\"),\n",
    ")\n",
    "\n",
    "planner_agent = Agent(\n",
    "    name=\"PlannerAgent\",\n",
    "    instructions=\"You are a research planner. Create 3 strategic web searches to comprehensively answer the query.\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=WebSearchPlan,\n",
    ")\n",
    "\n",
    "writer_agent = Agent(\n",
    "    name=\"WriterAgent\",\n",
    "    instructions=\"You are a senior researcher. Create a comprehensive, well-structured markdown report (800+ words) synthesizing all research findings.\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=ReportData,\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# AGENTIC DEEP SEARCH\n",
    "# ============================================\n",
    "\n",
    "async def execute_agentic_deep_search(query: str) -> Dict[str, Any]:\n",
    "    print(f\"üî¨ Starting Agentic Deep Search for: {query}\")\n",
    "    \n",
    "    with trace(\"Agentic Deep Search Pipeline\"):\n",
    "        \n",
    "        # Phase 1: Research Planning\n",
    "        print(\"üìã Phase 1: Creating Research Plan...\")\n",
    "        planning_result = await Runner.run(research_planner, f\"Research Query: {query}\")\n",
    "        research_plan = planning_result.final_output\n",
    "        \n",
    "        print(f\"   üìä Generated {len(research_plan.subqueries)} subqueries\")\n",
    "        print(f\"   üéØ Research Strategy: {research_plan.research_strategy}\")\n",
    "        print(f\"   üìà Estimated Depth: {research_plan.estimated_depth}/5\")\n",
    "        \n",
    "        # Phase 2: Parallel Deep Search Execution\n",
    "        print(\"üîç Phase 2: Executing Deep Research...\")\n",
    "        search_tasks = []\n",
    "        \n",
    "        sorted_subqueries = sorted(research_plan.subqueries, key=lambda x: x.priority, reverse=True)\n",
    "        \n",
    "        for i, subquery in enumerate(sorted_subqueries):\n",
    "            print(f\"   üîé Subquery {i+1}: {subquery.subquery[:50]}...\")\n",
    "            search_input = f\"\"\"\n",
    "Subquery: {subquery.subquery}\n",
    "Reasoning: {subquery.reasoning}  \n",
    "Search Type: {subquery.search_type}\n",
    "Priority: {subquery.priority}/5\n",
    "\n",
    "Please conduct thorough research and provide detailed findings.\n",
    "\"\"\"\n",
    "            search_tasks.append(Runner.run(deep_search_agent, search_input))\n",
    "        \n",
    "        search_results = await asyncio.gather(*search_tasks)\n",
    "        research_findings = [result.final_output for result in search_results]\n",
    "        \n",
    "        print(f\"   ‚úÖ Completed {len(research_findings)} deep searches\")\n",
    "        \n",
    "        # Phase 3: Research Synthesis\n",
    "        print(\"üß† Phase 3: Synthesizing Research...\")\n",
    "        \n",
    "        synthesis_input = f\"\"\"\n",
    "Original Query: {query}\n",
    "Research Plan: {research_plan.research_strategy}\n",
    "\n",
    "Research Findings:\n",
    "\"\"\"\n",
    "        \n",
    "        for i, finding in enumerate(research_findings):\n",
    "            synthesis_input += f\"\"\"\n",
    "Subquery {i+1}: {finding.subquery}\n",
    "Key Findings: {'; '.join(finding.key_findings)}\n",
    "Confidence Level: {finding.confidence_level:.2f}\n",
    "Evidence Count: {len(finding.evidence)}\n",
    "Gaps Identified: {'; '.join(finding.gaps_identified)}\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        synthesis_result = await Runner.run(synthesis_agent, synthesis_input)\n",
    "        final_report = synthesis_result.final_output\n",
    "        \n",
    "        print(\"‚úÖ Agentic Deep Search Completed!\")\n",
    "        \n",
    "        results = {\n",
    "            \"query\": query,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"research_plan\": research_plan.dict(),\n",
    "            \"findings\": [finding.dict() for finding in research_findings],\n",
    "            \"final_report\": final_report.dict(),\n",
    "            \"methodology\": {\n",
    "                \"subqueries_executed\": len(research_findings),\n",
    "                \"total_evidence_pieces\": sum(len(f.evidence) for f in research_findings),\n",
    "                \"average_confidence\": sum(f.confidence_level for f in research_findings) / len(research_findings),\n",
    "                \"search_strategy\": research_plan.research_strategy\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Basic agentic search for compatibility\n",
    "async def perform_agentic_search(query: str):\n",
    "    with trace(\"Basic Agentic Search\"):\n",
    "        result = await Runner.run(planner_agent, f\"Query: {query}\")\n",
    "        search_plan = result.final_output\n",
    "        \n",
    "        search_tasks = []\n",
    "        for item in search_plan.searches:\n",
    "            input_text = f\"Search term: {item.query}\\nReason: {item.reason}\"\n",
    "            search_tasks.append(Runner.run(search_agent, input_text))\n",
    "        \n",
    "        search_results = await asyncio.gather(*search_tasks)\n",
    "        search_summaries = [result.final_output for result in search_results]\n",
    "        \n",
    "        input_text = f\"Original query: {query}\\nResearch findings: {search_summaries}\"\n",
    "        report_result = await Runner.run(writer_agent, input_text)\n",
    "        \n",
    "        return report_result.final_output, search_plan, search_summaries\n",
    "\n",
    "# ============================================\n",
    "# DUAL-MODEL EVALUATION\n",
    "# ============================================\n",
    "\n",
    "async def evaluate_with_gpt(query: str, research_report: str) -> DetailedEvaluation:\n",
    "    if not LLM_AVAILABLE:\n",
    "        return DetailedEvaluation(\n",
    "            criteria_scores=EvaluationCriteria(\n",
    "                accuracy_score=7.0, completeness_score=7.0, relevance_score=7.0,\n",
    "                clarity_score=7.0, depth_score=7.0\n",
    "            ),\n",
    "            overall_score=7.0,\n",
    "            strengths=[\"Mock evaluation - GPT-4 not available\"],\n",
    "            weaknesses=[\"Unable to evaluate - API not configured\"],\n",
    "            missing_aspects=[\"Full evaluation requires API access\"],\n",
    "            recommendations=[\"Configure OpenAI API key for real evaluation\"],\n",
    "            confidence_level=\"Low\"\n",
    "        )\n",
    "    \n",
    "    evaluation_prompt = f\"\"\"You are an expert research evaluator. Evaluate this research report based on the original query.\n",
    "\n",
    "Original Query: {query}\n",
    "\n",
    "Research Report:\n",
    "{research_report}\n",
    "\n",
    "Evaluate based on:\n",
    "1. Accuracy - Are facts correct and well-sourced?\n",
    "2. Completeness - Does it cover all important aspects?\n",
    "3. Relevance - Does it directly address the query?\n",
    "4. Clarity - Is it well-organized and easy to understand?\n",
    "5. Depth - Does it provide meaningful insights?\n",
    "\n",
    "Provide detailed evaluation with specific examples.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai_client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a critical research evaluator. Be thorough and specific.\"},\n",
    "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "            ],\n",
    "            response_format=DetailedEvaluation,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return response.choices[0].message.parsed\n",
    "    except Exception as e:\n",
    "        print(f\"GPT evaluation failed: {e}\")\n",
    "        return DetailedEvaluation(\n",
    "            criteria_scores=EvaluationCriteria(\n",
    "                accuracy_score=6.0, completeness_score=6.0, relevance_score=6.0,\n",
    "                clarity_score=6.0, depth_score=6.0\n",
    "            ),\n",
    "            overall_score=6.0,\n",
    "            strengths=[\"Evaluation failed - using fallback\"],\n",
    "            weaknesses=[f\"Error: {str(e)}\"],\n",
    "            missing_aspects=[\"Unable to complete evaluation\"],\n",
    "            recommendations=[\"Check API configuration\"],\n",
    "            confidence_level=\"Low\"\n",
    "        )\n",
    "\n",
    "async def evaluate_with_claude(query: str, research_report: str) -> DetailedEvaluation:\n",
    "    if not LLM_AVAILABLE:\n",
    "        return DetailedEvaluation(\n",
    "            criteria_scores=EvaluationCriteria(\n",
    "                accuracy_score=7.5, completeness_score=7.5, relevance_score=7.5,\n",
    "                clarity_score=7.5, depth_score=7.5\n",
    "            ),\n",
    "            overall_score=7.5,\n",
    "            strengths=[\"Mock evaluation - Claude not available\"],\n",
    "            weaknesses=[\"Unable to evaluate - API not configured\"],\n",
    "            missing_aspects=[\"Full evaluation requires API access\"],\n",
    "            recommendations=[\"Configure Anthropic API key for real evaluation\"],\n",
    "            confidence_level=\"Low\"\n",
    "        )\n",
    "    \n",
    "    evaluation_prompt = f\"\"\"You are an expert research evaluator. Evaluate this research report based on the original query.\n",
    "\n",
    "Original Query: {query}\n",
    "\n",
    "Research Report:\n",
    "{research_report}\n",
    "\n",
    "Evaluate based on:\n",
    "1. Accuracy - Are facts correct and well-sourced?\n",
    "2. Completeness - Does it cover all important aspects?\n",
    "3. Relevance - Does it directly address the query?\n",
    "4. Clarity - Is it well-organized and easy to understand?\n",
    "5. Depth - Does it provide meaningful insights?\n",
    "\n",
    "Return your evaluation in this exact JSON format:\n",
    "{{\n",
    "    \"criteria_scores\": {{\n",
    "        \"accuracy_score\": <0-10>,\n",
    "        \"completeness_score\": <0-10>,\n",
    "        \"relevance_score\": <0-10>,\n",
    "        \"clarity_score\": <0-10>,\n",
    "        \"depth_score\": <0-10>\n",
    "    }},\n",
    "    \"overall_score\": <0-10>,\n",
    "    \"strengths\": [\"strength1\", \"strength2\", ...],\n",
    "    \"weaknesses\": [\"weakness1\", \"weakness2\", ...],\n",
    "    \"missing_aspects\": [\"aspect1\", \"aspect2\", ...],\n",
    "    \"recommendations\": [\"recommendation1\", \"recommendation2\", ...],\n",
    "    \"confidence_level\": \"High/Medium/Low\"\n",
    "}}\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = claude_client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=2000,\n",
    "            temperature=0.3,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        evaluation_json = json.loads(response.content[0].text)\n",
    "        return DetailedEvaluation(**evaluation_json)\n",
    "    except Exception as e:\n",
    "        print(f\"Claude evaluation failed: {e}\")\n",
    "        return DetailedEvaluation(\n",
    "            criteria_scores=EvaluationCriteria(\n",
    "                accuracy_score=6.5, completeness_score=6.5, relevance_score=6.5,\n",
    "                clarity_score=6.5, depth_score=6.5\n",
    "            ),\n",
    "            overall_score=6.5,\n",
    "            strengths=[\"Evaluation failed - using fallback\"],\n",
    "            weaknesses=[f\"Error: {str(e)}\"],\n",
    "            missing_aspects=[\"Unable to complete evaluation\"],\n",
    "            recommendations=[\"Check API configuration\"],\n",
    "            confidence_level=\"Low\"\n",
    "        )\n",
    "\n",
    "def calculate_consensus(gpt_eval: DetailedEvaluation, claude_eval: DetailedEvaluation) -> Dict:\n",
    "    gpt_scores = gpt_eval.criteria_scores.model_dump()\n",
    "    claude_scores = claude_eval.criteria_scores.model_dump()\n",
    "    \n",
    "    score_differences = {}\n",
    "    for criterion, gpt_score in gpt_scores.items():\n",
    "        claude_score = claude_scores[criterion]\n",
    "        score_differences[criterion] = abs(gpt_score - claude_score)\n",
    "    \n",
    "    avg_difference = sum(score_differences.values()) / len(score_differences)\n",
    "    consensus_score = 10 - avg_difference\n",
    "    \n",
    "    divergence_areas = []\n",
    "    for criterion, diff in score_differences.items():\n",
    "        if diff > 2:\n",
    "            divergence_areas.append(\n",
    "                f\"{criterion}: GPT={gpt_scores[criterion]:.1f}, Claude={claude_scores[criterion]:.1f}\"\n",
    "            )\n",
    "    \n",
    "    all_recommendations = list(set(gpt_eval.recommendations + claude_eval.recommendations))\n",
    "    \n",
    "    return {\n",
    "        \"consensus_score\": round(consensus_score, 2),\n",
    "        \"divergence_areas\": divergence_areas,\n",
    "        \"final_recommendations\": all_recommendations,\n",
    "        \"score_differences\": score_differences\n",
    "    }\n",
    "\n",
    "async def finalize_research_evaluation(query: str, research_report: str) -> UniversalEvaluation:\n",
    "    print(\"ü§ñ Starting GPT-4 evaluation...\")\n",
    "    gpt_eval = await evaluate_with_gpt(query, research_report)\n",
    "    \n",
    "    print(\"üß† Starting Claude evaluation...\")\n",
    "    claude_eval = await evaluate_with_claude(query, research_report)\n",
    "    \n",
    "    print(\"üîÑ Building consensus...\")\n",
    "    consensus = calculate_consensus(gpt_eval, claude_eval)\n",
    "    \n",
    "    summary = f\"\"\"## Dual-Model Evaluation Summary\n",
    "\n",
    "**Query**: {query}\n",
    "\n",
    "### Overall Scores:\n",
    "- GPT-4 Overall Score: {gpt_eval.overall_score}/10\n",
    "- Claude Overall Score: {claude_eval.overall_score}/10\n",
    "- Consensus Score: {consensus['consensus_score']}/10\n",
    "\n",
    "### Key Findings:\n",
    "- **Areas of Agreement**: {5 - len(consensus['divergence_areas'])}/5 criteria\n",
    "- **Areas of Disagreement**: {len(consensus['divergence_areas'])} criteria with significant divergence\n",
    "\n",
    "### Confidence Levels:\n",
    "- GPT-4: {gpt_eval.confidence_level}\n",
    "- Claude: {claude_eval.confidence_level}\n",
    "\"\"\"\n",
    "    \n",
    "    return UniversalEvaluation(\n",
    "        query=query,\n",
    "        content_type=\"research_report\",\n",
    "        gpt_evaluation=gpt_eval,\n",
    "        claude_evaluation=claude_eval,\n",
    "        consensus_score=consensus['consensus_score'],\n",
    "        final_recommendations=consensus['final_recommendations'],\n",
    "        evaluation_summary=summary\n",
    "    )\n",
    "\n",
    "# ============================================\n",
    "# COMBINED SEARCH ORCHESTRATOR\n",
    "# ============================================\n",
    "\n",
    "async def execute_combined_deep_search(query: str, include_explorer=True, include_agentic=True, include_evaluation=True) -> tuple:\n",
    "    print(f\"üöÄ Starting Combined Deep Search: {query}\")\n",
    "    \n",
    "    results = {\n",
    "        \"query\": query,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"search_methods\": [],\n",
    "        \"type\": \"combined_deep_search\"\n",
    "    }\n",
    "    \n",
    "    # Explorer Search\n",
    "    if include_explorer:\n",
    "        print(\"üîç Phase 1: Explorer Search...\")\n",
    "        results[\"search_methods\"].append(\"explorer\")\n",
    "        \n",
    "        dd_urls = fetch_duckduckgo_links(query, 8)\n",
    "        brave_urls = fetch_brave_links(query, 8)\n",
    "        all_urls = list(set(dd_urls + brave_urls))\n",
    "        \n",
    "        print(f\"   üìä Found {len(dd_urls)} DuckDuckGo + {len(brave_urls)} Brave URLs\")\n",
    "        \n",
    "        scraped_data = scrape_pages_text(all_urls[:6])\n",
    "        \n",
    "        results[\"explorer_results\"] = []\n",
    "        for item in scraped_data:\n",
    "            item[\"type\"] = \"scraped_content\"\n",
    "            results[\"explorer_results\"].append(item)\n",
    "        \n",
    "        print(f\"   ‚úÖ Scraped {len(scraped_data)} pages\")\n",
    "    \n",
    "    # Agentic Deep Search\n",
    "    if include_agentic:\n",
    "        print(\"üß† Phase 2: Agentic Deep Search...\")\n",
    "        results[\"search_methods\"].append(\"agentic\")\n",
    "        \n",
    "        agentic_results = await execute_agentic_deep_search(query)\n",
    "        results[\"agentic_results\"] = agentic_results\n",
    "        \n",
    "        print(f\"   ‚úÖ Completed agentic research with {agentic_results['methodology']['subqueries_executed']} subqueries\")\n",
    "    \n",
    "    # Dual-Model Evaluation\n",
    "    if include_evaluation and include_agentic and \"agentic_results\" in results:\n",
    "        print(\"üìä Phase 3: Dual-Model Evaluation...\")\n",
    "        \n",
    "        report_text = results[\"agentic_results\"][\"final_report\"][\"detailed_analysis\"]\n",
    "        evaluation = await finalize_research_evaluation(query, report_text)\n",
    "        \n",
    "        results[\"evaluation\"] = evaluation.dict()\n",
    "        results[\"consensus_score\"] = evaluation.consensus_score\n",
    "        results[\"quality_approved\"] = evaluation.consensus_score >= 7.5\n",
    "        \n",
    "        print(f\"   üìà Consensus Score: {evaluation.consensus_score}/10\")\n",
    "        print(f\"   ‚úÖ Quality: {'APPROVED' if results['quality_approved'] else 'NEEDS IMPROVEMENT'}\")\n",
    "    \n",
    "    # Create summary\n",
    "    summary_parts = []\n",
    "    if include_explorer:\n",
    "        explorer_count = len(results.get('explorer_results', []))\n",
    "        summary_parts.append(f\"Explorer: {explorer_count} sources\")\n",
    "    \n",
    "    if include_agentic:\n",
    "        methodology = results.get('agentic_results', {}).get('methodology', {})\n",
    "        subqueries = methodology.get('subqueries_executed', 0)\n",
    "        evidence = methodology.get('total_evidence_pieces', 0)\n",
    "        summary_parts.append(f\"Agentic: {subqueries} subqueries, {evidence} evidence pieces\")\n",
    "    \n",
    "    if include_evaluation:\n",
    "        score = results.get('consensus_score', 0)\n",
    "        summary_parts.append(f\"Evaluation: {score}/10\")\n",
    "    \n",
    "    results[\"combined_summary\"] = \" | \".join(summary_parts)\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f\"combined_deep_search_{timestamp}.json\"\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Complete results saved to: {filename}\")\n",
    "    \n",
    "    return results, filepath\n",
    "\n",
    "# Main research function\n",
    "async def run_research(query: str):\n",
    "    results, filepath = await execute_combined_deep_search(query, True, True, True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üìä RESEARCH SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"üéØ Query: {query}\")\n",
    "    print(f\"üîß Methods: {', '.join(results['search_methods'])}\")\n",
    "    \n",
    "    if 'evaluation' in results:\n",
    "        eval_data = results['evaluation']\n",
    "        print(f\"üìà GPT-4 Score: {eval_data['gpt_evaluation']['overall_score']}/10\")\n",
    "        print(f\"üß† Claude Score: {eval_data['claude_evaluation']['overall_score']}/10\")\n",
    "        print(f\"üéØ Consensus: {results['consensus_score']}/10\")\n",
    "        print(f\"‚úÖ Quality: {'APPROVED' if results['quality_approved'] else 'NEEDS IMPROVEMENT'}\")\n",
    "    \n",
    "    print(f\"üíæ Saved to: {filepath}\")\n",
    "    \n",
    "    return {\n",
    "        \"report\": type('obj', (object,), {\n",
    "            \"short_summary\": results[\"agentic_results\"][\"final_report\"][\"executive_summary\"],\n",
    "            \"markdown_report\": results[\"agentic_results\"][\"final_report\"][\"detailed_analysis\"],\n",
    "            \"follow_up_questions\": results[\"agentic_results\"][\"final_report\"][\"follow_up_questions\"]\n",
    "        })(),\n",
    "        \"evaluation\": type('obj', (object,), {\n",
    "            \"gpt_evaluation\": type('obj', (object,), {\n",
    "                \"overall_score\": results[\"evaluation\"][\"gpt_evaluation\"][\"overall_score\"],\n",
    "                \"criteria_scores\": type('obj', (object,), results[\"evaluation\"][\"gpt_evaluation\"][\"criteria_scores\"])()\n",
    "            })(),\n",
    "            \"claude_evaluation\": type('obj', (object,), {\n",
    "                \"overall_score\": results[\"evaluation\"][\"claude_evaluation\"][\"overall_score\"],\n",
    "                \"criteria_scores\": type('obj', (object,), results[\"evaluation\"][\"claude_evaluation\"][\"criteria_scores\"])()\n",
    "            })(),\n",
    "            \"consensus_score\": results[\"consensus_score\"],\n",
    "            \"final_recommendations\": results[\"evaluation\"][\"final_recommendations\"]\n",
    "        })(),\n",
    "        \"quality_approved\": results[\"quality_approved\"],\n",
    "        \"filepath\": filepath\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# FILE MANAGEMENT UTILITIES\n",
    "# ============================================\n",
    "\n",
    "def list_available_json_files():\n",
    "    files = []\n",
    "    if os.path.exists(DATA_DIR):\n",
    "        for filename in os.listdir(DATA_DIR):\n",
    "            if filename.endswith('.json'):\n",
    "                filepath = os.path.join(DATA_DIR, filename)\n",
    "                try:\n",
    "                    with open(filepath, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                    \n",
    "                    stat = os.stat(filepath)\n",
    "                    \n",
    "                    files.append({\n",
    "                        'filename': filename,\n",
    "                        'filepath': filepath,\n",
    "                        'query': data.get('query', 'Unknown'),\n",
    "                        'type': data.get('type', 'unknown'),\n",
    "                        'timestamp': data.get('timestamp', 'Unknown'),\n",
    "                        'size': stat.st_size,\n",
    "                        'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),\n",
    "                        'has_evaluation': 'evaluation' in data or 'consensus_score' in data\n",
    "                    })\n",
    "                except:\n",
    "                    files.append({\n",
    "                        'filename': filename,\n",
    "                        'filepath': filepath,\n",
    "                        'query': 'Error reading file',\n",
    "                        'type': 'error',\n",
    "                        'timestamp': 'Unknown',\n",
    "                        'size': os.path.getsize(filepath),\n",
    "                        'modified': datetime.fromtimestamp(os.path.getmtime(filepath)).isoformat(),\n",
    "                        'has_evaluation': False\n",
    "                    })\n",
    "    \n",
    "    files.sort(key=lambda x: x['modified'], reverse=True)\n",
    "    return files\n",
    "\n",
    "def display_research_summary():\n",
    "    files = list_available_json_files()\n",
    "    \n",
    "    if not files:\n",
    "        print(\"üìÇ No research files found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìä Research Files Summary ({len(files)} total)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    by_type = {}\n",
    "    total_size = 0\n",
    "    \n",
    "    for file_info in files:\n",
    "        file_type = file_info['type']\n",
    "        if file_type not in by_type:\n",
    "            by_type[file_type] = []\n",
    "        by_type[file_type].append(file_info)\n",
    "        total_size += file_info['size']\n",
    "    \n",
    "    for file_type, type_files in by_type.items():\n",
    "        print(f\"üîß {file_type}: {len(type_files)} files\")\n",
    "        for file_info in type_files[:3]:\n",
    "            status = \"üìä Evaluated\" if file_info['has_evaluation'] else \"üìÑ Raw\"\n",
    "            print(f\"   {status} {file_info['filename']}: {file_info['query'][:40]}...\")\n",
    "        if len(type_files) > 3:\n",
    "            print(f\"   ... and {len(type_files) - 3} more\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"üíæ Total storage: {total_size/1024:.1f} KB\")\n",
    "\n",
    "# ============================================\n",
    "# GRADIO INTERFACE\n",
    "# ============================================\n",
    "\n",
    "def run_explorer_search_interface(query: str):\n",
    "    if not query.strip():\n",
    "        return \"‚ùå Please enter a search query\", \"\", \"\", \"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîç Starting Explorer Search for: {query}\")\n",
    "        all_data = []\n",
    "        \n",
    "        print(\"ü¶Ü Fetching DuckDuckGo results...\")\n",
    "        dd_urls = fetch_duckduckgo_links(query, 5)\n",
    "        if dd_urls:\n",
    "            dd_scraped = scrape_pages_text(dd_urls[:3])\n",
    "            for entry in dd_scraped:\n",
    "                entry[\"source\"] = \"duckduckgo\"\n",
    "                entry[\"type\"] = \"scraped_content\"\n",
    "                all_data.append(entry)\n",
    "        \n",
    "        print(\"üõ°Ô∏è Fetching Brave results...\")\n",
    "        brave_urls = fetch_brave_links(query, 5)\n",
    "        if brave_urls:\n",
    "            brave_scraped = scrape_pages_text(brave_urls[:3])\n",
    "            for entry in brave_scraped:\n",
    "                entry[\"source\"] = \"brave\"\n",
    "                entry[\"type\"] = \"scraped_content\"\n",
    "                all_data.append(entry)\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"explorer_{timestamp}.json\"\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                \"query\": query,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"type\": \"explorer_only\",\n",
    "                \"results\": all_data\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        progress = f\"‚úÖ Explorer search completed!\\nüìä Found {len(all_data)} results\\nüíæ Saved: {filename}\"\n",
    "        summary = f\"DuckDuckGo: {len(dd_urls)} URLs | Brave: {len(brave_urls)} URLs | Scraped: {len(all_data)} pages\"\n",
    "        \n",
    "        report = \"# üîç Explorer Search Results\\n\\n\"\n",
    "        for i, item in enumerate(all_data[:5], 1):\n",
    "            if not item.get(\"error\"):\n",
    "                report += f\"## Source {i}: {item['source'].title()}\\n\"\n",
    "                report += f\"**URL**: {item.get('url', 'Unknown')}\\n\\n\"\n",
    "                report += f\"{item.get('text', 'No content')[:500]}...\\n\\n---\\n\\n\"\n",
    "        \n",
    "        return progress, summary, report, filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Explorer search failed: {str(e)}\"\n",
    "        return error_msg, \"\", \"\", \"\"\n",
    "\n",
    "def run_complete_research_interface(query: str):\n",
    "    if not query.strip():\n",
    "        return \"‚ùå Please enter a search query\", \"\", \"\", \"\"\n",
    "    \n",
    "    try:\n",
    "        async def async_research():\n",
    "            return await run_research(query)\n",
    "        \n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                future = executor.submit(asyncio.run, async_research())\n",
    "                results = future.result(timeout=300)\n",
    "        else:\n",
    "            results = asyncio.run(async_research())\n",
    "        \n",
    "        if results:\n",
    "            progress = f\"\"\"‚úÖ Complete research finished!\n",
    "üîç Query: {query}\n",
    "üìä GPT-4 Score: {results['evaluation'].gpt_evaluation.overall_score}/10\n",
    "üß† Claude Score: {results['evaluation'].claude_evaluation.overall_score}/10\n",
    "üéØ Consensus: {results['evaluation'].consensus_score}/10\n",
    "‚úÖ Quality: {'APPROVED' if results['quality_approved'] else 'NEEDS IMPROVEMENT'}\n",
    "üíæ Saved: {os.path.basename(results['filepath'])}\"\"\"\n",
    "            \n",
    "            summary = results['report'].short_summary\n",
    "            \n",
    "            report = f\"\"\"# üî¨ Complete Research Report\n",
    "\n",
    "## üìñ Research Findings\n",
    "{results['report'].markdown_report}\n",
    "\n",
    "## üìä Dual-Model Evaluation Results\n",
    "\n",
    "### Overall Scores\n",
    "- **GPT-4 Evaluation**: {results['evaluation'].gpt_evaluation.overall_score}/10\n",
    "- **Claude Evaluation**: {results['evaluation'].claude_evaluation.overall_score}/10  \n",
    "- **Consensus Score**: {results['evaluation'].consensus_score}/10\n",
    "\n",
    "### Quality Assessment\n",
    "**Status**: {'üü¢ APPROVED' if results['quality_approved'] else 'üî¥ NEEDS IMPROVEMENT'}\n",
    "\n",
    "### Key Recommendations\n",
    "{chr(10).join(f\"‚Ä¢ {rec}\" for rec in results['evaluation'].final_recommendations[:5])}\n",
    "\n",
    "### Follow-up Questions\n",
    "{chr(10).join(f\"‚Ä¢ {q}\" for q in results['report'].follow_up_questions)}\n",
    "\"\"\"\n",
    "            \n",
    "            return progress, summary, report, results['filepath']\n",
    "        else:\n",
    "            return \"‚ùå Research failed - no results returned\", \"\", \"\", \"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Research failed: {str(e)}\\n\\nüí° Make sure all requirements are installed\"\n",
    "        return error_msg, \"\", \"\", \"\"\n",
    "\n",
    "def get_available_files():\n",
    "    try:\n",
    "        files = list_available_json_files()\n",
    "        if not files:\n",
    "            return [(\"No files found\", \"\")]\n",
    "        \n",
    "        options = []\n",
    "        for file_info in files:\n",
    "            query_preview = file_info['query'][:35]\n",
    "            if len(file_info['query']) > 35:\n",
    "                query_preview += '...'\n",
    "            \n",
    "            file_type = file_info['type']\n",
    "            if file_info['has_evaluation']:\n",
    "                file_type += \" (evaluated)\"\n",
    "            \n",
    "            display_name = f\"üìÑ {file_info['filename']} | {query_preview} | {file_type}\"\n",
    "            options.append((display_name, file_info['filepath']))\n",
    "        \n",
    "        return options\n",
    "    except Exception as e:\n",
    "        return [(f\"Error: {str(e)}\", \"\")]\n",
    "\n",
    "def format_file_table():\n",
    "    try:\n",
    "        files = list_available_json_files()\n",
    "        if not files:\n",
    "            return \"<p>üìÇ No research files found</p>\"\n",
    "        \n",
    "        table_data = []\n",
    "        for f in files[:10]:\n",
    "            size_kb = f['size'] / 1024\n",
    "            modified_date = f['modified'][:16].replace('T', ' ')\n",
    "            query_short = (f['query'][:30] + \"...\") if len(f['query']) > 30 else f['query']\n",
    "            \n",
    "            file_type = f['type']\n",
    "            if f['has_evaluation']:\n",
    "                file_type += \" ‚úì\"\n",
    "            \n",
    "            table_data.append([\n",
    "                f\"üìÑ {f['filename']}\",\n",
    "                query_short,\n",
    "                f\"üîß {file_type}\",\n",
    "                f\"üíæ {size_kb:.1f} KB\",\n",
    "                f\"üìÖ {modified_date}\"\n",
    "            ])\n",
    "        \n",
    "        df = pd.DataFrame(table_data, columns=[\n",
    "            \"File\", \"Query\", \"Type\", \"Size\", \"Modified\"\n",
    "        ])\n",
    "        \n",
    "        html_table = df.to_html(\n",
    "            index=False, \n",
    "            classes=\"table table-striped table-hover\",\n",
    "            escape=False\n",
    "        )\n",
    "        \n",
    "        return f\"\"\"\n",
    "        <div style=\"max-height: 400px; overflow-y: auto; border: 1px solid #ddd; border-radius: 5px; padding: 10px;\">\n",
    "            {html_table}\n",
    "        </div>\n",
    "        <p style=\"text-align: center; color: #666; margin-top: 10px;\">\n",
    "            üìä Latest 10 files | Total: {len(files)} files\n",
    "        </p>\n",
    "        \"\"\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"<p>‚ùå Error loading file table: {str(e)}</p>\"\n",
    "\n",
    "# Custom CSS\n",
    "custom_css = \"\"\"\n",
    ".gradio-container {\n",
    "    max-width: 1400px !important;\n",
    "    margin: auto;\n",
    "}\n",
    ".tab-nav button {\n",
    "    font-size: 14px !important;\n",
    "    padding: 10px 16px !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"Deep Search Advanced\", theme=gr.themes.Soft(), css=custom_css) as demo:\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    # üî¨ Deep Search Advanced System\n",
    "    **Multi-Engine Search ‚Ä¢ Agentic AI Research ‚Ä¢ Dual-Model Evaluation**\n",
    "    \n",
    "    *Complete research solution with Explorer scraping, AI agents, and quality assessment*\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        \n",
    "        # Tab 1: Complete Research\n",
    "        with gr.TabItem(\"üöÄ Complete Research\"):\n",
    "            gr.Markdown(\"### Full Research Pipeline with Dual-Model Evaluation\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    research_query = gr.Textbox(\n",
    "                        label=\"üî¨ Research Query\", \n",
    "                        placeholder=\"Enter your research question for complete analysis...\",\n",
    "                        lines=3\n",
    "                    )\n",
    "                    research_btn = gr.Button(\"üöÄ Start Complete Research\", variant=\"primary\", size=\"lg\")\n",
    "                \n",
    "                with gr.Column(scale=3):\n",
    "                    research_progress = gr.Textbox(label=\"üìã Progress & Results\", lines=8, interactive=False)\n",
    "            \n",
    "            with gr.Row():\n",
    "                research_summary = gr.Textbox(label=\"üìä Executive Summary\", lines=4, interactive=False)\n",
    "                research_file = gr.Textbox(label=\"üíæ Saved File\", lines=2, interactive=False)\n",
    "            \n",
    "            research_report = gr.Textbox(label=\"üìñ Complete Research Report\", lines=20, max_lines=25, interactive=False)\n",
    "            \n",
    "            research_btn.click(\n",
    "                run_complete_research_interface,\n",
    "                inputs=research_query,\n",
    "                outputs=[research_progress, research_summary, research_report, research_file]\n",
    "            )\n",
    "        \n",
    "        # Tab 2: Explorer Search\n",
    "        with gr.TabItem(\"üîç Explorer Search\"):\n",
    "            gr.Markdown(\"### Multi-Engine Web Scraping (DuckDuckGo + Brave)\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    explorer_query = gr.Textbox(\n",
    "                        label=\"üîç Search Query\", \n",
    "                        placeholder=\"Enter search query for web scraping...\",\n",
    "                        lines=2\n",
    "                    )\n",
    "                    explorer_btn = gr.Button(\"üîç Start Explorer Search\", variant=\"primary\", size=\"lg\")\n",
    "                \n",
    "                with gr.Column(scale=3):\n",
    "                    explorer_progress = gr.Textbox(label=\"üìã Progress Log\", lines=6, interactive=False)\n",
    "            \n",
    "            with gr.Row():\n",
    "                explorer_summary = gr.Textbox(label=\"üìä Summary\", lines=4, interactive=False)\n",
    "                explorer_file = gr.Textbox(label=\"üíæ Saved File\", lines=2, interactive=False)\n",
    "            \n",
    "            explorer_report = gr.Textbox(label=\"üìñ Search Results\", lines=15, interactive=False)\n",
    "            \n",
    "            explorer_btn.click(\n",
    "                run_explorer_search_interface,\n",
    "                inputs=explorer_query,\n",
    "                outputs=[explorer_progress, explorer_summary, explorer_report, explorer_file]\n",
    "            )\n",
    "        \n",
    "        # Tab 3: File Manager\n",
    "        with gr.TabItem(\"üìÅ Research Files\"):\n",
    "            gr.Markdown(\"### Research History & File Management\")\n",
    "            \n",
    "            file_table_output = gr.HTML(format_file_table())\n",
    "            \n",
    "            with gr.Row():\n",
    "                refresh_table_btn = gr.Button(\"üîÑ Refresh Table\", variant=\"secondary\")\n",
    "                gr.Markdown(\"*Shows latest research files with evaluation indicators*\")\n",
    "            \n",
    "            refresh_table_btn.click(\n",
    "                lambda: gr.HTML.update(value=format_file_table()),\n",
    "                outputs=file_table_output\n",
    "            )\n",
    "    \n",
    "    # Footer\n",
    "    gr.Markdown(\"\"\"\n",
    "    ---\n",
    "    **üí° System Guide:**\n",
    "    \n",
    "    1. **Complete Research**: Full pipeline with web search, AI analysis, and dual-model evaluation\n",
    "    2. **Explorer Search**: Raw multi-engine web scraping for data collection  \n",
    "    3. **Research Files**: Browse and manage your research history\n",
    "    \n",
    "    **üéØ Quality Threshold**: Research with consensus score ‚â•7.5/10 is marked as approved\n",
    "    **üîß Technologies**: DuckDuckGo, Brave Search, GPT-4, Claude, Multi-Agent AI\n",
    "    \"\"\")\n",
    "\n",
    "# ============================================\n",
    "# LAUNCH & TEST FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "async def test_complete_system():\n",
    "    print(\"üöÄ Testing Complete System...\")\n",
    "    \n",
    "    # Test explorer\n",
    "    print(\"üß™ Testing Explorer Search...\")\n",
    "    query = \"latest AI developments 2025\"\n",
    "    dd_urls = fetch_duckduckgo_links(query, 3)\n",
    "    brave_urls = fetch_brave_links(query, 3)\n",
    "    \n",
    "    print(f\"ü¶Ü DuckDuckGo: {len(dd_urls)} URLs\")\n",
    "    print(f\"üõ°Ô∏è Brave: {len(brave_urls)} URLs\")\n",
    "    \n",
    "    if dd_urls or brave_urls:\n",
    "        print(\"‚úÖ Explorer search working!\")\n",
    "    else:\n",
    "        print(\"‚ùå Explorer search issues\")\n",
    "    \n",
    "    # Test agentic\n",
    "    print(\"üß™ Testing Agentic Search...\")\n",
    "    try:\n",
    "        query = \"AI agent frameworks comparison\"\n",
    "        results = await execute_agentic_deep_search(query)\n",
    "        \n",
    "        print(f\"‚úÖ Agentic search completed!\")\n",
    "        print(f\"üìä Subqueries: {results['methodology']['subqueries_executed']}\")\n",
    "        print(f\"üìö Evidence: {results['methodology']['total_evidence_pieces']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Agentic search failed: {e}\")\n",
    "    \n",
    "    # Test file management\n",
    "    files = list_available_json_files()\n",
    "    print(f\"üìÅ Found {len(files)} existing files\")\n",
    "    \n",
    "    print(\"‚úÖ System test completed!\")\n",
    "\n",
    "def show_system_status():\n",
    "    print(\"üî¨ Deep Search Advanced System Status\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"üìÅ Workspace: {WORKSPACE_DIR}\")\n",
    "    print(f\"üíæ Data Dir: {DATA_DIR}\")\n",
    "    print(f\"üìä Results Dir: {RESULTS_DIR}\")\n",
    "    print(f\"ü§ñ LLM Status: {'Available' if LLM_AVAILABLE else 'Mock Mode'}\")\n",
    "    \n",
    "    files = list_available_json_files()\n",
    "    print(f\"üìÑ Research Files: {len(files)}\")\n",
    "    \n",
    "    by_type = {}\n",
    "    for file_info in files:\n",
    "        file_type = file_info['type']\n",
    "        by_type[file_type] = by_type.get(file_type, 0) + 1\n",
    "    \n",
    "    for file_type, count in by_type.items():\n",
    "        print(f\"  üîß {file_type}: {count}\")\n",
    "    \n",
    "    print(\"\\nüöÄ Ready to launch!\")\n",
    "\n",
    "def launch_gradio_interface():\n",
    "    print(\"üåê Launching Gradio Interface...\")\n",
    "    demo.launch(\n",
    "        share=False,      # Keep localhost only\n",
    "        server_port=7860, # Default port\n",
    "        show_error=True,  # Show errors\n",
    "        debug=True,       # Debug mode\n",
    "        inbrowser=True    # Open browser automatically\n",
    "    )\n",
    "\n",
    "# Show system status\n",
    "show_system_status()\n",
    "\n",
    "print(\"\\nüí° Available Commands:\")\n",
    "print(\"  ‚Ä¢ await test_complete_system() - Test all functionality\")\n",
    "print(\"  ‚Ä¢ await run_research('your query') - Complete research\")\n",
    "print(\"  ‚Ä¢ display_research_summary() - Show file summary\")\n",
    "print(\"  ‚Ä¢ launch_gradio_interface() - Launch web interface\")\n",
    "print(\"  ‚Ä¢ demo.launch() - Quick launch Gradio\")\n",
    "\n",
    "print(\"\\n‚úÖ Deep Search Advanced System Ready!\")\n",
    "print(\"üöÄ To start: Run launch_gradio_interface() or demo.launch()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23612ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Combined Deep Search: To help Halim to create similar box sets in the future, we would like to create a Python algorithm that suggests a possible solution for any given set of jars.\n",
      "\n",
      "We will use a list to store the collection of jars available. For instance, using the current set of 10 jars, our list would be as follows:\n",
      "jars = [150,20,20,10,80,130,110,90,100,40]\n",
      "\n",
      "The aim of this challenge is to use a backtracking / recursive algorithm to work out a possible solution for this puzzle by creating three lists of jars, using the values from the above jars list to create 3 box sets of exactly 250g each.\n",
      "üîç Phase 1: Explorer Search...\n",
      "   üìä Found 0 DuckDuckGo + 7 Brave URLs\n",
      "Scraping: https://brave.com/download/?mtm_source=brave-search&mtm_medium=searchfooter&mtm_campaign=brave-search&mtm_content=evergreen\n",
      "Scraping: https://status.brave.app/\n",
      "Scraping: https://talk.brave.com/?mtm_source=brave-search&mtm_medium=searchfooter&mtm_campaign=brave-search&mtm_content=evergreen\n",
      "Scraping: https://account.brave.com/?intent=checkout&product=search\n",
      "Scraping: https://brave.com/wallet/?mtm_source=brave-search&mtm_medium=searchfooter&mtm_campaign=brave-search&mtm_content=evergreen\n",
      "   ‚úÖ Scraped 5 pages\n",
      "üß† Phase 2: Agentic Deep Search...\n",
      "üî¨ Starting Agentic Deep Search for: To help Halim to create similar box sets in the future, we would like to create a Python algorithm that suggests a possible solution for any given set of jars.\n",
      "\n",
      "We will use a list to store the collection of jars available. For instance, using the current set of 10 jars, our list would be as follows:\n",
      "jars = [150,20,20,10,80,130,110,90,100,40]\n",
      "\n",
      "The aim of this challenge is to use a backtracking / recursive algorithm to work out a possible solution for this puzzle by creating three lists of jars, using the values from the above jars list to create 3 box sets of exactly 250g each.\n",
      "üìã Phase 1: Creating Research Plan...\n",
      "   üìä Generated 5 subqueries\n",
      "   üéØ Research Strategy: Develop the algorithm using a combination of foundational theory on backtracking, existing methods of partitioning, Python recursion practices, and practical examples from algorithm challenges.\n",
      "   üìà Estimated Depth: 4/5\n",
      "üîç Phase 2: Executing Deep Research...\n",
      "   üîé Subquery 1: What are the principles and techniques of backtrac...\n",
      "   üîé Subquery 2: Are there existing algorithms or methods for parti...\n",
      "   üîé Subquery 3: What are the best practices and common pitfalls wh...\n",
      "   üîé Subquery 4: Can similar partitioning problems be found in comp...\n",
      "   üîé Subquery 5: What is the computational complexity of backtracki...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_efb58d95191bdff246b2f8c3be26557a in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. (request_id: req_efb58d95191bdff246b2f8c3be26557a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Combined Deep Search: To help Halim to create similar box sets in the future, we would like to create a Python algorithm that suggests a possible solution for any given set of jars.\n",
      "\n",
      "We will use a list to store the collection of jars available. For instance, using the current set of 10 jars, our list would be as follows:\n",
      "jars = [150,20,20,10,80,130,110,90,100,40]\n",
      "\n",
      "The aim of this challenge is to use a backtracking / recursive algorithm to work out a possible solution for this puzzle by creating three lists of jars, using the values from the above jars list to create 3 box sets of exactly 250g each.\n",
      "üîç Phase 1: Explorer Search...\n",
      "   üìä Found 0 DuckDuckGo + 7 Brave URLs\n",
      "Scraping: https://brave.com/download/?mtm_source=brave-search&mtm_medium=searchfooter&mtm_campaign=brave-search&mtm_content=evergreen\n",
      "Scraping: https://status.brave.app/\n",
      "Scraping: https://talk.brave.com/?mtm_source=brave-search&mtm_medium=searchfooter&mtm_campaign=brave-search&mtm_content=evergreen\n",
      "Scraping: https://account.brave.com/?intent=checkout&product=search\n",
      "Scraping: https://brave.com/wallet/?mtm_source=brave-search&mtm_medium=searchfooter&mtm_campaign=brave-search&mtm_content=evergreen\n",
      "   ‚úÖ Scraped 5 pages\n",
      "üß† Phase 2: Agentic Deep Search...\n",
      "üî¨ Starting Agentic Deep Search for: To help Halim to create similar box sets in the future, we would like to create a Python algorithm that suggests a possible solution for any given set of jars.\n",
      "\n",
      "We will use a list to store the collection of jars available. For instance, using the current set of 10 jars, our list would be as follows:\n",
      "jars = [150,20,20,10,80,130,110,90,100,40]\n",
      "\n",
      "The aim of this challenge is to use a backtracking / recursive algorithm to work out a possible solution for this puzzle by creating three lists of jars, using the values from the above jars list to create 3 box sets of exactly 250g each.\n",
      "üìã Phase 1: Creating Research Plan...\n",
      "   üìä Generated 5 subqueries\n",
      "   üéØ Research Strategy: Focus on understanding the underlying algorithms, their implementation in Python, and explore existing solutions to guide the new development.\n",
      "   üìà Estimated Depth: 4/5\n",
      "üîç Phase 2: Executing Deep Research...\n",
      "   üîé Subquery 1: What are backtracking and recursive algorithms?...\n",
      "   üîé Subquery 2: How can recursion be effectively implemented in Py...\n",
      "   üîé Subquery 3: What are existing algorithms for partitioning sets...\n",
      "   üîé Subquery 4: What are best practices for testing and validating...\n",
      "   üîé Subquery 5: What are similar projects or case studies on creat...\n",
      "   ‚úÖ Completed 5 deep searches\n",
      "üß† Phase 3: Synthesizing Research...\n",
      "‚úÖ Agentic Deep Search Completed!\n",
      "   ‚úÖ Completed agentic research with 5 subqueries\n",
      "üìä Phase 3: Dual-Model Evaluation...\n",
      "ü§ñ Starting GPT-4 evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepa\\AppData\\Local\\Temp\\ipykernel_18480\\2974044853.py:395: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  \"research_plan\": research_plan.dict(),\n",
      "C:\\Users\\deepa\\AppData\\Local\\Temp\\ipykernel_18480\\2974044853.py:396: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  \"findings\": [finding.dict() for finding in research_findings],\n",
      "C:\\Users\\deepa\\AppData\\Local\\Temp\\ipykernel_18480\\2974044853.py:397: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  \"final_report\": final_report.dict(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Starting Claude evaluation...\n",
      "üîÑ Building consensus...\n",
      "   üìà Consensus Score: 7.8/10\n",
      "   ‚úÖ Quality: APPROVED\n",
      "üíæ Complete results saved to: combined_deep_search_20250818_194405.json\n",
      "\n",
      "============================================================\n",
      "üìä RESEARCH SUMMARY\n",
      "============================================================\n",
      "üéØ Query: To help Halim to create similar box sets in the future, we would like to create a Python algorithm that suggests a possible solution for any given set of jars.\n",
      "\n",
      "We will use a list to store the collection of jars available. For instance, using the current set of 10 jars, our list would be as follows:\n",
      "jars = [150,20,20,10,80,130,110,90,100,40]\n",
      "\n",
      "The aim of this challenge is to use a backtracking / recursive algorithm to work out a possible solution for this puzzle by creating three lists of jars, using the values from the above jars list to create 3 box sets of exactly 250g each.\n",
      "üîß Methods: explorer, agentic\n",
      "üìà GPT-4 Score: 7.8/10\n",
      "üß† Claude Score: 5.6/10\n",
      "üéØ Consensus: 7.8/10\n",
      "‚úÖ Quality: APPROVED\n",
      "üíæ Saved to: d:\\Workspace\\LLMs_projects\\agents\\DeepSearch\\workspace\\data\\combined_deep_search_20250818_194405.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepa\\AppData\\Local\\Temp\\ipykernel_18480\\2974044853.py:678: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  results[\"evaluation\"] = evaluation.dict()\n"
     ]
    }
   ],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2640e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment and directories initialized!\n",
      "‚úÖ Configuration and models loaded!\n",
      "‚úÖ HTML parsing utilities loaded!\n",
      "üö´ Google search functions disabled (uncomment to enable)\n",
      "‚úÖ DuckDuckGo search functions loaded!\n",
      "‚úÖ Brave search functions loaded!\n",
      "‚úÖ Web scraping functions loaded!\n",
      "‚úÖ Enhanced explorer search orchestration loaded!\n",
      "‚úÖ Agentic search agents initialized!\n",
      "üìä Configuration: 3 searches, medium context, 1000+ word reports\n",
      "üî• Deep search agents ENABLED and ready!\n",
      "‚úÖ Agentic search execution functions loaded!\n",
      "‚úÖ Deep search execution functions loaded!\n",
      "üîß Deep search configuration: 5 iterations, refinement=enabled\n",
      "‚úÖ Universal JSON evaluator loaded!\n",
      "‚úÖ API error handling utilities loaded!\n",
      "üí° Use test_api_availability() to check current API status\n",
      "üí° Fallback evaluations activate automatically when needed\n",
      "‚úÖ Evaluation orchestration functions loaded!\n",
      "‚úÖ Combined search orchestration loaded!\n",
      "‚úÖ Jupyter execution interface ready!\n",
      "üí° Available functions:\n",
      "   - await run_research('your query')           # Agentic search with evaluation\n",
      "   - await run_combined_research('your query')  # Explorer + Agentic combined\n",
      "   - await run_explorer_only_research('query')  # Explorer-only with detailed JSON\n",
      "   - await universal_json_evaluator(filepath, query)  # Evaluate existing results\n",
      "üî• Deep search functions (NOW ENABLED!):\n",
      "   - await run_deep_chatgpt_research('query')    # Deep ChatGPT iterative research\n",
      "   - await run_deep_claude_research('query')     # Deep Claude-style systematic research\n",
      "   - await run_comparative_deep_research('query') # Comparative ChatGPT vs Claude research\n",
      "\n",
      "üîß Current configuration:\n",
      "   - Max links to extract: 20\n",
      "   - Max URLs to scrape: 5\n",
      "   - Max search results: 10\n",
      "   - Strategic searches: 3\n",
      "   - Deep search iterations: 5\n",
      "   - Search context size: medium\n",
      "\n",
      "üìÑ Explorer features:\n",
      "   - Detailed link tracking and progress display\n",
      "   - Comprehensive JSON saving with metadata\n",
      "   - Source-by-source analysis and statistics\n",
      "   - Performance metrics and error tracking\n",
      "\n",
      "================================================================================\n",
      "üìù CONFIGURATION CUSTOMIZATION GUIDE\n",
      "================================================================================\n",
      "\n",
      "üéõÔ∏è **How to customize the search system:**\n",
      "\n",
      "1. **Modify Search Scope:**\n",
      "   MAX_LINKS_TO_EXTRACT = 30        # Extract more links from search results\n",
      "   MAX_URLS_TO_SCRAPE = 8           # Scrape more web pages  \n",
      "   MAX_SEARCH_RESULTS = 15          # Get more search results per engine\n",
      "\n",
      "2. **Adjust Agentic Search:**\n",
      "   MAX_STRATEGIC_SEARCHES = 5       # Plan more strategic searches\n",
      "   SEARCH_CONTEXT_SIZE = \"large\"    # Use larger context for deeper analysis\n",
      "   REPORT_MIN_LENGTH = 1500         # Require longer, more detailed reports\n",
      "\n",
      "3. **Configure Deep Search:**\n",
      "   DEEP_SEARCH_ITERATIONS = 7       # More iterative refinement cycles\n",
      "   DEEP_SEARCH_REFINEMENT = True    # Enable progressive refinement\n",
      "\n",
      "4. **Performance Tuning:**\n",
      "   SCRAPING_TIMEOUT = 20            # Longer timeout for slow websites\n",
      "   MAX_TEXT_LENGTH = 5000           # Capture more text per page\n",
      "\n",
      "**Example: High-Intensity Research Setup**\n",
      "```python\n",
      "# Uncomment and modify these in Cell 2:\n",
      "# MAX_LINKS_TO_EXTRACT = 50\n",
      "# MAX_URLS_TO_SCRAPE = 10  \n",
      "# MAX_STRATEGIC_SEARCHES = 7\n",
      "# DEEP_SEARCH_ITERATIONS = 10\n",
      "# SEARCH_CONTEXT_SIZE = \"large\"\n",
      "# REPORT_MIN_LENGTH = 2000\n",
      "```\n",
      "\n",
      "**Example: Fast & Light Setup**\n",
      "```python\n",
      "# Uncomment and modify these in Cell 2:\n",
      "# MAX_LINKS_TO_EXTRACT = 10\n",
      "# MAX_URLS_TO_SCRAPE = 3\n",
      "# MAX_STRATEGIC_SEARCHES = 2  \n",
      "# DEEP_SEARCH_ITERATIONS = 3\n",
      "# SEARCH_CONTEXT_SIZE = \"small\"\n",
      "# REPORT_MIN_LENGTH = 500\n",
      "```\n",
      "\n",
      "üî• **Deep search capabilities are NOW ENABLED!**\n",
      "‚úÖ Deep search agents activated\n",
      "‚úÖ Deep search execution functions ready\n",
      "‚úÖ Deep search interface functions available\n",
      "‚úÖ Gradio Deep Search tab enabled\n",
      "\n",
      "üí° **Pro Tips:**\n",
      "- Increase MAX_STRATEGIC_SEARCHES for complex topics\n",
      "- Use \"large\" SEARCH_CONTEXT_SIZE for technical subjects\n",
      "- Enable DEEP_SEARCH_REFINEMENT for controversial topics\n",
      "- Adjust SCRAPING_TIMEOUT if you encounter many timeouts\n",
      "- Use Deep Search for the most comprehensive analysis\n",
      "\n",
      "‚úÖ Configuration guide complete!\n",
      "üîÑ Restart the kernel and rerun all cells after making configuration changes\n",
      "üî• Deep Search is now FULLY OPERATIONAL!\n",
      "‚úÖ Gradio interface ready!\n",
      "üí° Usage:\n",
      "   - interface = launch_gradio_interface()              # Launch locally\n",
      "   - interface = launch_gradio_interface(share=True)    # Create public link\n",
      "   - interface = launch_gradio_interface(debug=True)    # Enable debug mode\n",
      "\n",
      "============================================================\n",
      "üöÄ AUTO-LAUNCHING GRADIO INTERFACE\n",
      "============================================================\n",
      "üìä Configuration: 20 links, 5 URLs, 3 searches\n",
      "üåê Share publicly: No\n",
      "üîß Debug mode: Disabled\n",
      "üîå Port: 7860\n",
      "üéØ Starting Gradio server...\n",
      "‚ùå Failed to auto-launch Gradio interface: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\n",
      "üí° You can manually launch with: launch_gradio_interface()\n",
      "\n",
      "============================================================\n",
      "üéâ ADVANCED RESEARCH SYSTEM FULLY OPERATIONAL!\n",
      "============================================================\n",
      "üìö Complete system with:\n",
      "   ‚úÖ Multi-engine web scraping (DuckDuckGo + Brave)\n",
      "   ‚úÖ AI-powered agentic search\n",
      "   ‚úÖ Dual-model evaluation (GPT-4 + Claude)\n",
      "   ‚úÖ Deep search capabilities (ENABLED!)\n",
      "   ‚úÖ Beautiful Gradio web interface with Deep Search tab\n",
      "   ‚úÖ Fully configurable parameters\n",
      "   ‚úÖ Robust error handling with fallbacks\n",
      "\n",
      "üî• Enhanced Explorer Features:\n",
      "   ‚úÖ Real-time link discovery and display\n",
      "   ‚úÖ Detailed scraping progress with statistics\n",
      "   ‚úÖ Comprehensive JSON saving with metadata\n",
      "   ‚úÖ Performance metrics and error tracking\n",
      "   ‚úÖ Source-by-source analysis and breakdown\n",
      "   ‚úÖ Automatic file type detection in analysis\n",
      "\n",
      "üöÄ Ready for advanced research tasks!\n",
      "üî• Deep Search now available in both notebook and Gradio interface!\n",
      "üìä Explorer JSON data now provides complete transparency!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "# RESEARCHING: write a pytho3 program To help Halim to create similar box sets in the future, we would like to create a Python algorithm that suggests a possible solution for any given set of jars.\n",
      "\n",
      "We will use a list to store the collection of jars available. For instance, using the current set of 10 jars, our list would be as follows:\n",
      "jars = [150,20,20,10,80,130,110,90,100,40]\n",
      "\n",
      "The aim of this challenge is to use a backtracking / recursive algorithm to work out a possible solution for this puzzle by creating three lists of jars, using the values from the above jars list to create 3 box sets of exactly 250g each.\n",
      "############################################################\n",
      "\n",
      "==================================================\n",
      "üîç PHASE 1: DEEP SEARCH\n",
      "==================================================\n",
      "\n",
      "üìã Planning searches...\n",
      "\n",
      "üåê Performing web searches...\n",
      "\n",
      "üìù Writing comprehensive report...\n",
      "\n",
      "==================================================\n",
      "üìÑ INITIAL RESEARCH REPORT\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Report on Backtracking Algorithm for Jar Partitioning Problem\n",
       "\n",
       "## Executive Summary  \n",
       "The challenge involves partitioning a given set of jars, each with a specified weight, into three separate box sets, with each set containing a total weight of 250 grams. Given the jars represented as a list, the algorithm employs a backtracking approach to explore potential combinations systematically. This report synthesizes the approach taken, the implementation details, findings, and actionable insights.\n",
       "\n",
       "## Introduction  \n",
       "Halim needs assistance in creating box sets from a collection of jars, which can be described as a combination problem‚Äîa form of the partition problem in computational complexity. This problem is well-known in combinatorial optimization and requires splitting a list of integers into subsets meeting specific criteria. Here, the criteria are defined as achieving equal weights across subsets, specifically at 250 grams per box. \n",
       "\n",
       "### Problem Statement  \n",
       "Given the list of jars' weights:  \n",
       "```python  \n",
       "jars = [150, 20, 20, 10, 80, 130, 110, 90, 100, 40]  \n",
       "```  \n",
       "The goal is to implement an algorithm that analyzes these weights and suggests valid partitions to form three separate box sets, effectively utilizing the backtracking approach.\n",
       "\n",
       "## Research Findings  \n",
       "The partitioning problem is NP-complete, inherently exhibiting challenges when scaling data size. However, backtracking algorithms can effectively navigate smaller datasets by exploring all possible partitions and systematically validating results.\n",
       "\n",
       "### Key Steps in Backtracking Algorithm  \n",
       "To tackle the jar partition problem, the backtracking approach involves the following:\n",
       "\n",
       "1. **Sort the Array**: The jar weights are sorted in descending order to optimize the search process and quickly eliminate combinations that exceed target weights.\n",
       "2. **Calculate Target Sum**: The total weight of jars is computed. If it's not dividable by three, valid box sets cannot be formed, and the function exits early.\n",
       "3. **Define Backtracking Function**: This recursive function will attempt to build each box set by evaluating weights one by one. If a weight exceeds the box capacity (250g), the algorithm backtracks and tries different combinations.\n",
       "4. **Base Case**: The recursion concludes when all jars are processed, and each box set accumulates the required weights.\n",
       "\n",
       "### Backtracking Code Implementation  \n",
       "Below is the sample Python implementation of the backtracking algorithm to solve the jar partitioning problem:\n",
       "\n",
       "```python  \n",
       "def can_partition(jars, target_weight=250, num_boxes=3):  \n",
       "    def backtrack(index, current_sums):  \n",
       "        # Check if all boxes have reached the target weight  \n",
       "        if all(weight == target_weight for weight in current_sums):  \n",
       "            return True  \n",
       "        if current_sums.count(target_weight) == num_boxes:  \n",
       "            return True  \n",
       "        if index >= len(jars):  \n",
       "            return False  \n",
       "        for i in range(num_boxes):  \n",
       "            if current_sums[i] + jars[index] <= target_weight:  \n",
       "                current_sums[i] += jars[index]  \n",
       "                if backtrack(index + 1, current_sums):  \n",
       "                    return True  \n",
       "                current_sums[i] -= jars[index]  \n",
       "            if current_sums[i] == 0:  \n",
       "                break  \n",
       "        return False  \n",
       "  \n",
       "    return backtrack(0, [0] * num_boxes)  \n",
       "\n",
       "# Example Usage  \n",
       "jars = [150, 20, 20, 10, 80, 130, 110, 90, 100, 40]  \n",
       "result = can_partition(jars)  \n",
       "print(\"Can partition into three sets of 250g: \", result)  \n",
       "```  \n",
       "\n",
       "### Explanation of Code  \n",
       "- **Function Definition**: `can_partition()` takes a list of jars, the target weight for each box, and the number of boxes. \n",
       "- **Recursive Function**: The inner function `backtrack()` handles indices and current sums of the box weights. It checks if a configuration meets the weight requirement, if the weight surpasses the limit, or if it should continue exploring other configurations.\n",
       "- **Optimal Check**: The condition `if current_sums[i] == 0: break` prevents unnecessary further checks if the current box is still empty, optimizing performance further.\n",
       "\n",
       "## Main Findings  \n",
       "- **Efficiency**: The backtracking method is feasible for relatively small datasets, successfully partitioning jars into three box sets.\n",
       "- **Performance Implications**: While this approach offers a clear path for smaller problems, its exponential nature poses limitations with larger datasets or increased constraints.\n",
       "- **Compression Techniques**: Analyzing ways to preprocess weights can enhance efficiency, potentially applying dynamic programming for expanded datasets.\n",
       "\n",
       "## Discussion  \n",
       "The algorithm offers a foundational approach to partition problems. Future enhancements could involve integrating dynamic programming techniques to manage larger sets more effectively or employing heuristic methods for approximation when exact solutions become computationally prohibitive.\n",
       "\n",
       "### Considerations  \n",
       "- **Edge Cases**: Scenarios where the total weight isn‚Äôt exactly dividable by three need handling to avoid runtime errors.\n",
       "- **Scalability**: As the dataset grows, switching to a more efficient method (e.g., dynamic programming) will likely be necessary to maintain performance and speed.\n",
       "\n",
       "## Conclusions  \n",
       "The development of a backtracking algorithm for partitioning jar weights effectively demonstrates practical applications of recursion and combinatorial optimization in Python. The approach lays the groundwork for addressing more complex partitioning problems while offering insights into computational limitations and performance optimizations. Future adjustments, particularly in data handling and larger data sets, will be pivotal for creating scalable solutions.\n",
       "\n",
       "## Actionable Insights  \n",
       "1. Enhance the current function to handle larger inputs efficiently through preprocessing or alternative algorithms.\n",
       "2. Conduct further research on hybrid approaches combining backtracking with heuristic or approximation strategies for quicker results in larger datasets.\n",
       "3. Implement error handling to manage edge cases more robustly, particularly with unevenly weighted sets.\n",
       "\n",
       "---  \n",
       "This report encapsulates significant findings and approaches in developing an effective solution to Halim's jar partition problem, contributing towards systematic partitioning problems and algorithmic exploration in Python."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üî¨ PHASE 2: DUAL-MODEL EVALUATION\n",
      "==================================================\n",
      "ü§ñ Running GPT-4 evaluation...\n",
      "ü§ñ GPT-4 evaluation attempt 1/3...\n",
      "üß† Running Claude evaluation...\n",
      "üß† Claude evaluation attempt 1/3...\n",
      "‚ö†Ô∏è Claude attempt 1 failed: Extra data: line 42 column 1 (char 1571)\n",
      "üö´ Claude error: Extra data: line 42 column 1 (char 1571), using fallback evaluation...\n",
      "üîß Generating fallback Claude evaluation...\n",
      "üîÑ Building evaluation consensus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepa\\AppData\\Local\\Temp\\ipykernel_18480\\1201890045.py:1174: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  gpt_scores = gpt_eval.criteria_scores.dict()\n",
      "C:\\Users\\deepa\\AppData\\Local\\Temp\\ipykernel_18480\\1201890045.py:1175: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  claude_scores = claude_eval.criteria_scores.dict()\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "# üìä Research Quality Evaluation\n",
       "\n",
       "## üéØ Query Analysis\n",
       "**Original Query**: write a pytho3 program To help Halim to create similar box sets in the future, we would like to create a Python algorithm that suggests a possible solution for any given set of jars.\n",
       "\n",
       "We will use a list to store the collection of jars available. For instance, using the current set of 10 jars, our list would be as follows:\n",
       "jars = [150,20,20,10,80,130,110,90,100,40]\n",
       "\n",
       "The aim of this challenge is to use a backtracking / recursive algorithm to work out a possible solution for this puzzle by creating three lists of jars, using the values from the above jars list to create 3 box sets of exactly 250g each.\n",
       "**Content Type**: Agentic\n",
       "\n",
       "## ‚ö†Ô∏è API Status\n",
       "- **GPT-4**: ‚úÖ API evaluation successful\n",
       "- **Claude**: Used fallback evaluation (API overloaded)\n",
       "\n",
       "*Note: Fallback evaluations use heuristic scoring and may be less accurate.*\n",
       "\n",
       "## üìà Evaluation Scores\n",
       "\n",
       "### GPT-4 Assessment \n",
       "- **Overall Score**: 7.8/10\n",
       "- **Accuracy**: 8.0/10\n",
       "- **Completeness**: 7.0/10  \n",
       "- **Relevance**: 9.0/10\n",
       "- **Clarity**: 8.0/10\n",
       "- **Depth**: 7.0/10\n",
       "\n",
       "### Claude Assessment (Fallback)\n",
       "- **Overall Score**: 6.9/10\n",
       "- **Accuracy**: 6.0/10\n",
       "- **Completeness**: 8.0/10\n",
       "- **Relevance**: 7.0/10  \n",
       "- **Clarity**: 6.5/10\n",
       "- **Depth**: 7.0/10\n",
       "\n",
       "### ü§ù Consensus Analysis\n",
       "- **Consensus Score**: 8.7/10\n",
       "- **Quality Rating**: ‚úÖ HIGH QUALITY\n",
       "- **Note**: Consensus may be affected by API fallbacks\n",
       "\n",
       "## üí™ Strengths\n",
       "- The content provides a clear and structured approach to solving the jar partitioning problem using a backtracking algorithm.\n",
       "- The Python code implementation is well-documented and demonstrates the application of the backtracking technique effectively.\n",
       "- The report includes a logical explanation of the problem and the steps involved in the algorithm, which enhances understanding.\n",
       "\n",
       "## üîß Areas for Improvement  \n",
       "- The report lacks a detailed analysis of potential limitations or challenges associated with the algorithm, such as performance issues with larger datasets.\n",
       "- There is limited discussion on alternative approaches or optimizations that could be considered for this problem.\n",
       "- The explanation of the code could be expanded to include more detailed comments on each step of the algorithm.\n",
       "\n",
       "## üéØ Recommendations\n",
       "- Expand the discussion to include potential optimizations or alternative approaches to solving the partition problem.\n",
       "- Include a section on the computational complexity of the algorithm to provide insights into its efficiency and scalability.\n",
       "- Verify key facts with additional sources\n",
       "- Re-evaluate when Claude API is available\n",
       "- Consider expert review for technical content\n",
       "\n",
       "---\n",
       "*Evaluation completed at 2025-08-18 19:33:21*\n",
       "*Some evaluations used fallback methods due to API limitations*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Evaluation saved to: d:\\Workspace\\LLMs_projects\\agents\\DeepSearch\\workspace\\data\\evaluation_20250818_193321.json\n",
      "\n",
      "==================================================\n",
      "üîß PHASE 3: QUALITY CHECK\n",
      "==================================================\n",
      "\n",
      "‚úÖ Research Quality: APPROVED\n",
      "Consensus Score: 8.7/10\n",
      "Quality Threshold: 7.5/10\n",
      "\n",
      "==================================================\n",
      "‚ú® DEEP SEARCH WITH EVALUATION COMPLETE\n",
      "==================================================\n",
      "\n",
      "üìä Summary for 'write a pytho3 program To help Halim to create similar box sets in the future, we would like to create a Python algorithm that suggests a possible solution for any given set of jars.\n",
      "\n",
      "We will use a list to store the collection of jars available. For instance, using the current set of 10 jars, our list would be as follows:\n",
      "jars = [150,20,20,10,80,130,110,90,100,40]\n",
      "\n",
      "The aim of this challenge is to use a backtracking / recursive algorithm to work out a possible solution for this puzzle by creating three lists of jars, using the values from the above jars list to create 3 box sets of exactly 250g each.':\n",
      "   - Report length: 6322 characters\n",
      "   - GPT-4 Score: 7.8/10\n",
      "   - Claude Score: 6.9/10\n",
      "   - Consensus: 8.7/10\n",
      "   - Quality: ‚úÖ Approved\n",
      "   - Saved to: d:\\Workspace\\LLMs_projects\\agents\\DeepSearch\\workspace\\data\\evaluation_20250818_193321.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 1: IMPORTS AND ENVIRONMENT SETUP\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import random\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "# AI Agent imports\n",
    "from agents import Agent, WebSearchTool, trace, Runner, gen_trace_id, function_tool\n",
    "from agents.model_settings import ModelSettings\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LLM clients for evaluation\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# UI and display imports\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Initialize environment\n",
    "load_dotenv(override=True)\n",
    "openai_client = OpenAI()\n",
    "claude_client = Anthropic()\n",
    "\n",
    "# Setup workspace directories\n",
    "BASE_DIR = os.getcwd()\n",
    "WORKSPACE_DIR = os.path.join(BASE_DIR, \"workspace\")\n",
    "RESULTS_DIR = os.path.join(WORKSPACE_DIR, \"results\")\n",
    "DATA_DIR = os.path.join(WORKSPACE_DIR, \"data\")\n",
    "\n",
    "for dir_path in [WORKSPACE_DIR, RESULTS_DIR, DATA_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Environment and directories initialized!\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 2: CONFIGURATION AND PYDANTIC MODELS\n",
    "# ============================================\n",
    "\n",
    "# ===== USER CONFIGURABLE SETTINGS =====\n",
    "# Modify these variables based on your requirements\n",
    "\n",
    "# Web scraping configuration\n",
    "MAX_LINKS_TO_EXTRACT = 20      # Maximum links to extract from search results\n",
    "MAX_URLS_TO_SCRAPE = 5         # Maximum URLs to scrape content from\n",
    "MAX_TEXT_LENGTH = 3000         # Maximum text length per scraped page\n",
    "SCRAPING_TIMEOUT = 15          # Timeout in seconds for web scraping\n",
    "\n",
    "# Agentic search configuration  \n",
    "MAX_STRATEGIC_SEARCHES = 3     # Number of strategic searches to plan\n",
    "SEARCH_CONTEXT_SIZE = \"medium\" # \"small\", \"medium\", \"large\" - affects search depth\n",
    "MAX_SEARCH_RESULTS = 10        # Maximum results per search\n",
    "REPORT_MIN_LENGTH = 1000       # Minimum report length in words\n",
    "\n",
    "# Deep search configuration\n",
    "ENABLE_DEEP_SEARCH = True      # Enable/disable deep search mode\n",
    "DEEP_SEARCH_ITERATIONS = 5     # Number of deep search iterations\n",
    "DEEP_SEARCH_REFINEMENT = True  # Enable search refinement\n",
    "\n",
    "# User agents for web scraping\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"\n",
    "]\n",
    "\n",
    "# Pydantic Models for structured data\n",
    "class WebSearchItem(BaseModel):\n",
    "    reason: str = Field(description=\"Reasoning for this search\")\n",
    "    query: str = Field(description=\"Search term to use\")\n",
    "\n",
    "class WebSearchPlan(BaseModel):\n",
    "    searches: List[WebSearchItem] = Field(description=\"List of strategic searches\")\n",
    "\n",
    "class ReportData(BaseModel):\n",
    "    short_summary: str = Field(description=\"Short summary\")\n",
    "    markdown_report: str = Field(description=\"Detailed report\")\n",
    "    follow_up_questions: List[str] = Field(description=\"Follow-up questions\")\n",
    "\n",
    "class CombinedSearchResults(BaseModel):\n",
    "    query: str\n",
    "    timestamp: str\n",
    "    explorer_results: List[Dict] = Field(description=\"Raw scraping results\")\n",
    "    agentic_results: ReportData = Field(description=\"AI-generated report\")\n",
    "    combined_summary: str = Field(description=\"Summary of both approaches\")\n",
    "\n",
    "# Evaluation Models\n",
    "class EvaluationCriteria(BaseModel):\n",
    "    accuracy_score: float = Field(description=\"Factual correctness (0-10)\")\n",
    "    completeness_score: float = Field(description=\"Comprehensive coverage (0-10)\")\n",
    "    relevance_score: float = Field(description=\"Query relevance (0-10)\")\n",
    "    clarity_score: float = Field(description=\"Organization and clarity (0-10)\")\n",
    "    depth_score: float = Field(description=\"Insight and analysis depth (0-10)\")\n",
    "\n",
    "class DetailedEvaluation(BaseModel):\n",
    "    criteria_scores: EvaluationCriteria\n",
    "    overall_score: float\n",
    "    strengths: List[str]\n",
    "    weaknesses: List[str]\n",
    "    missing_aspects: List[str]\n",
    "    recommendations: List[str]\n",
    "    confidence_level: str\n",
    "\n",
    "class UniversalEvaluation(BaseModel):\n",
    "    query: str\n",
    "    content_type: str  # \"explorer\", \"agentic\", \"combined\"\n",
    "    gpt_evaluation: DetailedEvaluation\n",
    "    claude_evaluation: DetailedEvaluation\n",
    "    consensus_score: float\n",
    "    final_recommendations: List[str]\n",
    "    evaluation_summary: str\n",
    "\n",
    "print(\"‚úÖ Configuration and models loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 3: HTML PARSING UTILITIES\n",
    "# ============================================\n",
    "\n",
    "class LinkExtractor(HTMLParser):\n",
    "    \"\"\"Custom HTML parser to extract links from web pages\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.links = []\n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'a':\n",
    "            for attr_name, attr_value in attrs:\n",
    "                if attr_name == 'href' and attr_value and attr_value.startswith('http'):\n",
    "                    self.links.append(attr_value)\n",
    "\n",
    "def simple_html_parse(html_content, max_links=None):\n",
    "    \"\"\"Simple HTML parser to extract links using built-in HTMLParser\"\"\"\n",
    "    if max_links is None:\n",
    "        max_links = MAX_LINKS_TO_EXTRACT\n",
    "        \n",
    "    parser = LinkExtractor()\n",
    "    try:\n",
    "        parser.feed(html_content)\n",
    "        return parser.links[:max_links]\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing HTML: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ HTML parsing utilities loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 4: GOOGLE SEARCH FUNCTIONS (DISABLED)\n",
    "# ============================================\n",
    "\n",
    "# Google API Configuration (commented out to avoid costs)\n",
    "# GOOGLE_API_URL = \"https://google-search-master-mega.p.rapidapi.com/search\"\n",
    "# GOOGLE_HEADERS = {\n",
    "#     \"x-rapidapi-key\": \"your-api-key-here\",\n",
    "#     \"x-rapidapi-host\": \"google-search-master-mega.p.rapidapi.com\"\n",
    "# }\n",
    "\n",
    "# def fetch_google_results(query=\"\"):\n",
    "#     \"\"\"\n",
    "#     Fetch search results from Google via RapidAPI using urllib.\n",
    "#     CURRENTLY DISABLED - Uncomment to re-enable Google search\n",
    "#     \"\"\"\n",
    "#     params = {\n",
    "#         \"q\": query,\n",
    "#         \"gl\": \"us\",\n",
    "#         \"hl\": \"en\", \n",
    "#         \"num\": \"10\",\n",
    "#         \"page\": \"1\"\n",
    "#     }\n",
    "#     \n",
    "#     url = GOOGLE_API_URL + \"?\" + urllib.parse.urlencode(params)\n",
    "#     \n",
    "#     try:\n",
    "#         req = urllib.request.Request(url)\n",
    "#         for key, value in GOOGLE_HEADERS.items():\n",
    "#             req.add_header(key, value)\n",
    "#         \n",
    "#         with urllib.request.urlopen(req, timeout=10) as response:\n",
    "#             data = response.read().decode('utf-8')\n",
    "#             return json.loads(data)\n",
    "#     \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching Google results: {e}\")\n",
    "#         return {}\n",
    "\n",
    "print(\"üö´ Google search functions disabled (uncomment to enable)\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 5: DUCKDUCKGO SEARCH FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def fetch_duckduckgo_links(query, results=None):\n",
    "    \"\"\"\n",
    "    Fetch search results from DuckDuckGo using direct HTML scraping.\n",
    "    \n",
    "    This function:\n",
    "    1. Builds DuckDuckGo search URL with query parameters\n",
    "    2. Sends HTTP request with random User-Agent to avoid blocking\n",
    "    3. Parses HTML response to extract result links\n",
    "    4. Filters out DuckDuckGo internal links\n",
    "    5. Returns clean list of external URLs\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        results = MAX_SEARCH_RESULTS\n",
    "        \n",
    "    endpoint = \"https://duckduckgo.com/html/\"\n",
    "    params = {\"q\": query}\n",
    "    url = endpoint + \"?\" + urllib.parse.urlencode(params)\n",
    "    \n",
    "    try:\n",
    "        # Create request with random user agent to avoid detection\n",
    "        req = urllib.request.Request(url)\n",
    "        req.add_header('User-Agent', random.choice(USER_AGENTS))\n",
    "        \n",
    "        # Fetch HTML content\n",
    "        with urllib.request.urlopen(req, timeout=10) as response:\n",
    "            html_content = response.read().decode('utf-8')\n",
    "        \n",
    "        # Extract links from HTML\n",
    "        links = simple_html_parse(html_content, results)\n",
    "        \n",
    "        # Filter out DuckDuckGo internal links\n",
    "        clean_links = [link for link in links if 'duckduckgo.com' not in link]\n",
    "        return clean_links[:results]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching DuckDuckGo results: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ DuckDuckGo search functions loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 6: BRAVE SEARCH FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def fetch_brave_links(query, results=None):\n",
    "    \"\"\"\n",
    "    Fetch search results from Brave Search using direct HTML scraping.\n",
    "    \n",
    "    Similar to DuckDuckGo but targets Brave Search engine.\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        results = MAX_SEARCH_RESULTS\n",
    "        \n",
    "    endpoint = \"https://search.brave.com/search\"\n",
    "    params = {\"q\": query}\n",
    "    url = endpoint + \"?\" + urllib.parse.urlencode(params)\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(url)\n",
    "        req.add_header('User-Agent', random.choice(USER_AGENTS))\n",
    "        req.add_header('Accept-Encoding', 'identity')\n",
    "        \n",
    "        with urllib.request.urlopen(req, timeout=10) as response:\n",
    "            html_content = response.read().decode('utf-8')\n",
    "        \n",
    "        links = simple_html_parse(html_content, results)\n",
    "        clean_links = [link for link in links if 'search.brave.com' not in link]\n",
    "        return clean_links[:results]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Brave results: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ Brave search functions loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 7: WEB SCRAPING FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def scrape_pages_text(urls, max_urls=None, source_name=\"Unknown\"):\n",
    "    \"\"\"\n",
    "    Extract text content from a list of URLs with detailed progress tracking.\n",
    "    \n",
    "    This function:\n",
    "    1. Iterates through provided URLs (limited to max_urls for performance)\n",
    "    2. Fetches HTML content from each URL\n",
    "    3. Strips HTML tags to extract plain text\n",
    "    4. Returns structured data with URL, text content, and metadata\n",
    "    \"\"\"\n",
    "    if max_urls is None:\n",
    "        max_urls = MAX_URLS_TO_SCRAPE\n",
    "        \n",
    "    collected = []\n",
    "    \n",
    "    print(f\"üîó Starting to scrape {min(len(urls), max_urls)} URLs from {source_name}:\")\n",
    "    \n",
    "    for i, url in enumerate(urls[:max_urls], 1):\n",
    "        print(f\"\\nüìÑ [{i}/{min(len(urls), max_urls)}] Scraping: {url}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Create request with random user agent\n",
    "            req = urllib.request.Request(url)\n",
    "            req.add_header('User-Agent', random.choice(USER_AGENTS))\n",
    "            \n",
    "            # Fetch HTML content with timeout\n",
    "            with urllib.request.urlopen(req, timeout=SCRAPING_TIMEOUT) as response:\n",
    "                content_type = response.headers.get('content-type', '')\n",
    "                content_length = response.headers.get('content-length', 'Unknown')\n",
    "                html_content = response.read().decode('utf-8', errors='ignore')\n",
    "            \n",
    "            # Simple text extraction using regex\n",
    "            import re\n",
    "            # Remove script and style tags with content\n",
    "            text = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)\n",
    "            text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "            # Remove all HTML tags\n",
    "            text = re.sub(r'<[^>]+>', ' ', text)\n",
    "            # Clean up whitespace\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "            # Calculate processing time\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Store structured result\n",
    "            result = {\n",
    "                \"url\": url,\n",
    "                \"source_engine\": source_name, \n",
    "                \"text\": text[:MAX_TEXT_LENGTH],  # Limit text length to avoid memory issues\n",
    "                \"full_text_length\": len(text),\n",
    "                \"truncated_length\": len(text[:MAX_TEXT_LENGTH]),\n",
    "                \"scraped_at\": datetime.now().isoformat(),\n",
    "                \"processing_time_seconds\": round(processing_time, 2),\n",
    "                \"content_type\": content_type,\n",
    "                \"content_length\": content_length,\n",
    "                \"success\": True,\n",
    "                \"error\": False\n",
    "            }\n",
    "            \n",
    "            collected.append(result)\n",
    "            \n",
    "            # Success message with stats\n",
    "            print(f\"   ‚úÖ Success! {len(text):,} chars extracted in {processing_time:.2f}s\")\n",
    "            if len(text) > MAX_TEXT_LENGTH:\n",
    "                print(f\"   ‚ö†Ô∏è  Text truncated from {len(text):,} to {MAX_TEXT_LENGTH:,} chars\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            processing_time = time.time() - start_time\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Store error information for debugging\n",
    "            result = {\n",
    "                \"url\": url,\n",
    "                \"source_engine\": source_name,\n",
    "                \"text\": f\"Error: {error_msg}\",\n",
    "                \"full_text_length\": 0,\n",
    "                \"truncated_length\": 0,\n",
    "                \"scraped_at\": datetime.now().isoformat(),\n",
    "                \"processing_time_seconds\": round(processing_time, 2),\n",
    "                \"content_type\": \"error\",\n",
    "                \"content_length\": \"0\",\n",
    "                \"success\": False,\n",
    "                \"error\": True,\n",
    "                \"error_details\": error_msg\n",
    "            }\n",
    "            \n",
    "            collected.append(result)\n",
    "            print(f\"   ‚ùå Failed! {error_msg}\")\n",
    "    \n",
    "    # Summary for this batch\n",
    "    successful = len([item for item in collected if not item.get('error')])\n",
    "    failed = len([item for item in collected if item.get('error')])\n",
    "    total_chars = sum(item.get('full_text_length', 0) for item in collected if not item.get('error'))\n",
    "    \n",
    "    print(f\"\\nüìä {source_name} Scraping Summary:\")\n",
    "    print(f\"   ‚úÖ Successful: {successful}/{len(collected)}\")\n",
    "    print(f\"   ‚ùå Failed: {failed}/{len(collected)}\")\n",
    "    print(f\"   üìÑ Total text extracted: {total_chars:,} characters\")\n",
    "    \n",
    "    return collected\n",
    "\n",
    "print(\"‚úÖ Web scraping functions loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 8: EXPLORER SEARCH ORCHESTRATION\n",
    "# ============================================\n",
    "\n",
    "def perform_explorer_search(query, save_results=True):\n",
    "    \"\"\"\n",
    "    Enhanced explorer search with detailed link tracking and JSON saving.\n",
    "    \n",
    "    This function now only uses DuckDuckGo and Brave Search engines.\n",
    "    Google search has been commented out to avoid API costs and dependencies.\n",
    "    \n",
    "    Search Flow:\n",
    "    1. DuckDuckGo: Fetch links and scrape top results\n",
    "    2. Brave Search: Fetch links and scrape top results\n",
    "    3. Show detailed link information\n",
    "    4. Save comprehensive JSON data\n",
    "    \"\"\"\n",
    "    print(f\"üîç Starting Explorer Search for: {query}\")\n",
    "    print(f\"üìä Configuration: {MAX_SEARCH_RESULTS} results, {MAX_URLS_TO_SCRAPE} URLs to scrape\")\n",
    "    \n",
    "    all_data = []\n",
    "    search_metadata = {\n",
    "        \"query\": query,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"search_config\": {\n",
    "            \"max_search_results\": MAX_SEARCH_RESULTS,\n",
    "            \"max_urls_to_scrape\": MAX_URLS_TO_SCRAPE,\n",
    "            \"max_text_length\": MAX_TEXT_LENGTH,\n",
    "            \"scraping_timeout\": SCRAPING_TIMEOUT\n",
    "        },\n",
    "        \"search_engines\": [],\n",
    "        \"total_links_found\": 0,\n",
    "        \"total_scraped\": 0,\n",
    "        \"errors\": []\n",
    "    }\n",
    "    \n",
    "    # DuckDuckGo search (Active)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ü¶Ü DUCKDUCKGO SEARCH\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    dd_urls = fetch_duckduckgo_links(query, MAX_SEARCH_RESULTS)\n",
    "    if dd_urls:\n",
    "        print(f\"‚úÖ Found {len(dd_urls)} DuckDuckGo links:\")\n",
    "        for i, url in enumerate(dd_urls, 1):\n",
    "            print(f\"  {i:2d}. {url}\")\n",
    "        \n",
    "        print(f\"\\nüîó Scraping top {min(len(dd_urls), MAX_URLS_TO_SCRAPE)} DuckDuckGo URLs...\")\n",
    "        dd_scraped = scrape_pages_text(dd_urls[:MAX_URLS_TO_SCRAPE], source_name=\"DuckDuckGo\")\n",
    "        for entry in dd_scraped:\n",
    "            entry[\"source\"] = \"duckduckgo_scraped\"\n",
    "            entry[\"type\"] = \"scraped_content\"\n",
    "            all_data.append(entry)\n",
    "        \n",
    "        search_metadata[\"search_engines\"].append({\n",
    "            \"name\": \"DuckDuckGo\",\n",
    "            \"links_found\": len(dd_urls),\n",
    "            \"links_scraped\": len(dd_scraped),\n",
    "            \"successful_scrapes\": len([item for item in dd_scraped if not item.get('error')]),\n",
    "            \"all_links\": dd_urls\n",
    "        })\n",
    "        search_metadata[\"total_links_found\"] += len(dd_urls)\n",
    "        search_metadata[\"total_scraped\"] += len(dd_scraped)\n",
    "    else:\n",
    "        print(\"‚ùå No DuckDuckGo links found\")\n",
    "        search_metadata[\"errors\"].append(\"DuckDuckGo: No links found\")\n",
    "    \n",
    "    # Brave Search (Active)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üõ°Ô∏è BRAVE SEARCH\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    brave_urls = fetch_brave_links(query, MAX_SEARCH_RESULTS)\n",
    "    if brave_urls:\n",
    "        print(f\"‚úÖ Found {len(brave_urls)} Brave links:\")\n",
    "        for i, url in enumerate(brave_urls, 1):\n",
    "            print(f\"  {i:2d}. {url}\")\n",
    "        \n",
    "        print(f\"\\nüîó Scraping top {min(len(brave_urls), MAX_URLS_TO_SCRAPE)} Brave URLs...\")\n",
    "        brave_scraped = scrape_pages_text(brave_urls[:MAX_URLS_TO_SCRAPE], source_name=\"Brave\")\n",
    "        for entry in brave_scraped:\n",
    "            entry[\"source\"] = \"brave_scraped\"\n",
    "            entry[\"type\"] = \"scraped_content\"\n",
    "            all_data.append(entry)\n",
    "        \n",
    "        search_metadata[\"search_engines\"].append({\n",
    "            \"name\": \"Brave\",\n",
    "            \"links_found\": len(brave_urls),\n",
    "            \"links_scraped\": len(brave_scraped),\n",
    "            \"successful_scrapes\": len([item for item in brave_scraped if not item.get('error')]),\n",
    "            \"all_links\": brave_urls\n",
    "        })\n",
    "        search_metadata[\"total_links_found\"] += len(brave_urls)\n",
    "        search_metadata[\"total_scraped\"] += len(brave_scraped)\n",
    "    else:\n",
    "        print(\"‚ùå No Brave links found\")\n",
    "        search_metadata[\"errors\"].append(\"Brave: No links found\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä EXPLORER SEARCH SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"üîç Query: {query}\")\n",
    "    print(f\"üìä Total links found: {search_metadata['total_links_found']}\")\n",
    "    print(f\"üîó Total pages scraped: {search_metadata['total_scraped']}\")\n",
    "    print(f\"‚úÖ Successful scrapes: {len([item for item in all_data if not item.get('error')])}\")\n",
    "    print(f\"‚ùå Failed scrapes: {len([item for item in all_data if item.get('error')])}\")\n",
    "    print(f\"üåê Search engines used: {', '.join([engine['name'] for engine in search_metadata['search_engines']])}\")\n",
    "    \n",
    "    # Create comprehensive explorer results\n",
    "    explorer_results = {\n",
    "        \"metadata\": search_metadata,\n",
    "        \"scraped_data\": all_data,\n",
    "        \"summary\": {\n",
    "            \"query\": query,\n",
    "            \"total_sources\": len(all_data),\n",
    "            \"successful_scrapes\": len([item for item in all_data if not item.get('error')]),\n",
    "            \"total_text_length\": sum(len(item.get('text', '')) for item in all_data if not item.get('error')),\n",
    "            \"search_engines_used\": [engine['name'] for engine in search_metadata['search_engines']],\n",
    "            \"completed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save explorer results to JSON if requested\n",
    "    if save_results:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"explorer_search_{timestamp}.json\"\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        with open(filepath, \"w\", encoding='utf-8') as f:\n",
    "            json.dump(explorer_results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üíæ Explorer results saved to: {filepath}\")\n",
    "        print(f\"üìÑ File size: {os.path.getsize(filepath)} bytes\")\n",
    "    \n",
    "    print(\"‚úÖ Explorer search completed successfully!\")\n",
    "    return all_data, explorer_results\n",
    "\n",
    "print(\"‚úÖ Enhanced explorer search orchestration loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 9: AGENTIC SEARCH AGENT DEFINITIONS\n",
    "# ============================================\n",
    "\n",
    "# Agent instructions\n",
    "SEARCH_INSTRUCTIONS = f\"\"\"You are a research assistant. Search the web and provide a concise 200-word summary of key findings. Focus on facts, insights, and actionable information. Use up to {MAX_SEARCH_RESULTS} search results for comprehensive coverage.\"\"\"\n",
    "\n",
    "PLANNER_INSTRUCTIONS = f\"\"\"You are a research planner. Create {MAX_STRATEGIC_SEARCHES} strategic web searches to comprehensively answer the query. Each search should target a different aspect or angle of the topic.\"\"\"\n",
    "\n",
    "WRITER_INSTRUCTIONS = f\"\"\"You are a senior researcher. Create a comprehensive, well-structured markdown report ({REPORT_MIN_LENGTH}+ words) synthesizing all research findings. Include executive summary, main findings, analysis, and conclusions. Ensure depth and actionable insights.\"\"\"\n",
    "\n",
    "# Initialize AI agents\n",
    "search_agent = Agent(\n",
    "    name=\"Search agent\",\n",
    "    instructions=SEARCH_INSTRUCTIONS,\n",
    "    tools=[WebSearchTool(search_context_size=SEARCH_CONTEXT_SIZE)],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    model_settings=ModelSettings(tool_choice=\"required\"),\n",
    ")\n",
    "\n",
    "planner_agent = Agent(\n",
    "    name=\"PlannerAgent\",\n",
    "    instructions=PLANNER_INSTRUCTIONS,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=WebSearchPlan,\n",
    ")\n",
    "\n",
    "writer_agent = Agent(\n",
    "    name=\"WriterAgent\",\n",
    "    instructions=WRITER_INSTRUCTIONS,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=ReportData,\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# DEEP SEARCH AGENTS (NOW ENABLED!)\n",
    "# ============================================\n",
    "\n",
    "# Deep Search ChatGPT Agent\n",
    "DEEP_CHATGPT_INSTRUCTIONS = f\"\"\"You are an advanced ChatGPT researcher performing deep analysis. \n",
    "Conduct {DEEP_SEARCH_ITERATIONS} iterations of research with progressive refinement. \n",
    "For each iteration:\n",
    "1. Analyze previous findings\n",
    "2. Identify knowledge gaps\n",
    "3. Perform targeted searches\n",
    "4. Synthesize insights\n",
    "5. Plan next iteration\n",
    "\n",
    "Provide comprehensive analysis with critical evaluation, multiple perspectives, and actionable recommendations.\"\"\"\n",
    "\n",
    "deep_chatgpt_agent = Agent(\n",
    "    name=\"DeepChatGPTAgent\",\n",
    "    instructions=DEEP_CHATGPT_INSTRUCTIONS,\n",
    "    tools=[WebSearchTool(search_context_size=\"large\")],\n",
    "    model=\"gpt-4o\",  # Use more powerful model for deep search\n",
    "    model_settings=ModelSettings(temperature=0.7, tool_choice=\"required\"),\n",
    ")\n",
    "\n",
    "# Deep Search Claude Agent  \n",
    "DEEP_CLAUDE_INSTRUCTIONS = f\"\"\"You are an advanced Claude researcher performing deep analytical research.\n",
    "Execute {DEEP_SEARCH_ITERATIONS} research cycles with systematic refinement.\n",
    "\n",
    "Research methodology:\n",
    "1. Systematic information gathering\n",
    "2. Critical source evaluation\n",
    "3. Multi-angle analysis\n",
    "4. Gap identification and targeted follow-up\n",
    "5. Comprehensive synthesis\n",
    "\n",
    "Emphasize critical thinking, source credibility, logical reasoning, and practical implications.\"\"\"\n",
    "\n",
    "# Note: This would require Claude API integration\n",
    "# deep_claude_agent = Agent(\n",
    "#     name=\"DeepClaudeAgent\", \n",
    "#     instructions=DEEP_CLAUDE_INSTRUCTIONS,\n",
    "#     tools=[WebSearchTool(search_context_size=\"large\")],\n",
    "#     model=\"claude-3-5-sonnet-20241022\",  # Would need Claude API setup\n",
    "#     model_settings=ModelSettings(temperature=0.6),\n",
    "# )\n",
    "\n",
    "# Comparative Deep Search Agent\n",
    "COMPARATIVE_INSTRUCTIONS = f\"\"\"You are a comparative research specialist. \n",
    "Compare and contrast findings from multiple AI perspectives (ChatGPT vs Claude approaches).\n",
    "\n",
    "Analysis framework:\n",
    "1. Identify convergent findings (high confidence)\n",
    "2. Highlight divergent perspectives (requires investigation)\n",
    "3. Evaluate evidence quality and source reliability\n",
    "4. Synthesize balanced conclusions\n",
    "5. Recommend areas for further research\n",
    "\n",
    "Provide meta-analysis of research quality and reliability.\"\"\"\n",
    "\n",
    "comparative_agent = Agent(\n",
    "    name=\"ComparativeAgent\",\n",
    "    instructions=COMPARATIVE_INSTRUCTIONS,\n",
    "    model=\"gpt-4o\",\n",
    "    output_type=ReportData,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Agentic search agents initialized!\")\n",
    "print(f\"üìä Configuration: {MAX_STRATEGIC_SEARCHES} searches, {SEARCH_CONTEXT_SIZE} context, {REPORT_MIN_LENGTH}+ word reports\")\n",
    "print(\"üî• Deep search agents ENABLED and ready!\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# CELL 10: AGENTIC SEARCH EXECUTION\n",
    "# ============================================\n",
    "\n",
    "async def perform_agentic_search(query: str):\n",
    "    \"\"\"\n",
    "    Enhanced agentic search pipeline.\n",
    "    \n",
    "    Note: This uses OpenAI Agents SDK which has its own built-in web search capabilities,\n",
    "    separate from the explorer search engines above.\n",
    "    \"\"\"\n",
    "    print(f\"ü§ñ Starting Agentic Search for: {query}\")\n",
    "    \n",
    "    with trace(\"Agentic Search Pipeline\"):\n",
    "        # Plan searches\n",
    "        print(\"üìã Planning strategic searches...\")\n",
    "        result = await Runner.run(planner_agent, f\"Query: {query}\")\n",
    "        search_plan = result.final_output\n",
    "        \n",
    "        # Execute searches\n",
    "        print(f\"üåê Executing {len(search_plan.searches)} strategic searches...\")\n",
    "        search_tasks = []\n",
    "        for item in search_plan.searches:\n",
    "            input_text = f\"Search term: {item.query}\\nReason: {item.reason}\"\n",
    "            search_tasks.append(Runner.run(search_agent, input_text))\n",
    "        \n",
    "        search_results = await asyncio.gather(*search_tasks)\n",
    "        search_summaries = [result.final_output for result in search_results]\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        print(\"üìù Synthesizing comprehensive report...\")\n",
    "        input_text = f\"Original query: {query}\\nResearch findings: {search_summaries}\"\n",
    "        report_result = await Runner.run(writer_agent, input_text)\n",
    "        \n",
    "        print(\"‚úÖ Agentic search completed\")\n",
    "        return report_result.final_output, search_plan, search_summaries\n",
    "\n",
    "# Helper functions for agentic search components\n",
    "async def plan_searches(query: str):\n",
    "    \"\"\"Plan strategic searches for the query\"\"\"\n",
    "    result = await Runner.run(planner_agent, f\"Query: {query}\")\n",
    "    return result.final_output\n",
    "\n",
    "async def perform_searches(search_plan):\n",
    "    \"\"\"Execute planned searches\"\"\"\n",
    "    search_tasks = []\n",
    "    for item in search_plan.searches:\n",
    "        input_text = f\"Search term: {item.query}\\nReason: {item.reason}\"\n",
    "        search_tasks.append(Runner.run(search_agent, input_text))\n",
    "    \n",
    "    search_results = await asyncio.gather(*search_tasks)\n",
    "    return [result.final_output for result in search_results]\n",
    "\n",
    "async def write_report(query: str, search_results):\n",
    "    \"\"\"Generate comprehensive report from search results\"\"\"\n",
    "    input_text = f\"Original query: {query}\\nResearch findings: {search_results}\"\n",
    "    report_result = await Runner.run(writer_agent, input_text)\n",
    "    return report_result.final_output\n",
    "\n",
    "print(\"‚úÖ Agentic search execution functions loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 10B: DEEP SEARCH EXECUTION (NOW ENABLED!)\n",
    "# ============================================\n",
    "\n",
    "async def perform_deep_chatgpt_search(query: str):\n",
    "    \"\"\"\n",
    "    Perform deep iterative search using ChatGPT with progressive refinement\n",
    "    \"\"\"\n",
    "    print(f\"üß† Starting Deep ChatGPT Search for: {query}\")\n",
    "    \n",
    "    with trace(\"Deep ChatGPT Search Pipeline\"):\n",
    "        search_history = []\n",
    "        refined_insights = []\n",
    "        \n",
    "        for iteration in range(DEEP_SEARCH_ITERATIONS):\n",
    "            print(f\"üîÑ ChatGPT Iteration {iteration + 1}/{DEEP_SEARCH_ITERATIONS}\")\n",
    "            \n",
    "            # Build context from previous iterations\n",
    "            context = f\"Query: {query}\\n\"\n",
    "            if search_history:\n",
    "                context += f\"Previous findings: {search_history}\\n\"\n",
    "            context += f\"Focus for iteration {iteration + 1}: Identify gaps and explore new angles\"\n",
    "            \n",
    "            # Perform search with context\n",
    "            result = await Runner.run(deep_chatgpt_agent, context)\n",
    "            search_summary = result.final_output\n",
    "            \n",
    "            search_history.append({\n",
    "                \"iteration\": iteration + 1,\n",
    "                \"findings\": search_summary,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úì Iteration {iteration + 1} completed\")\n",
    "            \n",
    "            # Brief pause between iterations\n",
    "            if iteration < DEEP_SEARCH_ITERATIONS - 1:\n",
    "                await asyncio.sleep(2)\n",
    "        \n",
    "        # Synthesize final report\n",
    "        print(\"üìù Synthesizing ChatGPT deep research findings...\")\n",
    "        synthesis_input = f\"Deep research query: {query}\\nIterative findings: {search_history}\\nCreate comprehensive final report.\"\n",
    "        final_result = await Runner.run(writer_agent, synthesis_input)\n",
    "        \n",
    "        return {\n",
    "            \"type\": \"deep_chatgpt\",\n",
    "            \"query\": query,\n",
    "            \"iterations\": DEEP_SEARCH_ITERATIONS,\n",
    "            \"search_history\": search_history,\n",
    "            \"final_report\": final_result.final_output,\n",
    "            \"completed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "async def perform_deep_claude_search(query: str):\n",
    "    \"\"\"\n",
    "    Perform deep systematic search using Claude methodology\n",
    "    Note: This requires Claude API integration - currently simulated with GPT\n",
    "    \"\"\"\n",
    "    print(f\"üé≠ Starting Deep Claude Search for: {query}\")\n",
    "    \n",
    "    # Since we don't have Claude agent set up, we'll simulate with a Claude-style approach using GPT\n",
    "    claude_style_instructions = f\"\"\"You are simulating Claude's analytical approach. \n",
    "    Perform systematic research with emphasis on:\n",
    "    - Critical evaluation of sources\n",
    "    - Logical reasoning chains  \n",
    "    - Multiple perspective analysis\n",
    "    - Methodical gap identification\n",
    "    - Conservative confidence levels\n",
    "    \n",
    "    Query: {query}\n",
    "    Iterations: {DEEP_SEARCH_ITERATIONS}\"\"\"\n",
    "    \n",
    "    with trace(\"Deep Claude-Style Search Pipeline\"):\n",
    "        analysis_phases = []\n",
    "        \n",
    "        for phase in range(DEEP_SEARCH_ITERATIONS):\n",
    "            print(f\"üîç Claude-style Analysis Phase {phase + 1}/{DEEP_SEARCH_ITERATIONS}\")\n",
    "            \n",
    "            phase_context = f\"{claude_style_instructions}\\nPhase {phase + 1}: \"\n",
    "            if phase == 0:\n",
    "                phase_context += \"Initial comprehensive search and source gathering\"\n",
    "            elif phase == 1:\n",
    "                phase_context += \"Critical evaluation and credibility assessment\"\n",
    "            elif phase == 2:\n",
    "                phase_context += \"Multi-angle analysis and perspective gathering\"\n",
    "            elif phase == 3:\n",
    "                phase_context += \"Gap identification and targeted investigation\"\n",
    "            else:\n",
    "                phase_context += \"Synthesis and final verification\"\n",
    "            \n",
    "            # Use search agent with Claude-style prompting\n",
    "            result = await Runner.run(search_agent, phase_context)\n",
    "            phase_findings = result.final_output\n",
    "            \n",
    "            analysis_phases.append({\n",
    "                \"phase\": phase + 1,\n",
    "                \"focus\": phase_context.split(\": \")[-1],\n",
    "                \"findings\": phase_findings,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úì Phase {phase + 1} completed\")\n",
    "            await asyncio.sleep(1)\n",
    "        \n",
    "        # Final synthesis with Claude-style rigor\n",
    "        print(\"üìä Performing Claude-style systematic synthesis...\")\n",
    "        synthesis_prompt = f\"\"\"Synthesize research with Claude-style analytical rigor:\n",
    "        \n",
    "        Query: {query}\n",
    "        Research phases: {analysis_phases}\n",
    "        \n",
    "        Apply systematic evaluation:\n",
    "        1. Evidence quality assessment\n",
    "        2. Logical consistency check\n",
    "        3. Confidence level assignment\n",
    "        4. Alternative perspective consideration\n",
    "        5. Practical implication analysis\"\"\"\n",
    "        \n",
    "        final_result = await Runner.run(writer_agent, synthesis_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"type\": \"deep_claude_style\", \n",
    "            \"query\": query,\n",
    "            \"phases\": DEEP_SEARCH_ITERATIONS,\n",
    "            \"analysis_phases\": analysis_phases,\n",
    "            \"final_report\": final_result.final_output,\n",
    "            \"completed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "async def perform_comparative_deep_search(query: str):\n",
    "    \"\"\"\n",
    "    Perform both ChatGPT and Claude-style deep searches, then compare findings\n",
    "    \"\"\"\n",
    "    print(f\"‚öñÔ∏è Starting Comparative Deep Search for: {query}\")\n",
    "    \n",
    "    with trace(\"Comparative Deep Search Pipeline\"):\n",
    "        # Run both deep searches in parallel\n",
    "        print(\"üîÑ Running ChatGPT and Claude-style searches in parallel...\")\n",
    "        chatgpt_task = asyncio.create_task(perform_deep_chatgpt_search(query))\n",
    "        claude_task = asyncio.create_task(perform_deep_claude_search(query))\n",
    "        \n",
    "        chatgpt_results, claude_results = await asyncio.gather(chatgpt_task, claude_task)\n",
    "        \n",
    "        # Comparative analysis\n",
    "        print(\"üìä Performing comparative analysis...\")\n",
    "        comparison_prompt = f\"\"\"Perform comparative analysis of two AI research approaches:\n",
    "        \n",
    "        Query: {query}\n",
    "        \n",
    "        ChatGPT Approach Results:\n",
    "        {chatgpt_results['final_report'].markdown_report}\n",
    "        \n",
    "        Claude-Style Approach Results:  \n",
    "        {claude_results['final_report'].markdown_report}\n",
    "        \n",
    "        Compare and analyze:\n",
    "        1. Convergent findings (high confidence conclusions)\n",
    "        2. Divergent perspectives (areas of disagreement)\n",
    "        3. Methodology differences\n",
    "        4. Evidence quality and source coverage\n",
    "        5. Practical implications and recommendations\n",
    "        6. Areas requiring further investigation\n",
    "        \n",
    "        Provide meta-analysis of research quality and synthesized recommendations.\"\"\"\n",
    "        \n",
    "        comparative_result = await Runner.run(comparative_agent, comparison_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"type\": \"comparative_deep_search\",\n",
    "            \"query\": query,\n",
    "            \"chatgpt_results\": chatgpt_results,\n",
    "            \"claude_results\": claude_results,\n",
    "            \"comparative_analysis\": comparative_result.final_output,\n",
    "            \"total_iterations\": DEEP_SEARCH_ITERATIONS * 2,\n",
    "            \"completed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Deep search execution functions loaded!\")\n",
    "print(f\"üîß Deep search configuration: {DEEP_SEARCH_ITERATIONS} iterations, refinement={'enabled' if DEEP_SEARCH_REFINEMENT else 'disabled'}\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# CELL 11: UNIVERSAL JSON EVALUATOR SYSTEM\n",
    "# ============================================\n",
    "\n",
    "def extract_content_from_json(json_data: Dict, query: str) -> Dict:\n",
    "    \"\"\"Universal content extractor that handles any JSON structure\"\"\"\n",
    "    extracted = {\n",
    "        \"query\": query,\n",
    "        \"content_type\": \"unknown\",\n",
    "        \"text_content\": \"\",\n",
    "        \"structured_data\": {},\n",
    "        \"source_info\": {}\n",
    "    }\n",
    "    \n",
    "    # Detect content type and extract accordingly\n",
    "    if \"agentic_results\" in json_data and \"explorer_results\" in json_data:\n",
    "        # Combined results\n",
    "        extracted[\"content_type\"] = \"combined\"\n",
    "        \n",
    "        # Extract agentic content\n",
    "        if \"markdown_report\" in json_data.get(\"agentic_results\", {}):\n",
    "            extracted[\"text_content\"] += json_data[\"agentic_results\"][\"markdown_report\"]\n",
    "        \n",
    "        # Extract explorer summary\n",
    "        explorer_results = json_data.get(\"explorer_results\", [])\n",
    "        scraped_texts = []\n",
    "        for item in explorer_results:\n",
    "            if isinstance(item, dict) and \"text\" in item and not item.get(\"error\"):\n",
    "                scraped_texts.append(item[\"text\"][:500])  # First 500 chars\n",
    "        \n",
    "        if scraped_texts:\n",
    "            extracted[\"text_content\"] += f\"\\n\\n## Explorer Findings:\\n\" + \"\\n\".join(scraped_texts)\n",
    "        \n",
    "        extracted[\"structured_data\"] = {\n",
    "            \"agentic_summary\": json_data.get(\"agentic_results\", {}).get(\"short_summary\", \"\"),\n",
    "            \"explorer_count\": len(explorer_results),\n",
    "            \"sources_scraped\": len([item for item in explorer_results if \"url\" in item])\n",
    "        }\n",
    "    \n",
    "    elif \"report\" in json_data and \"markdown_report\" in json_data[\"report\"]:\n",
    "        # Agentic-only results\n",
    "        extracted[\"content_type\"] = \"agentic\"\n",
    "        extracted[\"text_content\"] = json_data[\"report\"][\"markdown_report\"]\n",
    "        extracted[\"structured_data\"] = {\n",
    "            \"summary\": json_data[\"report\"].get(\"short_summary\", \"\"),\n",
    "            \"follow_up_questions\": json_data[\"report\"].get(\"follow_up_questions\", [])\n",
    "        }\n",
    "    \n",
    "    elif isinstance(json_data, list) or \"data\" in json_data:\n",
    "        # Explorer-only results\n",
    "        extracted[\"content_type\"] = \"explorer\"\n",
    "        \n",
    "        # Handle list format or nested data\n",
    "        data_list = json_data if isinstance(json_data, list) else json_data.get(\"data\", [])\n",
    "        \n",
    "        explorer_texts = []\n",
    "        source_count = 0\n",
    "        \n",
    "        for item in data_list:\n",
    "            if isinstance(item, dict):\n",
    "                if \"text\" in item and not item.get(\"error\", False):\n",
    "                    explorer_texts.append(f\"Source: {item.get('url', 'Unknown')}\\n{item['text'][:800]}\")\n",
    "                    source_count += 1\n",
    "                elif \"data\" in item:\n",
    "                    # Handle nested structures like Google API results\n",
    "                    source_count += 1\n",
    "        \n",
    "        extracted[\"text_content\"] = \"\\n\\n---\\n\\n\".join(explorer_texts[:5])  # Limit to 5 sources\n",
    "        extracted[\"structured_data\"] = {\n",
    "            \"total_sources\": source_count,\n",
    "            \"scraped_sources\": len([item for item in data_list if isinstance(item, dict) and \"text\" in item])\n",
    "        }\n",
    "    \n",
    "    return extracted\n",
    "\n",
    "async def evaluate_with_gpt(content: Dict) -> DetailedEvaluation:\n",
    "    \"\"\"Enhanced GPT evaluation with retry logic and error handling\"\"\"\n",
    "    evaluation_prompt = f\"\"\"You are an expert research evaluator. Evaluate this research content:\n",
    "\n",
    "Query: {content['query']}\n",
    "Content Type: {content['content_type']}\n",
    "\n",
    "Research Content:\n",
    "{content['text_content'][:4000]}  # Limit for token constraints\n",
    "\n",
    "Additional Context:\n",
    "{json.dumps(content['structured_data'], indent=2)}\n",
    "\n",
    "Evaluate based on:\n",
    "1. Accuracy - Factual correctness and reliability\n",
    "2. Completeness - Coverage of the topic\n",
    "3. Relevance - Direct relationship to the query\n",
    "4. Clarity - Organization and readability  \n",
    "5. Depth - Level of insight and analysis\n",
    "\n",
    "Consider the content type when evaluating. Explorer results should be judged on breadth and source diversity, while agentic results should be judged on synthesis and insight quality.\"\"\"\n",
    "\n",
    "    max_retries = 3\n",
    "    retry_delay = 2\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"ü§ñ GPT-4 evaluation attempt {attempt + 1}/{max_retries}...\")\n",
    "            \n",
    "            response = openai_client.beta.chat.completions.parse(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a critical research evaluator. Provide detailed, constructive assessment.\"},\n",
    "                    {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "                ],\n",
    "                response_format=DetailedEvaluation,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.parsed\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            print(f\"‚ö†Ô∏è GPT-4 attempt {attempt + 1} failed: {error_str}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"üîÑ Retrying GPT-4 evaluation in {retry_delay} seconds...\")\n",
    "                await asyncio.sleep(retry_delay)\n",
    "                retry_delay *= 2\n",
    "                continue\n",
    "            else:\n",
    "                print(\"üö´ GPT-4 persistently failing, using fallback evaluation...\")\n",
    "                return create_fallback_gpt_evaluation(content)\n",
    "    \n",
    "    return create_fallback_gpt_evaluation(content)\n",
    "\n",
    "def create_fallback_gpt_evaluation(content: Dict) -> DetailedEvaluation:\n",
    "    \"\"\"Create a fallback evaluation when GPT-4 API is unavailable\"\"\"\n",
    "    print(\"üîß Generating fallback GPT-4 evaluation...\")\n",
    "    \n",
    "    text_length = len(content.get('text_content', ''))\n",
    "    \n",
    "    # Heuristic scoring\n",
    "    accuracy_score = 6.5\n",
    "    completeness_score = min(8.5, 5.0 + (text_length / 900))\n",
    "    relevance_score = 7.5\n",
    "    clarity_score = 7.0\n",
    "    depth_score = min(7.5, 4.0 + (text_length / 700))\n",
    "    \n",
    "    overall_score = (accuracy_score + completeness_score + relevance_score + clarity_score + depth_score) / 5\n",
    "    \n",
    "    return DetailedEvaluation(\n",
    "        criteria_scores=EvaluationCriteria(\n",
    "            accuracy_score=accuracy_score,\n",
    "            completeness_score=completeness_score,\n",
    "            relevance_score=relevance_score,\n",
    "            clarity_score=clarity_score,\n",
    "            depth_score=depth_score\n",
    "        ),\n",
    "        overall_score=round(overall_score, 1),\n",
    "        strengths=[\n",
    "            \"Comprehensive information gathering\",\n",
    "            \"Well-organized content structure\",\n",
    "            \"Good coverage of query topics\"\n",
    "        ],\n",
    "        weaknesses=[\n",
    "            \"Could not verify with GPT-4 API\",\n",
    "            \"Limited by automated evaluation\",\n",
    "            \"May need human expert review\"\n",
    "        ],\n",
    "        missing_aspects=[\n",
    "            \"Expert domain knowledge validation\",\n",
    "            \"Advanced fact-checking\",\n",
    "            \"Nuanced analysis capabilities\"\n",
    "        ],\n",
    "        recommendations=[\n",
    "            \"Re-evaluate when API is available\",\n",
    "            \"Consider manual expert review\",\n",
    "            \"Validate key claims independently\"\n",
    "        ],\n",
    "        confidence_level=\"Medium\"\n",
    "    )\n",
    "\n",
    "async def evaluate_with_claude(content: Dict) -> DetailedEvaluation:\n",
    "    \"\"\"Enhanced Claude evaluation with retry logic and fallback\"\"\"\n",
    "    evaluation_prompt = f\"\"\"Evaluate this research content as an expert evaluator:\n",
    "\n",
    "Query: {content['query']}\n",
    "Content Type: {content['content_type']}\n",
    "\n",
    "Research Content:\n",
    "{content['text_content'][:4000]}\n",
    "\n",
    "Additional Context:\n",
    "{json.dumps(content['structured_data'], indent=2)}\n",
    "\n",
    "Evaluate on accuracy, completeness, relevance, clarity, and depth (0-10 each).\n",
    "Adapt your evaluation criteria based on content type.\n",
    "\n",
    "Return in JSON format:\n",
    "{{\n",
    "    \"criteria_scores\": {{\n",
    "        \"accuracy_score\": <0-10>,\n",
    "        \"completeness_score\": <0-10>,\n",
    "        \"relevance_score\": <0-10>,\n",
    "        \"clarity_score\": <0-10>,\n",
    "        \"depth_score\": <0-10>\n",
    "    }},\n",
    "    \"overall_score\": <0-10>,\n",
    "    \"strengths\": [\"strength1\", \"strength2\", ...],\n",
    "    \"weaknesses\": [\"weakness1\", \"weakness2\", ...],\n",
    "    \"missing_aspects\": [\"aspect1\", \"aspect2\", ...],\n",
    "    \"recommendations\": [\"recommendation1\", \"recommendation2\", ...],\n",
    "    \"confidence_level\": \"High/Medium/Low\"\n",
    "}}\"\"\"\n",
    "\n",
    "    max_retries = 3\n",
    "    retry_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"üß† Claude evaluation attempt {attempt + 1}/{max_retries}...\")\n",
    "            \n",
    "            response = claude_client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=2000,\n",
    "                temperature=0.3,\n",
    "                messages=[{\"role\": \"user\", \"content\": evaluation_prompt}]\n",
    "            )\n",
    "            \n",
    "            evaluation_json = json.loads(response.content[0].text)\n",
    "            return DetailedEvaluation(**evaluation_json)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            print(f\"‚ö†Ô∏è Claude attempt {attempt + 1} failed: {error_str}\")\n",
    "            \n",
    "            # Check if it's an overload error\n",
    "            if \"overloaded\" in error_str.lower() or \"529\" in error_str:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"üîÑ Claude overloaded, retrying in {retry_delay} seconds...\")\n",
    "                    await asyncio.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Exponential backoff\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"üö´ Claude persistently overloaded, using fallback evaluation...\")\n",
    "                    return create_fallback_claude_evaluation(content)\n",
    "            else:\n",
    "                # For other errors, try fallback immediately\n",
    "                print(f\"üö´ Claude error: {error_str}, using fallback evaluation...\")\n",
    "                return create_fallback_claude_evaluation(content)\n",
    "    \n",
    "    # If all retries failed, use fallback\n",
    "    return create_fallback_claude_evaluation(content)\n",
    "\n",
    "def create_fallback_claude_evaluation(content: Dict) -> DetailedEvaluation:\n",
    "    \"\"\"Create a fallback evaluation when Claude API is unavailable\"\"\"\n",
    "    print(\"üîß Generating fallback Claude evaluation...\")\n",
    "    \n",
    "    # Create reasonable fallback scores based on content analysis\n",
    "    text_length = len(content.get('text_content', ''))\n",
    "    \n",
    "    # Basic heuristic scoring\n",
    "    completeness_score = min(8.0, 4.0 + (text_length / 1000))  # Longer content = more complete\n",
    "    relevance_score = 7.0  # Assume reasonable relevance\n",
    "    clarity_score = 6.5    # Neutral clarity score\n",
    "    accuracy_score = 6.0   # Conservative accuracy score\n",
    "    depth_score = min(7.0, 3.0 + (text_length / 800))\n",
    "    \n",
    "    overall_score = (completeness_score + relevance_score + clarity_score + accuracy_score + depth_score) / 5\n",
    "    \n",
    "    return DetailedEvaluation(\n",
    "        criteria_scores=EvaluationCriteria(\n",
    "            accuracy_score=accuracy_score,\n",
    "            completeness_score=completeness_score,\n",
    "            relevance_score=relevance_score,\n",
    "            clarity_score=clarity_score,\n",
    "            depth_score=depth_score\n",
    "        ),\n",
    "        overall_score=round(overall_score, 1),\n",
    "        strengths=[\n",
    "            \"Content covers multiple aspects of the query\",\n",
    "            \"Information appears well-structured\",\n",
    "            \"Reasonable depth of analysis\"\n",
    "        ],\n",
    "        weaknesses=[\n",
    "            \"Could not verify accuracy with Claude API\",\n",
    "            \"May benefit from additional source validation\",\n",
    "            \"Evaluation limited by API availability\"\n",
    "        ],\n",
    "        missing_aspects=[\n",
    "            \"Real-time fact verification\",\n",
    "            \"Cross-reference validation\",\n",
    "            \"Expert domain analysis\"\n",
    "        ],\n",
    "        recommendations=[\n",
    "            \"Verify key facts with additional sources\",\n",
    "            \"Consider expert review for technical content\",\n",
    "            \"Re-evaluate when Claude API is available\"\n",
    "        ],\n",
    "        confidence_level=\"Medium\"\n",
    "    )\n",
    "\n",
    "def calculate_consensus(gpt_eval: DetailedEvaluation, claude_eval: DetailedEvaluation) -> Dict:\n",
    "    \"\"\"Calculate consensus between evaluations with fallback handling\"\"\"\n",
    "    gpt_scores = gpt_eval.criteria_scores.dict()\n",
    "    claude_scores = claude_eval.criteria_scores.dict()\n",
    "    \n",
    "    score_differences = {}\n",
    "    total_diff = 0\n",
    "    \n",
    "    for criterion, gpt_score in gpt_scores.items():\n",
    "        claude_score = claude_scores[criterion]\n",
    "        diff = abs(gpt_score - claude_score)\n",
    "        score_differences[criterion] = diff\n",
    "        total_diff += diff\n",
    "    \n",
    "    avg_difference = total_diff / len(score_differences)\n",
    "    consensus_score = max(0, 10 - avg_difference)\n",
    "    \n",
    "    # Combine recommendations\n",
    "    all_recommendations = list(set(gpt_eval.recommendations + claude_eval.recommendations))\n",
    "    \n",
    "    # Check if either evaluation was a fallback\n",
    "    gpt_fallback = gpt_eval.confidence_level == \"Medium\" and \"Could not verify with GPT-4 API\" in gpt_eval.weaknesses\n",
    "    claude_fallback = claude_eval.confidence_level == \"Medium\" and \"Could not verify accuracy with Claude API\" in claude_eval.weaknesses\n",
    "    \n",
    "    evaluation_notes = []\n",
    "    if gpt_fallback:\n",
    "        evaluation_notes.append(\"GPT-4 evaluation used fallback due to API issues\")\n",
    "    if claude_fallback:\n",
    "        evaluation_notes.append(\"Claude evaluation used fallback due to API overload\")\n",
    "    \n",
    "    return {\n",
    "        \"consensus_score\": round(consensus_score, 2),\n",
    "        \"final_recommendations\": all_recommendations,\n",
    "        \"score_differences\": score_differences,\n",
    "        \"avg_difference\": round(avg_difference, 2),\n",
    "        \"evaluation_notes\": evaluation_notes,\n",
    "        \"api_fallbacks_used\": gpt_fallback or claude_fallback\n",
    "    }\n",
    "\n",
    "async def universal_json_evaluator(filepath: str, query: str) -> UniversalEvaluation:\n",
    "    \"\"\"Universal evaluator that can process any JSON research file\"\"\"\n",
    "    print(f\"üìä Loading and evaluating: {filepath}\")\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        # Extract content using universal extractor\n",
    "        content = extract_content_from_json(json_data, query)\n",
    "        print(f\"üîç Detected content type: {content['content_type']}\")\n",
    "        \n",
    "        # Dual evaluation\n",
    "        print(\"ü§ñ Running GPT-4 evaluation...\")\n",
    "        gpt_eval = await evaluate_with_gpt(content)\n",
    "        \n",
    "        print(\"üß† Running Claude evaluation...\")\n",
    "        claude_eval = await evaluate_with_claude(content)\n",
    "        \n",
    "        # Build consensus\n",
    "        print(\"üîÑ Building evaluation consensus...\")\n",
    "        consensus = calculate_consensus(gpt_eval, claude_eval)\n",
    "        \n",
    "        # Create summary\n",
    "        summary = f\"\"\"\n",
    "# Universal Research Evaluation Report\n",
    "\n",
    "**Query**: {query}  \n",
    "**Content Type**: {content['content_type'].title()}  \n",
    "**File**: {filepath}\n",
    "\n",
    "## Evaluation Scores\n",
    "- **GPT-4 Overall**: {gpt_eval.overall_score}/10\n",
    "- **Claude Overall**: {claude_eval.overall_score}/10  \n",
    "- **Consensus Score**: {consensus['consensus_score']}/10\n",
    "- **Agreement Level**: {5 - len([d for d in consensus['score_differences'].values() if d > 2])}/5 criteria\n",
    "\n",
    "## Content Analysis\n",
    "{json.dumps(content['structured_data'], indent=2)}\n",
    "\n",
    "## Quality Assessment\n",
    "{'‚úÖ **HIGH QUALITY** - Exceeds standards' if consensus['consensus_score'] >= 7.5 else \n",
    " '‚ö†Ô∏è **MODERATE QUALITY** - Meets basic standards' if consensus['consensus_score'] >= 6.0 else\n",
    " '‚ùå **NEEDS IMPROVEMENT** - Below standards'}\n",
    "\"\"\"\n",
    "        \n",
    "        return UniversalEvaluation(\n",
    "            query=query,\n",
    "            content_type=content['content_type'],\n",
    "            gpt_evaluation=gpt_eval,\n",
    "            claude_evaluation=claude_eval,\n",
    "            consensus_score=consensus['consensus_score'],\n",
    "            final_recommendations=consensus['final_recommendations'],\n",
    "            evaluation_summary=summary\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Evaluation failed: {str(e)}\")\n",
    "\n",
    "print(\"‚úÖ Universal JSON evaluator loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# API ERROR HANDLING UTILITIES\n",
    "# ============================================\n",
    "\n",
    "def get_api_status_help():\n",
    "    \"\"\"Get help text for API issues\"\"\"\n",
    "    return \"\"\"\n",
    "## üîß API Troubleshooting Guide\n",
    "\n",
    "### Claude API Overload (Error 529)\n",
    "**What it means**: Claude's servers are temporarily overloaded\n",
    "**Solutions**:\n",
    "1. ‚è≥ **Wait and retry** - Usually resolves in 1-5 minutes\n",
    "2. üîÑ **Use fallback evaluation** - System automatically provides heuristic scoring\n",
    "3. üìä **Focus on GPT-4 scores** - Still get partial evaluation\n",
    "4. ‚öôÔ∏è **Reduce request frequency** - Space out evaluations\n",
    "\n",
    "### GPT-4 API Issues\n",
    "**Common causes**: Rate limits, server issues, API key problems\n",
    "**Solutions**:\n",
    "1. üîë **Check API key** - Ensure valid OpenAI API key\n",
    "2. üí≥ **Check billing** - Ensure account has credits\n",
    "3. ‚è±Ô∏è **Rate limiting** - Reduce request frequency\n",
    "4. üîÑ **Use fallback** - System provides alternative scoring\n",
    "\n",
    "### When Fallback Evaluations Are Used\n",
    "**Fallback scoring uses**:\n",
    "- Content length analysis\n",
    "- Structural assessment  \n",
    "- Heuristic quality metrics\n",
    "- Conservative confidence levels\n",
    "\n",
    "**Limitations**:\n",
    "- Less nuanced than AI evaluation\n",
    "- Cannot verify factual accuracy\n",
    "- Limited domain expertise\n",
    "- Medium confidence ratings\n",
    "\n",
    "### üí° Best Practices\n",
    "1. **Monitor API status** - Check provider status pages\n",
    "2. **Use off-peak hours** - Better availability\n",
    "3. **Implement delays** - Space out API calls\n",
    "4. **Have fallbacks** - System handles gracefully\n",
    "5. **Re-evaluate later** - Try again when APIs recover\n",
    "\n",
    "### üîÑ Manual Re-evaluation\n",
    "```python\n",
    "# Re-run evaluation when APIs are available\n",
    "evaluation = await universal_json_evaluator(\"your_file.json\", \"your_query\")\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "async def test_api_availability():\n",
    "    \"\"\"Test if APIs are currently available\"\"\"\n",
    "    api_status = {\n",
    "        \"gpt4_available\": False,\n",
    "        \"claude_available\": False,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Test GPT-4\n",
    "    try:\n",
    "        test_response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # Use cheaper model for testing\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Test\"}],\n",
    "            max_tokens=1,\n",
    "            temperature=0\n",
    "        )\n",
    "        api_status[\"gpt4_available\"] = True\n",
    "        print(\"‚úÖ GPT-4 API: Available\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GPT-4 API: Unavailable - {str(e)[:50]}...\")\n",
    "    \n",
    "    # Test Claude\n",
    "    try:\n",
    "        test_response = claude_client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=1,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "        )\n",
    "        api_status[\"claude_available\"] = True\n",
    "        print(\"‚úÖ Claude API: Available\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Claude API: Unavailable - {str(e)[:50]}...\")\n",
    "    \n",
    "    return api_status\n",
    "\n",
    "print(\"‚úÖ API error handling utilities loaded!\")\n",
    "print(\"üí° Use test_api_availability() to check current API status\")\n",
    "print(\"üí° Fallback evaluations activate automatically when needed\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# CELL 12: EVALUATION ORCHESTRATION\n",
    "# ============================================\n",
    "\n",
    "async def finalize_research_evaluation(query: str, report_content: str) -> UniversalEvaluation:\n",
    "    \"\"\"Finalize research with dual-model evaluation\"\"\"\n",
    "    \n",
    "    # Create content structure for evaluation\n",
    "    content = {\n",
    "        \"query\": query,\n",
    "        \"content_type\": \"agentic\",\n",
    "        \"text_content\": report_content,\n",
    "        \"structured_data\": {\n",
    "            \"report_length\": len(report_content),\n",
    "            \"generated_at\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"source_info\": {}\n",
    "    }\n",
    "    \n",
    "    # Dual evaluation\n",
    "    print(\"ü§ñ Running GPT-4 evaluation...\")\n",
    "    gpt_eval = await evaluate_with_gpt(content)\n",
    "    \n",
    "    print(\"üß† Running Claude evaluation...\")\n",
    "    claude_eval = await evaluate_with_claude(content)\n",
    "    \n",
    "    # Build consensus\n",
    "    print(\"üîÑ Building evaluation consensus...\")\n",
    "    consensus = calculate_consensus(gpt_eval, claude_eval)\n",
    "    \n",
    "    # Create summary\n",
    "    summary = f\"\"\"\n",
    "# Research Evaluation Report\n",
    "\n",
    "**Query**: {query}  \n",
    "**Content Type**: Agentic Research Report\n",
    "**Report Length**: {len(report_content)} characters\n",
    "\n",
    "## Evaluation Scores\n",
    "- **GPT-4 Overall**: {gpt_eval.overall_score}/10\n",
    "- **Claude Overall**: {claude_eval.overall_score}/10  \n",
    "- **Consensus Score**: {consensus['consensus_score']}/10\n",
    "\n",
    "## Quality Assessment\n",
    "{'‚úÖ **HIGH QUALITY** - Exceeds standards' if consensus['consensus_score'] >= 7.5 else \n",
    " '‚ö†Ô∏è **MODERATE QUALITY** - Meets basic standards' if consensus['consensus_score'] >= 6.0 else\n",
    " '‚ùå **NEEDS IMPROVEMENT** - Below standards'}\n",
    "\"\"\"\n",
    "    \n",
    "    return UniversalEvaluation(\n",
    "        query=query,\n",
    "        content_type=\"agentic\",\n",
    "        gpt_evaluation=gpt_eval,\n",
    "        claude_evaluation=claude_eval,\n",
    "        consensus_score=consensus['consensus_score'],\n",
    "        final_recommendations=consensus['final_recommendations'],\n",
    "        evaluation_summary=summary\n",
    "    )\n",
    "\n",
    "def create_evaluation_report(evaluation: UniversalEvaluation) -> str:\n",
    "    \"\"\"Create formatted evaluation report with API status information\"\"\"\n",
    "    \n",
    "    gpt_scores = evaluation.gpt_evaluation.criteria_scores\n",
    "    claude_scores = evaluation.claude_evaluation.criteria_scores\n",
    "    \n",
    "    # Check for API fallbacks\n",
    "    gpt_fallback = evaluation.gpt_evaluation.confidence_level == \"Medium\" and \"Could not verify with GPT-4 API\" in evaluation.gpt_evaluation.weaknesses\n",
    "    claude_fallback = evaluation.claude_evaluation.confidence_level == \"Medium\" and \"Could not verify accuracy with Claude API\" in evaluation.claude_evaluation.weaknesses\n",
    "    \n",
    "    api_status = \"\"\n",
    "    if gpt_fallback or claude_fallback:\n",
    "        api_status = \"\\n## ‚ö†Ô∏è API Status\\n\"\n",
    "        if gpt_fallback:\n",
    "            api_status += \"- **GPT-4**: Used fallback evaluation (API unavailable)\\n\"\n",
    "        else:\n",
    "            api_status += \"- **GPT-4**: ‚úÖ API evaluation successful\\n\"\n",
    "        if claude_fallback:\n",
    "            api_status += \"- **Claude**: Used fallback evaluation (API overloaded)\\n\"\n",
    "        else:\n",
    "            api_status += \"- **Claude**: ‚úÖ API evaluation successful\\n\"\n",
    "        api_status += \"\\n*Note: Fallback evaluations use heuristic scoring and may be less accurate.*\\n\"\n",
    "    \n",
    "    report = f\"\"\"\n",
    "# üìä Research Quality Evaluation\n",
    "\n",
    "## üéØ Query Analysis\n",
    "**Original Query**: {evaluation.query}\n",
    "**Content Type**: {evaluation.content_type.title()}\n",
    "{api_status}\n",
    "## üìà Evaluation Scores\n",
    "\n",
    "### GPT-4 Assessment {'(Fallback)' if gpt_fallback else ''}\n",
    "- **Overall Score**: {evaluation.gpt_evaluation.overall_score}/10\n",
    "- **Accuracy**: {gpt_scores.accuracy_score}/10\n",
    "- **Completeness**: {gpt_scores.completeness_score}/10  \n",
    "- **Relevance**: {gpt_scores.relevance_score}/10\n",
    "- **Clarity**: {gpt_scores.clarity_score}/10\n",
    "- **Depth**: {gpt_scores.depth_score}/10\n",
    "\n",
    "### Claude Assessment {'(Fallback)' if claude_fallback else ''}\n",
    "- **Overall Score**: {evaluation.claude_evaluation.overall_score}/10\n",
    "- **Accuracy**: {claude_scores.accuracy_score}/10\n",
    "- **Completeness**: {claude_scores.completeness_score}/10\n",
    "- **Relevance**: {claude_scores.relevance_score}/10  \n",
    "- **Clarity**: {claude_scores.clarity_score}/10\n",
    "- **Depth**: {claude_scores.depth_score}/10\n",
    "\n",
    "### ü§ù Consensus Analysis\n",
    "- **Consensus Score**: {evaluation.consensus_score}/10\n",
    "- **Quality Rating**: {'‚úÖ HIGH QUALITY' if evaluation.consensus_score >= 7.5 else '‚ö†Ô∏è MODERATE QUALITY' if evaluation.consensus_score >= 6.0 else '‚ùå NEEDS IMPROVEMENT'}\n",
    "{'- **Note**: Consensus may be affected by API fallbacks' if gpt_fallback or claude_fallback else ''}\n",
    "\n",
    "## üí™ Strengths\n",
    "{chr(10).join(f\"- {strength}\" for strength in evaluation.gpt_evaluation.strengths[:3])}\n",
    "\n",
    "## üîß Areas for Improvement  \n",
    "{chr(10).join(f\"- {weakness}\" for weakness in evaluation.gpt_evaluation.weaknesses[:3])}\n",
    "\n",
    "## üéØ Recommendations\n",
    "{chr(10).join(f\"- {rec}\" for rec in evaluation.final_recommendations[:5])}\n",
    "\n",
    "---\n",
    "*Evaluation completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "{'*Some evaluations used fallback methods due to API limitations*' if gpt_fallback or claude_fallback else ''}\n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "print(\"‚úÖ Evaluation orchestration functions loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 13: COMBINED SEARCH ORCHESTRATION\n",
    "# ============================================\n",
    "\n",
    "async def perform_combined_search(query: str) -> CombinedSearchResults:\n",
    "    \"\"\"\n",
    "    Perform both explorer and agentic search, then combine results\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Starting Combined Search for: {query}\")\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    \n",
    "    # Run both searches in parallel\n",
    "    print(\"‚ö° Running explorer and agentic searches in parallel...\")\n",
    "    \n",
    "    # Start both searches\n",
    "    explorer_task = asyncio.create_task(\n",
    "        asyncio.to_thread(lambda: perform_explorer_search(query, save_results=False))\n",
    "    )\n",
    "    agentic_task = asyncio.create_task(\n",
    "        perform_agentic_search(query)\n",
    "    )\n",
    "    \n",
    "    # Wait for both to complete\n",
    "    (explorer_results, explorer_metadata), (agentic_report, search_plan, search_summaries) = await asyncio.gather(\n",
    "        explorer_task, agentic_task\n",
    "    )\n",
    "    \n",
    "    # Create combined summary\n",
    "    explorer_sources = len([item for item in explorer_results if \"url\" in item])\n",
    "    agentic_searches = len(search_plan.searches) if search_plan else 0\n",
    "    \n",
    "    combined_summary = f\"\"\"\n",
    "# Combined Search Results for: {query}\n",
    "\n",
    "## Search Overview\n",
    "- **Timestamp**: {timestamp}\n",
    "- **Explorer Sources**: {explorer_sources} web pages scraped\n",
    "- **Agentic Searches**: {agentic_searches} strategic searches performed\n",
    "\n",
    "## Explorer Summary\n",
    "Scraped {explorer_sources} web pages from DuckDuckGo and Brave Search engines.\n",
    "\n",
    "## Agentic Summary\n",
    "{agentic_report.short_summary}\n",
    "\n",
    "## Integration\n",
    "This combined approach provides both raw web data (explorer) and synthesized insights (agentic) for comprehensive research coverage.\n",
    "\"\"\"\n",
    "    \n",
    "    # Save combined results\n",
    "    timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    combined_results = {\n",
    "        \"query\": query,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"explorer_results\": explorer_results,\n",
    "        \"explorer_metadata\": explorer_metadata,\n",
    "        \"agentic_results\": agentic_report.dict(),\n",
    "        \"combined_summary\": combined_summary\n",
    "    }\n",
    "    \n",
    "    filename = f\"combined_search_{timestamp_str}.json\"\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    with open(filepath, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(combined_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"üíæ Combined search results saved to: {filepath}\")\n",
    "    \n",
    "    return CombinedSearchResults(\n",
    "        query=query,\n",
    "        timestamp=timestamp,\n",
    "        explorer_results=explorer_results,\n",
    "        agentic_results=agentic_report,\n",
    "        combined_summary=combined_summary\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Combined search orchestration loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 14: JUPYTER EXECUTION INTERFACE\n",
    "# ============================================\n",
    "\n",
    "async def deep_search_with_evaluation_jupyter(query: str):\n",
    "    \"\"\"Jupyter-compatible version of deep search with evaluation\"\"\"\n",
    "    \n",
    "    with trace(\"Deep Search with Evaluation\"):\n",
    "        # Phase 1: Deep Search\n",
    "        print(\"=\" * 50)\n",
    "        print(\"üîç PHASE 1: DEEP SEARCH\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        print(\"\\nüìã Planning searches...\")\n",
    "        search_plan = await plan_searches(query)\n",
    "        \n",
    "        print(\"\\nüåê Performing web searches...\")\n",
    "        search_results = await perform_searches(search_plan)\n",
    "        \n",
    "        print(\"\\nüìù Writing comprehensive report...\")\n",
    "        report = await write_report(query, search_results)\n",
    "        \n",
    "        # Display initial report\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üìÑ INITIAL RESEARCH REPORT\")\n",
    "        print(\"=\" * 50)\n",
    "        display(Markdown(report.markdown_report))\n",
    "        \n",
    "        # Phase 2: Evaluation\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üî¨ PHASE 2: DUAL-MODEL EVALUATION\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Perform evaluation\n",
    "        final_eval = await finalize_research_evaluation(query, report.markdown_report)\n",
    "        \n",
    "        # Create and display evaluation report\n",
    "        evaluation_report = create_evaluation_report(final_eval)\n",
    "        display(Markdown(evaluation_report))\n",
    "        \n",
    "        # Save evaluation results\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"evaluation_{timestamp}.json\"\n",
    "        \n",
    "        evaluation_results = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query\": query,\n",
    "            \"gpt_overall_score\": final_eval.gpt_evaluation.overall_score,\n",
    "            \"claude_overall_score\": final_eval.claude_evaluation.overall_score,\n",
    "            \"consensus_score\": final_eval.consensus_score,\n",
    "            \"final_recommendations\": final_eval.final_recommendations,\n",
    "            \"report_summary\": report.short_summary,\n",
    "            \"follow_up_questions\": report.follow_up_questions,\n",
    "            \"full_report\": report.markdown_report\n",
    "        }\n",
    "        \n",
    "        # Save to DATA_DIR\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(evaluation_results, f, indent=2)\n",
    "        print(f\"\\nüíæ Evaluation saved to: {filepath}\")\n",
    "        \n",
    "        # Phase 3: Quality Gate\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üîß PHASE 3: QUALITY CHECK\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        quality_threshold = 7.5\n",
    "        meets_threshold = final_eval.consensus_score >= quality_threshold\n",
    "        \n",
    "        print(f\"\\n‚úÖ Research Quality: {'APPROVED' if meets_threshold else 'NEEDS IMPROVEMENT'}\")\n",
    "        print(f\"Consensus Score: {final_eval.consensus_score}/10\")\n",
    "        print(f\"Quality Threshold: {quality_threshold}/10\")\n",
    "        \n",
    "        if not meets_threshold:\n",
    "            print(\"\\n‚ö†Ô∏è Report needs improvement based on evaluation.\")\n",
    "            print(\"\\nTop 3 recommendations for improvement:\")\n",
    "            for i, rec in enumerate(final_eval.final_recommendations[:3], 1):\n",
    "                print(f\"  {i}. {rec}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"‚ú® DEEP SEARCH WITH EVALUATION COMPLETE\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        return {\n",
    "            \"report\": report,\n",
    "            \"evaluation\": final_eval,\n",
    "            \"quality_approved\": meets_threshold,\n",
    "            \"filepath\": filepath\n",
    "        }\n",
    "\n",
    "# Main execution function\n",
    "async def run_research(query: str):\n",
    "    \"\"\"Run research for a single query\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'#' * 60}\")\n",
    "        print(f\"# RESEARCHING: {query}\")\n",
    "        print(f\"{'#' * 60}\\n\")\n",
    "        \n",
    "        results = await deep_search_with_evaluation_jupyter(query)\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\nüìä Summary for '{query}':\")\n",
    "        print(f\"   - Report length: {len(results['report'].markdown_report)} characters\")\n",
    "        print(f\"   - GPT-4 Score: {results['evaluation'].gpt_evaluation.overall_score}/10\")\n",
    "        print(f\"   - Claude Score: {results['evaluation'].claude_evaluation.overall_score}/10\")\n",
    "        print(f\"   - Consensus: {results['evaluation'].consensus_score}/10\")\n",
    "        print(f\"   - Quality: {'‚úÖ Approved' if results['quality_approved'] else '‚ùå Needs Improvement'}\")\n",
    "        print(f\"   - Saved to: {results['filepath']}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing query '{query}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Alternative: Run combined search (explorer + agentic)\n",
    "async def run_combined_research(query: str):\n",
    "    \"\"\"Run combined explorer + agentic search\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'#' * 60}\")\n",
    "        print(f\"# COMBINED RESEARCH: {query}\")\n",
    "        print(f\"{'#' * 60}\\n\")\n",
    "        \n",
    "        results = await perform_combined_search(query)\n",
    "        \n",
    "        # Display results\n",
    "        display(Markdown(results.combined_summary))\n",
    "        display(Markdown(results.agentic_results.markdown_report))\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"combined_search_{timestamp}.json\"\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(results.dict(), f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüíæ Combined results saved to: {filepath}\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in combined research '{query}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ============================================\n",
    "# EXPLORER-ONLY EXECUTION FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "async def run_explorer_only_research(query: str):\n",
    "    \"\"\"Run explorer-only research with detailed JSON saving\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'#' * 60}\")\n",
    "        print(f\"# EXPLORER-ONLY RESEARCH: {query}\")\n",
    "        print(f\"{'#' * 60}\\n\")\n",
    "        \n",
    "        # Perform explorer search\n",
    "        explorer_results, explorer_metadata = perform_explorer_search(query, save_results=True)\n",
    "        \n",
    "        # Display results summary\n",
    "        print(f\"\\nüîç Explorer Research Summary:\")\n",
    "        print(f\"   - Total links found: {explorer_metadata['metadata']['total_links_found']}\")\n",
    "        print(f\"   - Pages scraped: {len(explorer_results)}\")\n",
    "        print(f\"   - Successful scrapes: {len([r for r in explorer_results if not r.get('error')])}\")\n",
    "        print(f\"   - Total text extracted: {sum(r.get('full_text_length', 0) for r in explorer_results if not r.get('error')):,} chars\")\n",
    "        \n",
    "        # Create markdown report for display\n",
    "        report_md = f\"\"\"\n",
    "# üîç Explorer Research Results\n",
    "\n",
    "## Query: {query}\n",
    "\n",
    "### üìä Summary Statistics\n",
    "- **Total Links Found**: {explorer_metadata['metadata']['total_links_found']}\n",
    "- **Pages Successfully Scraped**: {len([r for r in explorer_results if not r.get('error')])}\n",
    "- **Total Text Extracted**: {sum(r.get('full_text_length', 0) for r in explorer_results if not r.get('error')):,} characters\n",
    "- **Search Engines Used**: {', '.join([e['name'] for e in explorer_metadata['metadata']['search_engines']])}\n",
    "\n",
    "### üåê Source Breakdown\n",
    "\"\"\"\n",
    "        \n",
    "        for engine in explorer_metadata['metadata']['search_engines']:\n",
    "            report_md += f\"\"\"\n",
    "#### {engine['name']}\n",
    "- Links Found: {engine['links_found']}\n",
    "- Successfully Scraped: {engine['successful_scrapes']}/{engine['links_scraped']}\n",
    "- Success Rate: {(engine['successful_scrapes']/engine['links_scraped']*100) if engine['links_scraped'] > 0 else 0:.1f}%\n",
    "\"\"\"\n",
    "        \n",
    "        report_md += \"\\n### üìÑ Content Samples\\n\"\n",
    "        \n",
    "        successful_results = [r for r in explorer_results if not r.get('error')][:3]\n",
    "        for i, result in enumerate(successful_results, 1):\n",
    "            report_md += f\"\"\"\n",
    "#### Sample {i}: {result.get('source_engine', 'Unknown')}\n",
    "**URL**: {result.get('url', 'N/A')}  \n",
    "**Length**: {result.get('full_text_length', 0):,} characters  \n",
    "**Preview**: {result.get('text', '')[:200]}...\n",
    "\"\"\"\n",
    "        \n",
    "        display(Markdown(report_md))\n",
    "        \n",
    "        return {\n",
    "            \"type\": \"explorer_only\",\n",
    "            \"query\": query,\n",
    "            \"results\": explorer_results,\n",
    "            \"metadata\": explorer_metadata,\n",
    "            \"summary\": explorer_metadata['summary']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in explorer research '{query}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_explorer_results(filepath: str):\n",
    "    \"\"\"Load and display explorer search results from JSON file\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Check if it's an explorer results file\n",
    "        if \"metadata\" in data and \"scraped_data\" in data:\n",
    "            return data\n",
    "        elif \"explorer_results\" in data and \"explorer_metadata\" in data:\n",
    "            # Combined search file\n",
    "            return {\n",
    "                \"metadata\": data[\"explorer_metadata\"][\"metadata\"],\n",
    "                \"scraped_data\": data[\"explorer_results\"],\n",
    "                \"summary\": data[\"explorer_metadata\"][\"summary\"]\n",
    "            }\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading explorer results: {e}\")\n",
    "        return None\n",
    "\n",
    "def display_explorer_json_analysis(filepath: str, query: str = None):\n",
    "    \"\"\"Analyze and display explorer JSON results\"\"\"\n",
    "    data = load_explorer_results(filepath)\n",
    "    if not data:\n",
    "        return \"‚ùå File is not a valid explorer results file\"\n",
    "    \n",
    "    # Extract information\n",
    "    metadata = data.get(\"metadata\", {})\n",
    "    scraped_data = data.get(\"scraped_data\", [])\n",
    "    summary = data.get(\"summary\", {})\n",
    "    \n",
    "    # Detect query if not provided\n",
    "    if not query:\n",
    "        query = metadata.get(\"query\", summary.get(\"query\", \"Unknown\"))\n",
    "    \n",
    "    successful_scrapes = [item for item in scraped_data if not item.get('error')]\n",
    "    failed_scrapes = [item for item in scraped_data if item.get('error')]\n",
    "    \n",
    "    analysis = f\"\"\"\n",
    "# üìä Explorer Results Analysis\n",
    "\n",
    "## üîç Query Information\n",
    "**Original Query**: {query}  \n",
    "**Timestamp**: {metadata.get('timestamp', summary.get('completed_at', 'Unknown'))}  \n",
    "**File**: {filepath}\n",
    "\n",
    "## üìà Performance Metrics\n",
    "- **Total Links Found**: {metadata.get('total_links_found', 'Unknown')}\n",
    "- **Pages Scraped**: {len(scraped_data)}\n",
    "- **Successful Scrapes**: {len(successful_scrapes)}\n",
    "- **Failed Scrapes**: {len(failed_scrapes)}\n",
    "- **Success Rate**: {(len(successful_scrapes)/len(scraped_data)*100) if scraped_data else 0:.1f}%\n",
    "\n",
    "## üåê Search Engine Performance\n",
    "\"\"\"\n",
    "    \n",
    "    for engine in metadata.get('search_engines', []):\n",
    "        success_rate = (engine['successful_scrapes'] / engine['links_scraped'] * 100) if engine['links_scraped'] > 0 else 0\n",
    "        analysis += f\"\"\"\n",
    "### {engine['name']}\n",
    "- **Links Found**: {engine['links_found']}\n",
    "- **Links Scraped**: {engine['links_scraped']}\n",
    "- **Successful Scrapes**: {engine['successful_scrapes']}\n",
    "- **Success Rate**: {success_rate:.1f}%\n",
    "\"\"\"\n",
    "    \n",
    "    # Content analysis\n",
    "    if successful_scrapes:\n",
    "        total_chars = sum(item.get('full_text_length', 0) for item in successful_scrapes)\n",
    "        avg_chars = total_chars / len(successful_scrapes)\n",
    "        avg_time = sum(item.get('processing_time_seconds', 0) for item in successful_scrapes) / len(successful_scrapes)\n",
    "        \n",
    "        analysis += f\"\"\"\n",
    "## üìÑ Content Analysis\n",
    "- **Total Text Extracted**: {total_chars:,} characters\n",
    "- **Average Text per Page**: {avg_chars:,.0f} characters\n",
    "- **Average Processing Time**: {avg_time:.2f} seconds\n",
    "- **Longest Page**: {max(item.get('full_text_length', 0) for item in successful_scrapes):,} characters\n",
    "- **Shortest Page**: {min(item.get('full_text_length', 0) for item in successful_scrapes):,} characters\n",
    "\n",
    "## üîó Sample URLs (First 5)\n",
    "\"\"\"\n",
    "        for i, item in enumerate(successful_scrapes[:5], 1):\n",
    "            analysis += f\"{i}. {item.get('url', 'Unknown')} ({item.get('full_text_length', 0):,} chars)\\n\"\n",
    "    \n",
    "    if failed_scrapes:\n",
    "        analysis += f\"\"\"\n",
    "## ‚ùå Failed Scrapes ({len(failed_scrapes)})\n",
    "\"\"\"\n",
    "        for i, item in enumerate(failed_scrapes[:3], 1):\n",
    "            analysis += f\"{i}. {item.get('url', 'Unknown')} - {item.get('error_details', item.get('text', 'Unknown error'))}\\n\"\n",
    "    \n",
    "    analysis += f\"\"\"\n",
    "## üîß Configuration Used\n",
    "- **Max Search Results**: {metadata.get('search_config', {}).get('max_search_results', 'Unknown')}\n",
    "- **Max URLs to Scrape**: {metadata.get('search_config', {}).get('max_urls_to_scrape', 'Unknown')}\n",
    "- **Max Text Length**: {metadata.get('search_config', {}).get('max_text_length', 'Unknown')}\n",
    "- **Scraping Timeout**: {metadata.get('search_config', {}).get('scraping_timeout', 'Unknown')} seconds\n",
    "\n",
    "---\n",
    "*Analysis generated at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "async def run_deep_chatgpt_research(query: str):\n",
    "    \"\"\"Run deep ChatGPT research with iterative refinement\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'#' * 60}\")\n",
    "        print(f\"# DEEP CHATGPT RESEARCH: {query}\")\n",
    "        print(f\"{'#' * 60}\\n\")\n",
    "        \n",
    "        results = await perform_deep_chatgpt_search(query)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"üß† Deep ChatGPT Research Summary:\")\n",
    "        print(f\"   - Iterations: {results['iterations']}\")\n",
    "        print(f\"   - Total search phases: {len(results['search_history'])}\")\n",
    "        \n",
    "        display(Markdown(results['final_report'].markdown_report))\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"deep_chatgpt_{timestamp}.json\"\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\nüíæ Deep ChatGPT results saved to: {filepath}\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in deep ChatGPT research '{query}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "async def run_deep_claude_research(query: str):\n",
    "    \"\"\"Run deep Claude-style research with systematic analysis\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'#' * 60}\")\n",
    "        print(f\"# DEEP CLAUDE-STYLE RESEARCH: {query}\")\n",
    "        print(f\"{'#' * 60}\\n\")\n",
    "        \n",
    "        results = await perform_deep_claude_search(query)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"üé≠ Deep Claude-Style Research Summary:\")\n",
    "        print(f\"   - Analysis phases: {results['phases']}\")\n",
    "        print(f\"   - Systematic approach: ‚úÖ\")\n",
    "        \n",
    "        display(Markdown(results['final_report'].markdown_report))\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"deep_claude_style_{timestamp}.json\"\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\nüíæ Deep Claude-style results saved to: {filepath}\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in deep Claude-style research '{query}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "async def run_comparative_deep_research(query: str):\n",
    "    \"\"\"Run comparative deep research (ChatGPT vs Claude-style)\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'#' * 60}\")\n",
    "        print(f\"# COMPARATIVE DEEP RESEARCH: {query}\")\n",
    "        print(f\"{'#' * 60}\\n\")\n",
    "        \n",
    "        results = await perform_comparative_deep_search(query)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"‚öñÔ∏è Comparative Deep Research Summary:\")\n",
    "        print(f\"   - Total iterations: {results['total_iterations']}\")\n",
    "        print(f\"   - ChatGPT approach: ‚úÖ\")\n",
    "        print(f\"   - Claude-style approach: ‚úÖ\")\n",
    "        print(f\"   - Comparative analysis: ‚úÖ\")\n",
    "        \n",
    "        display(Markdown(\"## ChatGPT Research Results\"))\n",
    "        display(Markdown(results['chatgpt_results']['final_report'].markdown_report))\n",
    "        \n",
    "        display(Markdown(\"## Claude-Style Research Results\"))\n",
    "        display(Markdown(results['claude_results']['final_report'].markdown_report))\n",
    "        \n",
    "        display(Markdown(\"## Comparative Analysis\"))\n",
    "        display(Markdown(results['comparative_analysis'].markdown_report))\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"comparative_deep_{timestamp}.json\"\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\nüíæ Comparative deep research results saved to: {filepath}\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in comparative deep research '{query}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Jupyter execution interface ready!\")\n",
    "print(\"üí° Available functions:\")\n",
    "print(\"   - await run_research('your query')           # Agentic search with evaluation\")\n",
    "print(\"   - await run_combined_research('your query')  # Explorer + Agentic combined\")\n",
    "print(\"   - await run_explorer_only_research('query')  # Explorer-only with detailed JSON\")\n",
    "print(\"   - await universal_json_evaluator(filepath, query)  # Evaluate existing results\")\n",
    "print(\"üî• Deep search functions (NOW ENABLED!):\")\n",
    "print(\"   - await run_deep_chatgpt_research('query')    # Deep ChatGPT iterative research\")\n",
    "print(\"   - await run_deep_claude_research('query')     # Deep Claude-style systematic research\") \n",
    "print(\"   - await run_comparative_deep_research('query') # Comparative ChatGPT vs Claude research\")\n",
    "print(f\"\\nüîß Current configuration:\")\n",
    "print(f\"   - Max links to extract: {MAX_LINKS_TO_EXTRACT}\")\n",
    "print(f\"   - Max URLs to scrape: {MAX_URLS_TO_SCRAPE}\")\n",
    "print(f\"   - Max search results: {MAX_SEARCH_RESULTS}\")\n",
    "print(f\"   - Strategic searches: {MAX_STRATEGIC_SEARCHES}\")\n",
    "print(f\"   - Deep search iterations: {DEEP_SEARCH_ITERATIONS}\")\n",
    "print(f\"   - Search context size: {SEARCH_CONTEXT_SIZE}\")\n",
    "print(\"\\nüìÑ Explorer features:\")\n",
    "print(\"   - Detailed link tracking and progress display\")\n",
    "print(\"   - Comprehensive JSON saving with metadata\")\n",
    "print(\"   - Source-by-source analysis and statistics\")\n",
    "print(\"   - Performance metrics and error tracking\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 15: CONFIGURATION CUSTOMIZATION GUIDE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìù CONFIGURATION CUSTOMIZATION GUIDE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "üéõÔ∏è **How to customize the search system:**\n",
    "\n",
    "1. **Modify Search Scope:**\n",
    "   MAX_LINKS_TO_EXTRACT = 30        # Extract more links from search results\n",
    "   MAX_URLS_TO_SCRAPE = 8           # Scrape more web pages  \n",
    "   MAX_SEARCH_RESULTS = 15          # Get more search results per engine\n",
    "\n",
    "2. **Adjust Agentic Search:**\n",
    "   MAX_STRATEGIC_SEARCHES = 5       # Plan more strategic searches\n",
    "   SEARCH_CONTEXT_SIZE = \"large\"    # Use larger context for deeper analysis\n",
    "   REPORT_MIN_LENGTH = 1500         # Require longer, more detailed reports\n",
    "\n",
    "3. **Configure Deep Search:**\n",
    "   DEEP_SEARCH_ITERATIONS = 7       # More iterative refinement cycles\n",
    "   DEEP_SEARCH_REFINEMENT = True    # Enable progressive refinement\n",
    "\n",
    "4. **Performance Tuning:**\n",
    "   SCRAPING_TIMEOUT = 20            # Longer timeout for slow websites\n",
    "   MAX_TEXT_LENGTH = 5000           # Capture more text per page\n",
    "\n",
    "**Example: High-Intensity Research Setup**\n",
    "```python\n",
    "# Uncomment and modify these in Cell 2:\n",
    "# MAX_LINKS_TO_EXTRACT = 50\n",
    "# MAX_URLS_TO_SCRAPE = 10  \n",
    "# MAX_STRATEGIC_SEARCHES = 7\n",
    "# DEEP_SEARCH_ITERATIONS = 10\n",
    "# SEARCH_CONTEXT_SIZE = \"large\"\n",
    "# REPORT_MIN_LENGTH = 2000\n",
    "```\n",
    "\n",
    "**Example: Fast & Light Setup**\n",
    "```python\n",
    "# Uncomment and modify these in Cell 2:\n",
    "# MAX_LINKS_TO_EXTRACT = 10\n",
    "# MAX_URLS_TO_SCRAPE = 3\n",
    "# MAX_STRATEGIC_SEARCHES = 2  \n",
    "# DEEP_SEARCH_ITERATIONS = 3\n",
    "# SEARCH_CONTEXT_SIZE = \"small\"\n",
    "# REPORT_MIN_LENGTH = 500\n",
    "```\n",
    "\n",
    "üî• **Deep search capabilities are NOW ENABLED!**\n",
    "‚úÖ Deep search agents activated\n",
    "‚úÖ Deep search execution functions ready\n",
    "‚úÖ Deep search interface functions available\n",
    "‚úÖ Gradio Deep Search tab enabled\n",
    "\n",
    "üí° **Pro Tips:**\n",
    "- Increase MAX_STRATEGIC_SEARCHES for complex topics\n",
    "- Use \"large\" SEARCH_CONTEXT_SIZE for technical subjects\n",
    "- Enable DEEP_SEARCH_REFINEMENT for controversial topics\n",
    "- Adjust SCRAPING_TIMEOUT if you encounter many timeouts\n",
    "- Use Deep Search for the most comprehensive analysis\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Configuration guide complete!\")\n",
    "print(\"üîÑ Restart the kernel and rerun all cells after making configuration changes\")\n",
    "print(\"üî• Deep Search is now FULLY OPERATIONAL!\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# CELL 16: GRADIO USER INTERFACE\n",
    "# ============================================\n",
    "\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Create comprehensive Gradio interface for the search system\"\"\"\n",
    "    \n",
    "    # Custom CSS for better styling\n",
    "    custom_css = \"\"\"\n",
    "    .gradio-container {\n",
    "        max-width: 1200px !important;\n",
    "    }\n",
    "    .search-header {\n",
    "        text-align: center;\n",
    "        color: #2563eb;\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    .config-section {\n",
    "        background: #f8fafc;\n",
    "        padding: 15px;\n",
    "        border-radius: 8px;\n",
    "        margin: 10px 0;\n",
    "    }\n",
    "    .status-box {\n",
    "        padding: 10px;\n",
    "        border-radius: 5px;\n",
    "        margin: 5px 0;\n",
    "    }\n",
    "    .success { background: #dcfce7; border-left: 4px solid #16a34a; }\n",
    "    .error { background: #fef2f2; border-left: 4px solid #dc2626; }\n",
    "    .info { background: #dbeafe; border-left: 4px solid #2563eb; }\n",
    "    \"\"\"\n",
    "    \n",
    "    async def run_search_interface(query, search_type, max_links, max_urls, max_searches):\n",
    "        \"\"\"Interface wrapper for different search types\"\"\"\n",
    "        if not query.strip():\n",
    "            return \"‚ùå Please enter a search query\", \"\", \"\"\n",
    "        \n",
    "        try:\n",
    "            # Update global configuration temporarily\n",
    "            global MAX_LINKS_TO_EXTRACT, MAX_URLS_TO_SCRAPE, MAX_STRATEGIC_SEARCHES\n",
    "            original_links = MAX_LINKS_TO_EXTRACT\n",
    "            original_urls = MAX_URLS_TO_SCRAPE  \n",
    "            original_searches = MAX_STRATEGIC_SEARCHES\n",
    "            \n",
    "            MAX_LINKS_TO_EXTRACT = max_links\n",
    "            MAX_URLS_TO_SCRAPE = max_urls\n",
    "            MAX_STRATEGIC_SEARCHES = max_searches\n",
    "            \n",
    "            status = f\"üöÄ Starting {search_type} search for: '{query}'\\n\"\n",
    "            status += f\"üìä Config: {max_links} links, {max_urls} URLs, {max_searches} searches\\n\\n\"\n",
    "            \n",
    "            if search_type == \"Standard Agentic Search\":\n",
    "                results = await run_research(query)\n",
    "                if results:\n",
    "                    report = results['report'].markdown_report\n",
    "                    evaluation = f\"\"\"\n",
    "## üìä Evaluation Results\n",
    "- **GPT-4 Score**: {results['evaluation'].gpt_evaluation.overall_score}/10\n",
    "- **Claude Score**: {results['evaluation'].claude_evaluation.overall_score}/10  \n",
    "- **Consensus**: {results['evaluation'].consensus_score}/10\n",
    "- **Quality**: {'‚úÖ Approved' if results['quality_approved'] else '‚ùå Needs Improvement'}\n",
    "- **File**: {results['filepath']}\n",
    "\"\"\"\n",
    "                    status += \"‚úÖ Standard agentic search completed successfully!\"\n",
    "                    \n",
    "            elif search_type == \"Combined Explorer + Agentic\":\n",
    "                results = await run_combined_research(query)\n",
    "                if results:\n",
    "                    report = results.agentic_results.markdown_report\n",
    "                    evaluation = f\"\"\"\n",
    "## üìä Combined Search Results\n",
    "- **Explorer Sources**: {len([item for item in results.explorer_results if 'url' in item])}\n",
    "- **Search Engines**: DuckDuckGo, Brave\n",
    "- **Agentic Report**: Generated\n",
    "- **Combined Summary**: Available\n",
    "\"\"\"\n",
    "                    status += \"‚úÖ Combined search completed successfully!\"\n",
    "                    \n",
    "            elif search_type == \"Explorer Only\":\n",
    "                explorer_results, explorer_metadata = perform_explorer_search(query, save_results=True)\n",
    "                \n",
    "                # Create detailed explorer report\n",
    "                report = f\"\"\"\n",
    "# üîç Explorer Search Results for: {query}\n",
    "\n",
    "## üìä Search Summary\n",
    "- **Total Links Found**: {explorer_metadata['metadata']['total_links_found']}\n",
    "- **Pages Scraped**: {explorer_metadata['metadata']['total_scraped']}\n",
    "- **Successful Scrapes**: {len([r for r in explorer_results if not r.get('error')])}\n",
    "- **Search Engines**: {', '.join([engine['name'] for engine in explorer_metadata['metadata']['search_engines']])}\n",
    "\n",
    "## üåê Search Engine Details\n",
    "\"\"\"\n",
    "                \n",
    "                for engine in explorer_metadata['metadata']['search_engines']:\n",
    "                    report += f\"\"\"\n",
    "### {engine['name']} Results\n",
    "- **Links Found**: {engine['links_found']}\n",
    "- **Links Scraped**: {engine['links_scraped']}\n",
    "- **Successful Scrapes**: {engine['successful_scrapes']}\n",
    "\n",
    "**All {engine['name']} Links:**\n",
    "\"\"\"\n",
    "                    for i, link in enumerate(engine['all_links'][:10], 1):  # Show first 10 links\n",
    "                        report += f\"{i}. {link}\\n\"\n",
    "                    if len(engine['all_links']) > 10:\n",
    "                        report += f\"... and {len(engine['all_links']) - 10} more links\\n\"\n",
    "                \n",
    "                report += \"\\n## üìÑ Scraped Content Preview\\n\"\n",
    "                \n",
    "                for i, result in enumerate([r for r in explorer_results if not r.get('error')][:5], 1):\n",
    "                    report += f\"\"\"\n",
    "### Source {i}: {result.get('source_engine', 'Unknown')}\n",
    "**URL**: {result.get('url', 'N/A')}\n",
    "**Text Length**: {result.get('full_text_length', 0):,} characters\n",
    "**Processing Time**: {result.get('processing_time_seconds', 0)} seconds\n",
    "\n",
    "**Content Preview**:\n",
    "{result.get('text', 'No content')[:500]}...\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "                \n",
    "                evaluation = f\"\"\"\n",
    "## üìä Explorer Results Analysis\n",
    "- **Total Sources**: {len(explorer_results)}\n",
    "- **Successful Scrapes**: {len([r for r in explorer_results if not r.get('error')])}\n",
    "- **Failed Scrapes**: {len([r for r in explorer_results if r.get('error')])}\n",
    "- **Total Text Extracted**: {sum(r.get('full_text_length', 0) for r in explorer_results if not r.get('error')):,} characters\n",
    "- **Average Processing Time**: {sum(r.get('processing_time_seconds', 0) for r in explorer_results) / len(explorer_results):.2f} seconds\n",
    "- **Search Engines**: DuckDuckGo, Brave\n",
    "- **Saved to**: {explorer_metadata['summary']['completed_at']}\n",
    "\n",
    "### üîó Link Success Rate by Engine\n",
    "\"\"\"\n",
    "                for engine in explorer_metadata['metadata']['search_engines']:\n",
    "                    success_rate = (engine['successful_scrapes'] / engine['links_scraped'] * 100) if engine['links_scraped'] > 0 else 0\n",
    "                    evaluation += f\"- **{engine['name']}**: {success_rate:.1f}% ({engine['successful_scrapes']}/{engine['links_scraped']})\\n\"\n",
    "                \n",
    "                status += \"‚úÖ Explorer search completed successfully!\"\n",
    "            \n",
    "            # Restore original configuration\n",
    "            MAX_LINKS_TO_EXTRACT = original_links\n",
    "            MAX_URLS_TO_SCRAPE = original_urls\n",
    "            MAX_STRATEGIC_SEARCHES = original_searches\n",
    "            \n",
    "            return status, report, evaluation\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Restore original configuration on error\n",
    "            MAX_LINKS_TO_EXTRACT = original_links\n",
    "            MAX_URLS_TO_SCRAPE = original_urls\n",
    "            MAX_STRATEGIC_SEARCHES = original_searches\n",
    "            \n",
    "            error_msg = f\"‚ùå Error during {search_type}: {str(e)}\"\n",
    "            return error_msg, \"\", \"\"\n",
    "    \n",
    "    async def evaluate_file_interface(filepath, query):\n",
    "        \"\"\"Interface wrapper for file evaluation\"\"\"\n",
    "        if not filepath.strip():\n",
    "            return \"‚ùå Please provide a filepath\"\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(filepath):\n",
    "                return f\"‚ùå File not found: {filepath}\"\n",
    "            \n",
    "            # Check if it's an explorer results file\n",
    "            explorer_data = load_explorer_results(filepath)\n",
    "            if explorer_data:\n",
    "                # It's an explorer file - provide explorer analysis\n",
    "                if not query.strip():\n",
    "                    # Try to extract query from file\n",
    "                    query = explorer_data.get(\"metadata\", {}).get(\"query\") or explorer_data.get(\"summary\", {}).get(\"query\", \"\")\n",
    "                \n",
    "                return display_explorer_json_analysis(filepath, query)\n",
    "            else:\n",
    "                # It's a regular research file - use standard evaluation\n",
    "                if not query.strip():\n",
    "                    return \"‚ùå Please provide the original query for evaluation\"\n",
    "                \n",
    "                evaluation = await universal_json_evaluator(filepath, query)\n",
    "                return evaluation.evaluation_summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Evaluation error: {str(e)}\"\n",
    "    \n",
    "    def get_recent_files():\n",
    "        \"\"\"Get list of recent result files\"\"\"\n",
    "        try:\n",
    "            files = []\n",
    "            for filename in os.listdir(DATA_DIR):\n",
    "                if filename.endswith('.json'):\n",
    "                    filepath = os.path.join(DATA_DIR, filename)\n",
    "                    files.append(filepath)\n",
    "            return sorted(files, key=os.path.getmtime, reverse=True)[:10]\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    # Create Gradio interface\n",
    "    with gr.Blocks(css=custom_css, title=\"üîç Advanced Research System\") as interface:\n",
    "        \n",
    "        gr.HTML(\"\"\"\n",
    "        <div class=\"search-header\">\n",
    "            <h1>üîç Advanced AI Research System</h1>\n",
    "            <p>Powered by Multi-Engine Search + AI Analysis + Dual-Model Evaluation</p>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Tabs():\n",
    "            \n",
    "            # Main Search Tab\n",
    "            with gr.Tab(\"üöÄ Research Search\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        query_input = gr.Textbox(\n",
    "                            label=\"üîç Research Query\",\n",
    "                            placeholder=\"Enter your research question...\",\n",
    "                            lines=2\n",
    "                        )\n",
    "                        \n",
    "                        search_type = gr.Radio(\n",
    "                            choices=[\n",
    "                                \"Standard Agentic Search\",\n",
    "                                \"Combined Explorer + Agentic\", \n",
    "                                \"Explorer Only\"\n",
    "                            ],\n",
    "                            value=\"Standard Agentic Search\",\n",
    "                            label=\"Search Type\"\n",
    "                        )\n",
    "                        \n",
    "                    with gr.Column(scale=1):\n",
    "                        gr.HTML('<div class=\"config-section\">')\n",
    "                        gr.HTML(\"<h4>‚öôÔ∏è Configuration</h4>\")\n",
    "                        \n",
    "                        max_links = gr.Slider(\n",
    "                            minimum=5, maximum=100, value=MAX_LINKS_TO_EXTRACT,\n",
    "                            label=\"Max Links to Extract\"\n",
    "                        )\n",
    "                        max_urls = gr.Slider(\n",
    "                            minimum=1, maximum=20, value=MAX_URLS_TO_SCRAPE,\n",
    "                            label=\"Max URLs to Scrape\" \n",
    "                        )\n",
    "                        max_searches = gr.Slider(\n",
    "                            minimum=1, maximum=10, value=MAX_STRATEGIC_SEARCHES,\n",
    "                            label=\"Strategic Searches\"\n",
    "                        )\n",
    "                        gr.HTML('</div>')\n",
    "                \n",
    "                search_btn = gr.Button(\"üöÄ Start Research\", variant=\"primary\", size=\"lg\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        status_output = gr.Textbox(\n",
    "                            label=\"üìä Status & Progress\",\n",
    "                            lines=4,\n",
    "                            interactive=False\n",
    "                        )\n",
    "                    \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        report_output = gr.Markdown(\n",
    "                            label=\"üìÑ Research Report\",\n",
    "                            height=400\n",
    "                        )\n",
    "                    with gr.Column():\n",
    "                        evaluation_output = gr.Markdown(\n",
    "                            label=\"üìä Evaluation & Metrics\",\n",
    "                            height=400\n",
    "                        )\n",
    "                \n",
    "                search_btn.click(\n",
    "                    fn=run_search_interface,\n",
    "                    inputs=[query_input, search_type, max_links, max_urls, max_searches],\n",
    "                    outputs=[status_output, report_output, evaluation_output]\n",
    "                )\n",
    "            \n",
    "            # File Evaluation Tab\n",
    "            with gr.Tab(\"üìä Evaluate Results\"):\n",
    "                gr.HTML(\"<h3>üìä Analyze Research Files</h3>\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        file_input = gr.Textbox(\n",
    "                            label=\"üìÅ File Path\",\n",
    "                            placeholder=\"Enter path to JSON results file...\",\n",
    "                            lines=1\n",
    "                        )\n",
    "                        eval_query_input = gr.Textbox(\n",
    "                            label=\"üîç Original Query (Optional for Explorer files)\",\n",
    "                            placeholder=\"Enter the original research query (auto-detected for Explorer files)...\",\n",
    "                            lines=2\n",
    "                        )\n",
    "                        \n",
    "                        recent_files = gr.Dropdown(\n",
    "                            choices=get_recent_files(),\n",
    "                            label=\"üìÇ Recent Files\",\n",
    "                            interactive=True\n",
    "                        )\n",
    "                        \n",
    "                        def update_file_path(selected_file):\n",
    "                            return selected_file if selected_file else \"\"\n",
    "                        \n",
    "                        recent_files.change(\n",
    "                            fn=update_file_path,\n",
    "                            inputs=[recent_files],\n",
    "                            outputs=[file_input]\n",
    "                        )\n",
    "                \n",
    "                eval_btn = gr.Button(\"üìä Analyze File\", variant=\"secondary\")\n",
    "                \n",
    "                gr.HTML(\"\"\"\n",
    "                <div style=\"background: #f0f9ff; padding: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "                    <h4>üìÑ Supported File Types</h4>\n",
    "                    <ul>\n",
    "                        <li><strong>Explorer Results</strong>: <code>explorer_search_*.json</code> - Shows link analysis, scraping stats, content breakdown</li>\n",
    "                        <li><strong>Research Reports</strong>: <code>evaluation_*.json</code> - Provides AI-powered quality evaluation</li>\n",
    "                        <li><strong>Combined Results</strong>: <code>combined_search_*.json</code> - Analyzes both explorer and agentic components</li>\n",
    "                        <li><strong>Deep Search</strong>: <code>deep_*.json</code> - Analysis of iterative research results</li>\n",
    "                    </ul>\n",
    "                </div>\n",
    "                \"\"\")\n",
    "                \n",
    "                evaluation_result = gr.Markdown(\n",
    "                    label=\"üìä Analysis Report\",\n",
    "                    height=500\n",
    "                )\n",
    "                \n",
    "                eval_btn.click(\n",
    "                    fn=evaluate_file_interface,\n",
    "                    inputs=[file_input, eval_query_input],\n",
    "                    outputs=[evaluation_result]\n",
    "                )\n",
    "            \n",
    "            # Configuration Tab\n",
    "            with gr.Tab(\"‚öôÔ∏è System Config\"):\n",
    "                gr.HTML(\"<h3>‚öôÔ∏è Current System Configuration</h3>\")\n",
    "                \n",
    "                # API Status Section\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.HTML(\"<h4>üîå API Status</h4>\")\n",
    "                        api_status_btn = gr.Button(\"üîç Check API Availability\", variant=\"secondary\")\n",
    "                        api_status_output = gr.Markdown(\"Click button to check API status...\")\n",
    "                        \n",
    "                        async def check_api_status():\n",
    "                            status = await test_api_availability()\n",
    "                            \n",
    "                            status_text = f\"\"\"\n",
    "## üîå API Availability Check\n",
    "**Timestamp**: {status['timestamp']}\n",
    "\n",
    "### Service Status\n",
    "- **GPT-4**: {'‚úÖ Available' if status['gpt4_available'] else '‚ùå Unavailable'}\n",
    "- **Claude**: {'‚úÖ Available' if status['claude_available'] else '‚ùå Unavailable'}\n",
    "\n",
    "### Recommendations\n",
    "\"\"\"\n",
    "                            if status['gpt4_available'] and status['claude_available']:\n",
    "                                status_text += \"üéâ **All systems operational** - Full evaluation available\\n\"\n",
    "                            elif status['gpt4_available'] or status['claude_available']:\n",
    "                                status_text += \"‚ö†Ô∏è **Partial availability** - Fallback evaluation will be used\\n\"\n",
    "                            else:\n",
    "                                status_text += \"üö´ **Limited availability** - Fallback evaluations will be used for both models\\n\"\n",
    "                            \n",
    "                            if not status['claude_available']:\n",
    "                                status_text += \"\\n### Claude Troubleshooting\\n\"\n",
    "                                status_text += \"- Try again in 2-5 minutes\\n- System will use fallback scoring\\n- GPT-4 evaluation still available\\n\"\n",
    "                            \n",
    "                            if not status['gpt4_available']:\n",
    "                                status_text += \"\\n### GPT-4 Troubleshooting\\n\" \n",
    "                                status_text += \"- Check API key and billing\\n- Verify rate limits\\n- Claude evaluation still available\\n\"\n",
    "                            \n",
    "                            return status_text\n",
    "                        \n",
    "                        api_status_btn.click(\n",
    "                            fn=check_api_status,\n",
    "                            outputs=[api_status_output]\n",
    "                        )\n",
    "                \n",
    "                config_info = f\"\"\"\n",
    "## üìä Current Settings\n",
    "\n",
    "### Search Configuration\n",
    "- **Max Links to Extract**: {MAX_LINKS_TO_EXTRACT}\n",
    "- **Max URLs to Scrape**: {MAX_URLS_TO_SCRAPE}\n",
    "- **Max Search Results**: {MAX_SEARCH_RESULTS}\n",
    "- **Strategic Searches**: {MAX_STRATEGIC_SEARCHES}\n",
    "- **Search Context Size**: {SEARCH_CONTEXT_SIZE}\n",
    "\n",
    "### Performance Settings  \n",
    "- **Scraping Timeout**: {SCRAPING_TIMEOUT}s\n",
    "- **Max Text Length**: {MAX_TEXT_LENGTH} chars\n",
    "- **Report Min Length**: {REPORT_MIN_LENGTH} words\n",
    "\n",
    "### Deep Search (Commented Out)\n",
    "- **Deep Search Iterations**: {DEEP_SEARCH_ITERATIONS}\n",
    "- **Deep Search Refinement**: {DEEP_SEARCH_REFINEMENT}\n",
    "\n",
    "### üîß How to Modify Configuration\n",
    "1. Edit variables in **Cell 2** of the notebook\n",
    "2. Restart kernel and rerun all cells\n",
    "3. Or use the sliders in the Research Search tab for temporary changes\n",
    "\n",
    "### üí° Deep Search Capabilities\n",
    "To enable deep search features:\n",
    "1. Uncomment deep search agents in Cell 9\n",
    "2. Uncomment deep search functions in Cell 10B\n",
    "3. Uncomment deep search interfaces in Cell 14\n",
    "4. Add deep search tab to this Gradio interface\n",
    "\n",
    "### üîß API Error Handling\n",
    "- **Automatic Retries**: 3 attempts with exponential backoff\n",
    "- **Fallback Evaluations**: Heuristic scoring when APIs unavailable\n",
    "- **Graceful Degradation**: Partial evaluation when one API fails\n",
    "- **Status Monitoring**: Check API availability above\n",
    "\"\"\"\n",
    "                \n",
    "                gr.Markdown(config_info)\n",
    "                \n",
    "                refresh_btn = gr.Button(\"üîÑ Refresh File List\")\n",
    "                refresh_btn.click(\n",
    "                    fn=lambda: gr.Dropdown.update(choices=get_recent_files()),\n",
    "                    outputs=[recent_files]\n",
    "                )\n",
    "            \n",
    "            # Help Tab\n",
    "            with gr.Tab(\"‚ùì Help\"):\n",
    "                help_content = \"\"\"\n",
    "# üîç Advanced Research System - User Guide\n",
    "\n",
    "## üöÄ Quick Start\n",
    "1. **Enter your research query** in the search box\n",
    "2. **Select search type**:\n",
    "   - **Standard Agentic**: AI-powered analysis with evaluation\n",
    "   - **Combined**: Web scraping + AI analysis  \n",
    "   - **Explorer Only**: Raw web scraping\n",
    "3. **Adjust configuration** sliders if needed\n",
    "4. **Click \"Start Research\"** and wait for results\n",
    "\n",
    "## üìä Search Types Explained\n",
    "\n",
    "### Standard Agentic Search\n",
    "- Uses AI agents to plan strategic searches\n",
    "- Performs web research with built-in tools\n",
    "- Generates comprehensive reports\n",
    "- Includes dual-model evaluation (GPT-4 + Claude)\n",
    "- **Best for**: Complex analysis, professional reports\n",
    "\n",
    "### Combined Explorer + Agentic  \n",
    "- Scrapes web pages directly (DuckDuckGo + Brave)\n",
    "- Combines with AI analysis\n",
    "- Provides both raw data and insights\n",
    "- **Best for**: Comprehensive coverage, fact-checking\n",
    "\n",
    "### Explorer Only\n",
    "- Direct web scraping without AI processing\n",
    "- Raw content from multiple search engines\n",
    "- **Enhanced features**: Detailed link tracking, comprehensive JSON saving, performance metrics\n",
    "- **Best for**: Quick data gathering, source verification, link analysis\n",
    "\n",
    "## üìÑ File Analysis Features\n",
    "\n",
    "### Automatic File Type Detection\n",
    "The system automatically detects and analyzes different file types:\n",
    "\n",
    "#### üîç Explorer Results Files\n",
    "- **Format**: `explorer_search_YYYYMMDD_HHMMSS.json`\n",
    "- **Analysis**: Link success rates, scraping performance, content breakdown\n",
    "- **Auto-detection**: Query extracted from metadata\n",
    "- **Metrics**: Processing times, content lengths, error analysis\n",
    "\n",
    "#### üìä Research Report Files  \n",
    "- **Format**: `evaluation_YYYYMMDD_HHMMSS.json`\n",
    "- **Analysis**: AI-powered quality evaluation using GPT-4 + Claude\n",
    "- **Requires**: Original query for proper evaluation\n",
    "\n",
    "#### üîÑ Combined Search Files\n",
    "- **Format**: `combined_search_YYYYMMDD_HHMMSS.json`  \n",
    "- **Analysis**: Both explorer and agentic components\n",
    "- **Features**: Comprehensive analysis of all data sources\n",
    "\n",
    "## ‚öôÔ∏è Configuration Tips\n",
    "\n",
    "### Performance Settings\n",
    "- **Max Links**: Higher = more comprehensive, slower\n",
    "- **Max URLs**: Higher = more content, longer processing\n",
    "- **Strategic Searches**: Higher = deeper analysis\n",
    "\n",
    "### Quality vs Speed\n",
    "- **Fast**: 5 links, 2 URLs, 2 searches\n",
    "- **Balanced**: 20 links, 5 URLs, 3 searches  \n",
    "- **Comprehensive**: 50 links, 10 URLs, 7 searches\n",
    "\n",
    "## üîó Enhanced Explorer Features\n",
    "\n",
    "### Detailed Link Tracking\n",
    "- **Real-time display** of all discovered links\n",
    "- **Source attribution** (DuckDuckGo, Brave, etc.)\n",
    "- **Link success rates** by search engine\n",
    "- **Processing time tracking** per URL\n",
    "\n",
    "### Comprehensive JSON Saving\n",
    "- **Metadata preservation**: Search configuration, timestamps, engine details\n",
    "- **Full content storage**: Original text length + truncated versions\n",
    "- **Error tracking**: Detailed error messages and failure analysis\n",
    "- **Performance metrics**: Processing times, success rates, content statistics\n",
    "\n",
    "### Progress Monitoring\n",
    "- **Live scraping progress** with URL display\n",
    "- **Success/failure indicators** in real-time\n",
    "- **Character count tracking** as pages are processed\n",
    "- **Engine-by-engine breakdown** of results\n",
    "\n",
    "### Advanced Analysis\n",
    "```python\n",
    "# Explorer-only research with full JSON\n",
    "results = await run_explorer_only_research(\"your query\")\n",
    "\n",
    "# Load and analyze existing explorer files\n",
    "analysis = display_explorer_json_analysis(\"explorer_search_file.json\")\n",
    "```\n",
    "\n",
    "## üìÅ File Management\n",
    "- Results automatically saved to `workspace/data/`\n",
    "- **Explorer files**: `explorer_search_YYYYMMDD_HHMMSS.json`\n",
    "- **Combined files**: `combined_search_YYYYMMDD_HHMMSS.json`\n",
    "- **Evaluation files**: `evaluation_YYYYMMDD_HHMMSS.json`\n",
    "- Use \"Analyze Results\" tab for any file type\n",
    "\n",
    "## üîß API Error Handling\n",
    "\n",
    "### Common Issues & Solutions\n",
    "\n",
    "#### Claude API Overload (Error 529)\n",
    "**Symptoms**: \"Overloaded\" error message\n",
    "**Solutions**:\n",
    "- ‚è≥ Wait 2-5 minutes and retry\n",
    "- üîÑ System automatically uses fallback evaluation\n",
    "- ‚úÖ GPT-4 evaluation still works normally\n",
    "\n",
    "#### GPT-4 API Issues  \n",
    "**Symptoms**: Authentication or rate limit errors\n",
    "**Solutions**:\n",
    "- üîë Check OpenAI API key in environment\n",
    "- üí≥ Verify account billing and credits\n",
    "- ‚è±Ô∏è Wait between requests to avoid rate limits\n",
    "\n",
    "### Fallback Evaluations\n",
    "When APIs are unavailable, the system uses:\n",
    "- **Heuristic scoring** based on content analysis\n",
    "- **Conservative estimates** for accuracy and quality\n",
    "- **Clear labeling** of fallback vs. API evaluations\n",
    "- **Medium confidence** ratings\n",
    "\n",
    "### üí° Best Practices\n",
    "1. **Check API status** in System Config tab\n",
    "2. **Use off-peak hours** for better availability\n",
    "3. **Space out evaluations** to avoid rate limits\n",
    "4. **Re-evaluate later** when APIs recover\n",
    "5. **Monitor fallback indicators** in results\n",
    "\n",
    "## üîß Deep Search Features (Advanced)\n",
    "\n",
    "### How to Enable Deep Search\n",
    "1. **Uncomment Cell 9**: Deep search agent definitions\n",
    "2. **Uncomment Cell 10B**: Deep search execution functions  \n",
    "3. **Uncomment Cell 14**: Deep search interface functions\n",
    "4. **Restart kernel** and rerun all cells\n",
    "\n",
    "### Deep Search Modes\n",
    "\n",
    "#### üß† Deep ChatGPT Search\n",
    "- **5 iterative cycles** with progressive refinement\n",
    "- **Gap identification** and targeted follow-up\n",
    "- **Comprehensive synthesis** of findings\n",
    "- **Best for**: Complex, evolving topics\n",
    "\n",
    "#### üé≠ Deep Claude-Style Search  \n",
    "- **Systematic 5-phase** methodology\n",
    "- **Critical source evaluation**\n",
    "- **Conservative confidence** assessment\n",
    "- **Best for**: Controversial or technical topics\n",
    "\n",
    "#### ‚öñÔ∏è Comparative Deep Search\n",
    "- **Parallel ChatGPT + Claude** approaches\n",
    "- **Convergence/divergence** analysis\n",
    "- **Meta-research** quality assessment\n",
    "- **Best for**: Complex topics requiring multiple perspectives\n",
    "\n",
    "### Usage After Enabling\n",
    "```python\n",
    "# Deep iterative research\n",
    "await run_deep_chatgpt_research(\"query\")\n",
    "\n",
    "# Systematic deep analysis  \n",
    "await run_deep_claude_research(\"query\")\n",
    "\n",
    "# Comparative deep research\n",
    "await run_comparative_deep_research(\"query\")\n",
    "```\n",
    "\n",
    "### Configuration\n",
    "- **DEEP_SEARCH_ITERATIONS**: 5-10 cycles\n",
    "- **SEARCH_CONTEXT_SIZE**: \"large\" for deep analysis\n",
    "- **DEEP_SEARCH_REFINEMENT**: Enable progressive improvement\n",
    "\n",
    "## ‚ùì Troubleshooting\n",
    "- **Slow performance**: Reduce max links/URLs\n",
    "- **Timeout errors**: Increase scraping timeout in config\n",
    "- **Empty results**: Try different search terms\n",
    "- **Evaluation errors**: Check file path and query format\n",
    "- **API issues**: Use System Config tab to check status\n",
    "\n",
    "## üÜò Emergency Mode\n",
    "If both APIs fail:\n",
    "1. System continues with fallback evaluations\n",
    "2. Focus on the research content quality\n",
    "3. Manually verify key facts from sources\n",
    "4. Re-evaluate when APIs recover\n",
    "\n",
    "\"\"\"\n",
    "                gr.Markdown(help_content)\n",
    "            \n",
    "            # ============================================\n",
    "            # DEEP SEARCH TAB (NOW ENABLED!)\n",
    "            # ============================================\n",
    "            \n",
    "            # Deep Search Tab\n",
    "            with gr.Tab(\"üîç Deep Search\"):\n",
    "                gr.HTML(\"<h3>üîç Advanced Deep Search Capabilities</h3>\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        deep_query_input = gr.Textbox(\n",
    "                            label=\"üéØ Deep Research Query\",\n",
    "                            placeholder=\"Enter complex research question for deep analysis...\",\n",
    "                            lines=3\n",
    "                        )\n",
    "                        \n",
    "                        deep_search_type = gr.Radio(\n",
    "                            choices=[\n",
    "                                \"Deep ChatGPT (Iterative)\",\n",
    "                                \"Deep Claude-Style (Systematic)\", \n",
    "                                \"Comparative Deep Search (Both)\"\n",
    "                            ],\n",
    "                            value=\"Deep ChatGPT (Iterative)\",\n",
    "                            label=\"Deep Search Mode\"\n",
    "                        )\n",
    "                        \n",
    "                        deep_iterations = gr.Slider(\n",
    "                            minimum=3, maximum=15, value=DEEP_SEARCH_ITERATIONS,\n",
    "                            label=\"Search Iterations\"\n",
    "                        )\n",
    "                \n",
    "                async def run_deep_search_interface(query, search_type, iterations):\n",
    "                    if not query.strip():\n",
    "                        return \"‚ùå Please enter a deep research query\", \"\"\n",
    "                    \n",
    "                    try:\n",
    "                        # Update iterations temporarily\n",
    "                        global DEEP_SEARCH_ITERATIONS\n",
    "                        original_iterations = DEEP_SEARCH_ITERATIONS\n",
    "                        DEEP_SEARCH_ITERATIONS = iterations\n",
    "                        \n",
    "                        status = f\"üîç Starting {search_type} for: '{query}'\\n\"\n",
    "                        status += f\"üîÑ Iterations: {iterations}\\n\\n\"\n",
    "                        \n",
    "                        if search_type == \"Deep ChatGPT (Iterative)\":\n",
    "                            results = await run_deep_chatgpt_research(query)\n",
    "                            report = results['final_report'].markdown_report\n",
    "                            status += f\"‚úÖ Deep ChatGPT search completed!\\n\"\n",
    "                            status += f\"üìä {len(results['search_history'])} iterations performed\"\n",
    "                            \n",
    "                        elif search_type == \"Deep Claude-Style (Systematic)\":\n",
    "                            results = await run_deep_claude_research(query)\n",
    "                            report = results['final_report'].markdown_report  \n",
    "                            status += f\"‚úÖ Deep Claude-style search completed!\\n\"\n",
    "                            status += f\"üìä {len(results['analysis_phases'])} phases completed\"\n",
    "                            \n",
    "                        elif search_type == \"Comparative Deep Search (Both)\":\n",
    "                            results = await run_comparative_deep_research(query)\n",
    "                            report = results['comparative_analysis'].markdown_report\n",
    "                            status += f\"‚úÖ Comparative deep search completed!\\n\"\n",
    "                            status += f\"üìä {results['total_iterations']} total iterations\"\n",
    "                        \n",
    "                        # Restore original setting\n",
    "                        DEEP_SEARCH_ITERATIONS = original_iterations\n",
    "                        return status, report\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        DEEP_SEARCH_ITERATIONS = original_iterations\n",
    "                        return f\"‚ùå Deep search error: {str(e)}\", \"\"\n",
    "                \n",
    "                deep_search_btn = gr.Button(\"üîç Start Deep Search\", variant=\"primary\", size=\"lg\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        deep_status_output = gr.Textbox(\n",
    "                            label=\"üìä Deep Search Progress\",\n",
    "                            lines=4,\n",
    "                            interactive=False\n",
    "                        )\n",
    "                    with gr.Column():\n",
    "                        deep_report_output = gr.Markdown(\n",
    "                            label=\"üìÑ Deep Research Report\",\n",
    "                            height=500\n",
    "                        )\n",
    "                \n",
    "                deep_search_btn.click(\n",
    "                    fn=run_deep_search_interface,\n",
    "                    inputs=[deep_query_input, deep_search_type, deep_iterations],\n",
    "                    outputs=[deep_status_output, deep_report_output]\n",
    "                )\n",
    "                \n",
    "                gr.HTML(\"\"\"\n",
    "                <div style=\"background: #f0f9ff; padding: 15px; border-radius: 8px; margin: 10px 0;\">\n",
    "                    <h4>üîç Deep Search Capabilities</h4>\n",
    "                    <ul>\n",
    "                        <li><strong>Deep ChatGPT</strong>: 5-15 iterative cycles with progressive refinement</li>\n",
    "                        <li><strong>Deep Claude-Style</strong>: Systematic 5-phase analytical methodology</li>\n",
    "                        <li><strong>Comparative</strong>: Both approaches + meta-analysis comparison</li>\n",
    "                    </ul>\n",
    "                    <p><em>‚ö†Ô∏è Deep searches take 5-20 minutes depending on complexity and iterations</em></p>\n",
    "                </div>\n",
    "                \"\"\")\n",
    "\n",
    "        \n",
    "        gr.HTML(\"\"\"\n",
    "        <div style=\"text-align: center; margin-top: 20px; color: #6b7280;\">\n",
    "            <p>üîç Advanced Research System | Multi-Engine Search + AI Analysis + Dual Evaluation</p>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Create and launch interface\n",
    "def launch_gradio_interface(share=False, debug=False):\n",
    "    \"\"\"Launch the Gradio interface\"\"\"\n",
    "    print(\"üöÄ Launching Gradio interface...\")\n",
    "    print(f\"üìä Current config: {MAX_LINKS_TO_EXTRACT} links, {MAX_URLS_TO_SCRAPE} URLs, {MAX_STRATEGIC_SEARCHES} searches\")\n",
    "    \n",
    "    interface = create_gradio_interface()\n",
    "    \n",
    "    # Launch interface\n",
    "    interface.launch(\n",
    "        share=share,\n",
    "        debug=debug,\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7860,\n",
    "        show_error=True,\n",
    "        quiet=False\n",
    "    )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "print(\"‚úÖ Gradio interface ready!\")\n",
    "print(\"üí° Usage:\")\n",
    "print(\"   - interface = launch_gradio_interface()              # Launch locally\")\n",
    "print(\"   - interface = launch_gradio_interface(share=True)    # Create public link\")\n",
    "print(\"   - interface = launch_gradio_interface(debug=True)    # Enable debug mode\")\n",
    "\n",
    "# ============================================\n",
    "# AUTO-LAUNCH GRADIO INTERFACE\n",
    "# ============================================\n",
    "\n",
    "# Auto-launch settings\n",
    "AUTO_LAUNCH = True           # Set to False to disable auto-launch\n",
    "SHARE_PUBLICLY = False       # Set to True to create public shareable link\n",
    "DEBUG_MODE = False           # Set to True for debugging\n",
    "PORT = 7860                  # Port number for the interface\n",
    "\n",
    "if AUTO_LAUNCH:\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üöÄ AUTO-LAUNCHING GRADIO INTERFACE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üìä Configuration: {MAX_LINKS_TO_EXTRACT} links, {MAX_URLS_TO_SCRAPE} URLs, {MAX_STRATEGIC_SEARCHES} searches\")\n",
    "        print(f\"üåê Share publicly: {'Yes' if SHARE_PUBLICLY else 'No'}\")\n",
    "        print(f\"üîß Debug mode: {'Enabled' if DEBUG_MODE else 'Disabled'}\")\n",
    "        print(f\"üîå Port: {PORT}\")\n",
    "        \n",
    "        # Create and launch interface automatically\n",
    "        interface = create_gradio_interface()\n",
    "        \n",
    "        print(\"üéØ Starting Gradio server...\")\n",
    "        interface.launch(\n",
    "            share=SHARE_PUBLICLY,\n",
    "            debug=DEBUG_MODE,\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=PORT,\n",
    "            show_error=True,\n",
    "            quiet=False,\n",
    "            inbrowser=True,          # Automatically open in browser\n",
    "            prevent_thread_lock=False  # Allow notebook to continue\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Gradio interface launched successfully!\")\n",
    "        print(f\"üåê Access your interface at: http://localhost:{PORT}\")\n",
    "        if SHARE_PUBLICLY:\n",
    "            print(\"üîó Public link will be displayed above\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to auto-launch Gradio interface: {str(e)}\")\n",
    "        print(\"üí° You can manually launch with: launch_gradio_interface()\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚è∏Ô∏è Auto-launch disabled\")\n",
    "    print(\"üí° To launch manually: interface = launch_gradio_interface()\")\n",
    "    print(\"üîß To enable auto-launch: Set AUTO_LAUNCH = True in this cell\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ ADVANCED RESEARCH SYSTEM FULLY OPERATIONAL!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üìö Complete system with:\")\n",
    "print(\"   ‚úÖ Multi-engine web scraping (DuckDuckGo + Brave)\")\n",
    "print(\"   ‚úÖ AI-powered agentic search\")\n",
    "print(\"   ‚úÖ Dual-model evaluation (GPT-4 + Claude)\")\n",
    "print(\"   ‚úÖ Deep search capabilities (ENABLED!)\")\n",
    "print(\"   ‚úÖ Beautiful Gradio web interface with Deep Search tab\")\n",
    "print(\"   ‚úÖ Fully configurable parameters\")\n",
    "print(\"   ‚úÖ Robust error handling with fallbacks\")\n",
    "print(\"\\nüî• Enhanced Explorer Features:\")\n",
    "print(\"   ‚úÖ Real-time link discovery and display\")\n",
    "print(\"   ‚úÖ Detailed scraping progress with statistics\")\n",
    "print(\"   ‚úÖ Comprehensive JSON saving with metadata\")\n",
    "print(\"   ‚úÖ Performance metrics and error tracking\")\n",
    "print(\"   ‚úÖ Source-by-source analysis and breakdown\")\n",
    "print(\"   ‚úÖ Automatic file type detection in analysis\")\n",
    "print(\"\\nüöÄ Ready for advanced research tasks!\")\n",
    "print(\"üî• Deep Search now available in both notebook and Gradio interface!\")\n",
    "print(\"üìä Explorer JSON data now provides complete transparency!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
