{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aacfdb94",
   "metadata": {},
   "source": [
    "## Cell 1: IMPORTS AND ENVIRONMENT SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24ddf45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment and directories initialized!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import random\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "# AI Agent imports\n",
    "from agents import Agent, WebSearchTool, trace, Runner, gen_trace_id, function_tool\n",
    "from agents.model_settings import ModelSettings\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LLM clients for evaluation\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# UI and display imports\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Initialize environment\n",
    "load_dotenv(override=True)\n",
    "openai_client = OpenAI()\n",
    "claude_client = Anthropic()\n",
    "\n",
    "# Setup workspace directories\n",
    "BASE_DIR = os.getcwd()\n",
    "WORKSPACE_DIR = os.path.join(BASE_DIR, \"workspace\")\n",
    "RESULTS_DIR = os.path.join(WORKSPACE_DIR, \"results\")\n",
    "DATA_DIR = os.path.join(WORKSPACE_DIR, \"data\")\n",
    "\n",
    "for dir_path in [WORKSPACE_DIR, RESULTS_DIR, DATA_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"✅ Environment and directories initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e06e4",
   "metadata": {},
   "source": [
    "## CELL 2: CONFIGURATION AND PYDANTIC MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5436fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration and models loaded!\n"
     ]
    }
   ],
   "source": [
    "# ===== USER CONFIGURABLE SETTINGS =====\n",
    "# Modify these variables based on your requirements\n",
    "\n",
    "# Web scraping configuration\n",
    "MAX_LINKS_TO_EXTRACT = 20      # Maximum links to extract from search results\n",
    "MAX_URLS_TO_SCRAPE = 5         # Maximum URLs to scrape content from\n",
    "MAX_TEXT_LENGTH = 3000         # Maximum text length per scraped page\n",
    "SCRAPING_TIMEOUT = 15          # Timeout in seconds for web scraping\n",
    "\n",
    "# Agentic search configuration  \n",
    "MAX_STRATEGIC_SEARCHES = 3     # Number of strategic searches to plan\n",
    "SEARCH_CONTEXT_SIZE = \"medium\" # \"small\", \"medium\", \"large\" - affects search depth\n",
    "MAX_SEARCH_RESULTS = 10        # Maximum results per search\n",
    "REPORT_MIN_LENGTH = 1000       # Minimum report length in words\n",
    "\n",
    "# Deep search configuration\n",
    "ENABLE_DEEP_SEARCH = True      # Enable/disable deep search mode\n",
    "DEEP_SEARCH_ITERATIONS = 5     # Number of deep search iterations\n",
    "DEEP_SEARCH_REFINEMENT = True  # Enable search refinement\n",
    "\n",
    "# User agents for web scraping\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"\n",
    "]\n",
    "\n",
    "# Pydantic Models for structured data\n",
    "class WebSearchItem(BaseModel):\n",
    "    reason: str = Field(description=\"Reasoning for this search\")\n",
    "    query: str = Field(description=\"Search term to use\")\n",
    "\n",
    "class WebSearchPlan(BaseModel):\n",
    "    searches: List[WebSearchItem] = Field(description=\"List of strategic searches\")\n",
    "\n",
    "class ReportData(BaseModel):\n",
    "    short_summary: str = Field(description=\"Short summary\")\n",
    "    markdown_report: str = Field(description=\"Detailed report\")\n",
    "    follow_up_questions: List[str] = Field(description=\"Follow-up questions\")\n",
    "\n",
    "class CombinedSearchResults(BaseModel):\n",
    "    query: str\n",
    "    timestamp: str\n",
    "    explorer_results: List[Dict] = Field(description=\"Raw scraping results\")\n",
    "    agentic_results: ReportData = Field(description=\"AI-generated report\")\n",
    "    combined_summary: str = Field(description=\"Summary of both approaches\")\n",
    "\n",
    "# Evaluation Models\n",
    "class EvaluationCriteria(BaseModel):\n",
    "    accuracy_score: float = Field(description=\"Factual correctness (0-10)\")\n",
    "    completeness_score: float = Field(description=\"Comprehensive coverage (0-10)\")\n",
    "    relevance_score: float = Field(description=\"Query relevance (0-10)\")\n",
    "    clarity_score: float = Field(description=\"Organization and clarity (0-10)\")\n",
    "    depth_score: float = Field(description=\"Insight and analysis depth (0-10)\")\n",
    "\n",
    "class DetailedEvaluation(BaseModel):\n",
    "    criteria_scores: EvaluationCriteria\n",
    "    overall_score: float\n",
    "    strengths: List[str]\n",
    "    weaknesses: List[str]\n",
    "    missing_aspects: List[str]\n",
    "    recommendations: List[str]\n",
    "    confidence_level: str\n",
    "\n",
    "class UniversalEvaluation(BaseModel):\n",
    "    query: str\n",
    "    content_type: str  # \"explorer\", \"agentic\", \"combined\"\n",
    "    gpt_evaluation: DetailedEvaluation\n",
    "    claude_evaluation: DetailedEvaluation\n",
    "    consensus_score: float\n",
    "    final_recommendations: List[str]\n",
    "    evaluation_summary: str\n",
    "\n",
    "print(\"✅ Configuration and models loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd7ebdf",
   "metadata": {},
   "source": [
    "## CELL 3: HTML PARSING UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa7718d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HTML parsing utilities loaded!\n"
     ]
    }
   ],
   "source": [
    "class LinkExtractor(HTMLParser):\n",
    "    \"\"\"Custom HTML parser to extract links from web pages\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.links = []\n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'a':\n",
    "            for attr_name, attr_value in attrs:\n",
    "                if attr_name == 'href' and attr_value and attr_value.startswith('http'):\n",
    "                    self.links.append(attr_value)\n",
    "\n",
    "def simple_html_parse(html_content, max_links=None):\n",
    "    \"\"\"Simple HTML parser to extract links using built-in HTMLParser\"\"\"\n",
    "    if max_links is None:\n",
    "        max_links = MAX_LINKS_TO_EXTRACT\n",
    "        \n",
    "    parser = LinkExtractor()\n",
    "    try:\n",
    "        parser.feed(html_content)\n",
    "        return parser.links[:max_links]\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing HTML: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"✅ HTML parsing utilities loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef7cd4",
   "metadata": {},
   "source": [
    "## Search Engine APIs and Web Search Engine Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e382bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚫 Google search functions disabled (uncomment to enable)\n",
      "✅ DuckDuckGo search functions loaded!\n",
      "✅ Brave search functions loaded!\n",
      "✅ Web scraping functions loaded!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 4: GOOGLE SEARCH FUNCTIONS (DISABLED)\n",
    "# ============================================\n",
    "\n",
    "# Google API Configuration (commented out to avoid costs)\n",
    "# GOOGLE_API_URL = \"https://google-search-master-mega.p.rapidapi.com/search\"\n",
    "# GOOGLE_HEADERS = {\n",
    "#     \"x-rapidapi-key\": \"your-api-key-here\",\n",
    "#     \"x-rapidapi-host\": \"google-search-master-mega.p.rapidapi.com\"\n",
    "# }\n",
    "\n",
    "# def fetch_google_results(query=\"\"):\n",
    "#     \"\"\"\n",
    "#     Fetch search results from Google via RapidAPI using urllib.\n",
    "#     CURRENTLY DISABLED - Uncomment to re-enable Google search\n",
    "#     \"\"\"\n",
    "#     params = {\n",
    "#         \"q\": query,\n",
    "#         \"gl\": \"us\",\n",
    "#         \"hl\": \"en\", \n",
    "#         \"num\": \"10\",\n",
    "#         \"page\": \"1\"\n",
    "#     }\n",
    "#     \n",
    "#     url = GOOGLE_API_URL + \"?\" + urllib.parse.urlencode(params)\n",
    "#     \n",
    "#     try:\n",
    "#         req = urllib.request.Request(url)\n",
    "#         for key, value in GOOGLE_HEADERS.items():\n",
    "#             req.add_header(key, value)\n",
    "#         \n",
    "#         with urllib.request.urlopen(req, timeout=10) as response:\n",
    "#             data = response.read().decode('utf-8')\n",
    "#             return json.loads(data)\n",
    "#     \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching Google results: {e}\")\n",
    "#         return {}\n",
    "\n",
    "print(\"🚫 Google search functions disabled (uncomment to enable)\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 5: DUCKDUCKGO SEARCH FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def fetch_duckduckgo_links(query, results=None):\n",
    "    \"\"\"\n",
    "    Fetch search results from DuckDuckGo using direct HTML scraping.\n",
    "    \n",
    "    This function:\n",
    "    1. Builds DuckDuckGo search URL with query parameters\n",
    "    2. Sends HTTP request with random User-Agent to avoid blocking\n",
    "    3. Parses HTML response to extract result links\n",
    "    4. Filters out DuckDuckGo internal links\n",
    "    5. Returns clean list of external URLs\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        results = MAX_SEARCH_RESULTS\n",
    "        \n",
    "    endpoint = \"https://duckduckgo.com/html/\"\n",
    "    params = {\"q\": query}\n",
    "    url = endpoint + \"?\" + urllib.parse.urlencode(params)\n",
    "    \n",
    "    try:\n",
    "        # Create request with random user agent to avoid detection\n",
    "        req = urllib.request.Request(url)\n",
    "        req.add_header('User-Agent', random.choice(USER_AGENTS))\n",
    "        \n",
    "        # Fetch HTML content\n",
    "        with urllib.request.urlopen(req, timeout=10) as response:\n",
    "            html_content = response.read().decode('utf-8')\n",
    "        \n",
    "        # Extract links from HTML\n",
    "        links = simple_html_parse(html_content, results)\n",
    "        \n",
    "        # Filter out DuckDuckGo internal links\n",
    "        clean_links = [link for link in links if 'duckduckgo.com' not in link]\n",
    "        return clean_links[:results]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching DuckDuckGo results: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"✅ DuckDuckGo search functions loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 6: BRAVE SEARCH FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def fetch_brave_links(query, results=None):\n",
    "    \"\"\"\n",
    "    Fetch search results from Brave Search using direct HTML scraping.\n",
    "    \n",
    "    Similar to DuckDuckGo but targets Brave Search engine.\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        results = MAX_SEARCH_RESULTS\n",
    "        \n",
    "    endpoint = \"https://search.brave.com/search\"\n",
    "    params = {\"q\": query}\n",
    "    url = endpoint + \"?\" + urllib.parse.urlencode(params)\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(url)\n",
    "        req.add_header('User-Agent', random.choice(USER_AGENTS))\n",
    "        req.add_header('Accept-Encoding', 'identity')\n",
    "        \n",
    "        with urllib.request.urlopen(req, timeout=10) as response:\n",
    "            html_content = response.read().decode('utf-8')\n",
    "        \n",
    "        links = simple_html_parse(html_content, results)\n",
    "        clean_links = [link for link in links if 'search.brave.com' not in link]\n",
    "        return clean_links[:results]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Brave results: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"✅ Brave search functions loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 7: WEB SCRAPING FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def scrape_pages_text(urls, max_urls=None, source_name=\"Unknown\"):\n",
    "    \"\"\"\n",
    "    Extract text content from a list of URLs with detailed progress tracking.\n",
    "    \n",
    "    This function:\n",
    "    1. Iterates through provided URLs (limited to max_urls for performance)\n",
    "    2. Fetches HTML content from each URL\n",
    "    3. Strips HTML tags to extract plain text\n",
    "    4. Returns structured data with URL, text content, and metadata\n",
    "    \"\"\"\n",
    "    if max_urls is None:\n",
    "        max_urls = MAX_URLS_TO_SCRAPE\n",
    "        \n",
    "    collected = []\n",
    "    \n",
    "    print(f\"🔗 Starting to scrape {min(len(urls), max_urls)} URLs from {source_name}:\")\n",
    "    \n",
    "    for i, url in enumerate(urls[:max_urls], 1):\n",
    "        print(f\"\\n📄 [{i}/{min(len(urls), max_urls)}] Scraping: {url}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Create request with random user agent\n",
    "            req = urllib.request.Request(url)\n",
    "            req.add_header('User-Agent', random.choice(USER_AGENTS))\n",
    "            \n",
    "            # Fetch HTML content with timeout\n",
    "            with urllib.request.urlopen(req, timeout=SCRAPING_TIMEOUT) as response:\n",
    "                content_type = response.headers.get('content-type', '')\n",
    "                content_length = response.headers.get('content-length', 'Unknown')\n",
    "                html_content = response.read().decode('utf-8', errors='ignore')\n",
    "            \n",
    "            # Simple text extraction using regex\n",
    "            import re\n",
    "            # Remove script and style tags with content\n",
    "            text = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)\n",
    "            text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "            # Remove all HTML tags\n",
    "            text = re.sub(r'<[^>]+>', ' ', text)\n",
    "            # Clean up whitespace\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "            # Calculate processing time\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Store structured result\n",
    "            result = {\n",
    "                \"url\": url,\n",
    "                \"source_engine\": source_name, \n",
    "                \"text\": text[:MAX_TEXT_LENGTH],  # Limit text length to avoid memory issues\n",
    "                \"full_text_length\": len(text),\n",
    "                \"truncated_length\": len(text[:MAX_TEXT_LENGTH]),\n",
    "                \"scraped_at\": datetime.now().isoformat(),\n",
    "                \"processing_time_seconds\": round(processing_time, 2),\n",
    "                \"content_type\": content_type,\n",
    "                \"content_length\": content_length,\n",
    "                \"success\": True,\n",
    "                \"error\": False\n",
    "            }\n",
    "            \n",
    "            collected.append(result)\n",
    "            \n",
    "            # Success message with stats\n",
    "            print(f\"   ✅ Success! {len(text):,} chars extracted in {processing_time:.2f}s\")\n",
    "            if len(text) > MAX_TEXT_LENGTH:\n",
    "                print(f\"   ⚠️  Text truncated from {len(text):,} to {MAX_TEXT_LENGTH:,} chars\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            processing_time = time.time() - start_time\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Store error information for debugging\n",
    "            result = {\n",
    "                \"url\": url,\n",
    "                \"source_engine\": source_name,\n",
    "                \"text\": f\"Error: {error_msg}\",\n",
    "                \"full_text_length\": 0,\n",
    "                \"truncated_length\": 0,\n",
    "                \"scraped_at\": datetime.now().isoformat(),\n",
    "                \"processing_time_seconds\": round(processing_time, 2),\n",
    "                \"content_type\": \"error\",\n",
    "                \"content_length\": \"0\",\n",
    "                \"success\": False,\n",
    "                \"error\": True,\n",
    "                \"error_details\": error_msg\n",
    "            }\n",
    "            \n",
    "            collected.append(result)\n",
    "            print(f\"   ❌ Failed! {error_msg}\")\n",
    "    \n",
    "    # Summary for this batch\n",
    "    successful = len([item for item in collected if not item.get('error')])\n",
    "    failed = len([item for item in collected if item.get('error')])\n",
    "    total_chars = sum(item.get('full_text_length', 0) for item in collected if not item.get('error'))\n",
    "    \n",
    "    print(f\"\\n📊 {source_name} Scraping Summary:\")\n",
    "    print(f\"   ✅ Successful: {successful}/{len(collected)}\")\n",
    "    print(f\"   ❌ Failed: {failed}/{len(collected)}\")\n",
    "    print(f\"   📄 Total text extracted: {total_chars:,} characters\")\n",
    "    \n",
    "    return collected\n",
    "\n",
    "print(\"✅ Web scraping functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16889f2c",
   "metadata": {},
   "source": [
    "## CELL 8: EXPLORER SEARCH ORCHESTRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ec1a54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced explorer search orchestration loaded!\n"
     ]
    }
   ],
   "source": [
    "def perform_explorer_search(query, save_results=True):\n",
    "    \"\"\"\n",
    "    Enhanced explorer search with detailed link tracking and JSON saving.\n",
    "    \n",
    "    This function now only uses DuckDuckGo and Brave Search engines.\n",
    "    Google search has been commented out to avoid API costs and dependencies.\n",
    "    \n",
    "    Search Flow:\n",
    "    1. DuckDuckGo: Fetch links and scrape top results\n",
    "    2. Brave Search: Fetch links and scrape top results\n",
    "    3. Show detailed link information\n",
    "    4. Save comprehensive JSON data\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Starting Explorer Search for: {query}\")\n",
    "    print(f\"📊 Configuration: {MAX_SEARCH_RESULTS} results, {MAX_URLS_TO_SCRAPE} URLs to scrape\")\n",
    "    \n",
    "    all_data = []\n",
    "    search_metadata = {\n",
    "        \"query\": query,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"search_config\": {\n",
    "            \"max_search_results\": MAX_SEARCH_RESULTS,\n",
    "            \"max_urls_to_scrape\": MAX_URLS_TO_SCRAPE,\n",
    "            \"max_text_length\": MAX_TEXT_LENGTH,\n",
    "            \"scraping_timeout\": SCRAPING_TIMEOUT\n",
    "        },\n",
    "        \"search_engines\": [],\n",
    "        \"total_links_found\": 0,\n",
    "        \"total_scraped\": 0,\n",
    "        \"errors\": []\n",
    "    }\n",
    "    \n",
    "    # DuckDuckGo search (Active)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🦆 DUCKDUCKGO SEARCH\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    dd_urls = fetch_duckduckgo_links(query, MAX_SEARCH_RESULTS)\n",
    "    if dd_urls:\n",
    "        print(f\"✅ Found {len(dd_urls)} DuckDuckGo links:\")\n",
    "        for i, url in enumerate(dd_urls, 1):\n",
    "            print(f\"  {i:2d}. {url}\")\n",
    "        \n",
    "        print(f\"\\n🔗 Scraping top {min(len(dd_urls), MAX_URLS_TO_SCRAPE)} DuckDuckGo URLs...\")\n",
    "        dd_scraped = scrape_pages_text(dd_urls[:MAX_URLS_TO_SCRAPE], source_name=\"DuckDuckGo\")\n",
    "        for entry in dd_scraped:\n",
    "            entry[\"source\"] = \"duckduckgo_scraped\"\n",
    "            entry[\"type\"] = \"scraped_content\"\n",
    "            all_data.append(entry)\n",
    "        \n",
    "        search_metadata[\"search_engines\"].append({\n",
    "            \"name\": \"DuckDuckGo\",\n",
    "            \"links_found\": len(dd_urls),\n",
    "            \"links_scraped\": len(dd_scraped),\n",
    "            \"successful_scrapes\": len([item for item in dd_scraped if not item.get('error')]),\n",
    "            \"all_links\": dd_urls\n",
    "        })\n",
    "        search_metadata[\"total_links_found\"] += len(dd_urls)\n",
    "        search_metadata[\"total_scraped\"] += len(dd_scraped)\n",
    "    else:\n",
    "        print(\"❌ No DuckDuckGo links found\")\n",
    "        search_metadata[\"errors\"].append(\"DuckDuckGo: No links found\")\n",
    "    \n",
    "    # Brave Search (Active)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🛡️ BRAVE SEARCH\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    brave_urls = fetch_brave_links(query, MAX_SEARCH_RESULTS)\n",
    "    if brave_urls:\n",
    "        print(f\"✅ Found {len(brave_urls)} Brave links:\")\n",
    "        for i, url in enumerate(brave_urls, 1):\n",
    "            print(f\"  {i:2d}. {url}\")\n",
    "        \n",
    "        print(f\"\\n🔗 Scraping top {min(len(brave_urls), MAX_URLS_TO_SCRAPE)} Brave URLs...\")\n",
    "        brave_scraped = scrape_pages_text(brave_urls[:MAX_URLS_TO_SCRAPE], source_name=\"Brave\")\n",
    "        for entry in brave_scraped:\n",
    "            entry[\"source\"] = \"brave_scraped\"\n",
    "            entry[\"type\"] = \"scraped_content\"\n",
    "            all_data.append(entry)\n",
    "        \n",
    "        search_metadata[\"search_engines\"].append({\n",
    "            \"name\": \"Brave\",\n",
    "            \"links_found\": len(brave_urls),\n",
    "            \"links_scraped\": len(brave_scraped),\n",
    "            \"successful_scrapes\": len([item for item in brave_scraped if not item.get('error')]),\n",
    "            \"all_links\": brave_urls\n",
    "        })\n",
    "        search_metadata[\"total_links_found\"] += len(brave_urls)\n",
    "        search_metadata[\"total_scraped\"] += len(brave_scraped)\n",
    "    else:\n",
    "        print(\"❌ No Brave links found\")\n",
    "        search_metadata[\"errors\"].append(\"Brave: No links found\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📊 EXPLORER SEARCH SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"🔍 Query: {query}\")\n",
    "    print(f\"📊 Total links found: {search_metadata['total_links_found']}\")\n",
    "    print(f\"🔗 Total pages scraped: {search_metadata['total_scraped']}\")\n",
    "    print(f\"✅ Successful scrapes: {len([item for item in all_data if not item.get('error')])}\")\n",
    "    print(f\"❌ Failed scrapes: {len([item for item in all_data if item.get('error')])}\")\n",
    "    print(f\"🌐 Search engines used: {', '.join([engine['name'] for engine in search_metadata['search_engines']])}\")\n",
    "    \n",
    "    # Create comprehensive explorer results\n",
    "    explorer_results = {\n",
    "        \"metadata\": search_metadata,\n",
    "        \"scraped_data\": all_data,\n",
    "        \"summary\": {\n",
    "            \"query\": query,\n",
    "            \"total_sources\": len(all_data),\n",
    "            \"successful_scrapes\": len([item for item in all_data if not item.get('error')]),\n",
    "            \"total_text_length\": sum(len(item.get('text', '')) for item in all_data if not item.get('error')),\n",
    "            \"search_engines_used\": [engine['name'] for engine in search_metadata['search_engines']],\n",
    "            \"completed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save explorer results to JSON if requested\n",
    "    if save_results:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"explorer_search_{timestamp}.json\"\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        with open(filepath, \"w\", encoding='utf-8') as f:\n",
    "            json.dump(explorer_results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"💾 Explorer results saved to: {filepath}\")\n",
    "        print(f\"📄 File size: {os.path.getsize(filepath)} bytes\")\n",
    "    \n",
    "    print(\"✅ Explorer search completed successfully!\")\n",
    "    return all_data, explorer_results\n",
    "\n",
    "print(\"✅ Enhanced explorer search orchestration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a01ed3",
   "metadata": {},
   "source": [
    "## CELL 9: AGENTIC SEARCH AGENT DEFINITIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent instructions\n",
    "SEARCH_INSTRUCTIONS = f\"\"\"You are a research assistant. Search the web and provide a concise 200-word summary of key findings. Focus on facts, insights, and actionable information. Use up to {MAX_SEARCH_RESULTS} search results for comprehensive coverage.\"\"\"\n",
    "\n",
    "PLANNER_INSTRUCTIONS = f\"\"\"You are a research planner. Create {MAX_STRATEGIC_SEARCHES} strategic web searches to comprehensively answer the query. Each search should target a different aspect or angle of the topic.\"\"\"\n",
    "\n",
    "WRITER_INSTRUCTIONS = f\"\"\"You are a senior researcher. Create a comprehensive, well-structured markdown report ({REPORT_MIN_LENGTH}+ words) synthesizing all research findings. Include executive summary, main findings, analysis, and conclusions. Ensure depth and actionable insights.\"\"\"\n",
    "\n",
    "# Initialize AI agents\n",
    "search_agent = Agent(\n",
    "    name=\"Search agent\",\n",
    "    instructions=SEARCH_INSTRUCTIONS,\n",
    "    tools=[WebSearchTool(search_context_size=SEARCH_CONTEXT_SIZE)],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    model_settings=ModelSettings(tool_choice=\"required\"),\n",
    ")\n",
    "\n",
    "planner_agent = Agent(\n",
    "    name=\"PlannerAgent\",\n",
    "    instructions=PLANNER_INSTRUCTIONS,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=WebSearchPlan,\n",
    ")\n",
    "\n",
    "writer_agent = Agent(\n",
    "    name=\"WriterAgent\",\n",
    "    instructions=WRITER_INSTRUCTIONS,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=ReportData,\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# DEEP SEARCH AGENTS (NOW ENABLED!)\n",
    "# ============================================\n",
    "\n",
    "# Deep Search ChatGPT Agent\n",
    "DEEP_CHATGPT_INSTRUCTIONS = f\"\"\"You are an advanced ChatGPT researcher performing deep analysis. \n",
    "Conduct {DEEP_SEARCH_ITERATIONS} iterations of research with progressive refinement. \n",
    "For each iteration:\n",
    "1. Analyze previous findings\n",
    "2. Identify knowledge gaps\n",
    "3. Perform targeted searches\n",
    "4. Synthesize insights\n",
    "5. Plan next iteration\n",
    "\n",
    "Provide comprehensive analysis with critical evaluation, multiple perspectives, and actionable recommendations.\"\"\"\n",
    "\n",
    "deep_chatgpt_agent = Agent(\n",
    "    name=\"DeepChatGPTAgent\",\n",
    "    instructions=DEEP_CHATGPT_INSTRUCTIONS,\n",
    "    tools=[WebSearchTool(search_context_size=\"large\")],\n",
    "    model=\"gpt-4o\",  # Use more powerful model for deep search\n",
    "    model_settings=ModelSettings(temperature=0.7, tool_choice=\"required\"),\n",
    ")\n",
    "\n",
    "# Deep Search Claude Agent  \n",
    "DEEP_CLAUDE_INSTRUCTIONS = f\"\"\"You are an advanced Claude researcher performing deep analytical research.\n",
    "Execute {DEEP_SEARCH_ITERATIONS} research cycles with systematic refinement.\n",
    "\n",
    "Research methodology:\n",
    "1. Systematic information gathering\n",
    "2. Critical source evaluation\n",
    "3. Multi-angle analysis\n",
    "4. Gap identification and targeted follow-up\n",
    "5. Comprehensive synthesis\n",
    "\n",
    "Emphasize critical thinking, source credibility, logical reasoning, and practical implications.\"\"\"\n",
    "\n",
    "# Note: This would require Claude API integration\n",
    "# deep_claude_agent = Agent(\n",
    "#     name=\"DeepClaudeAgent\", \n",
    "#     instructions=DEEP_CLAUDE_INSTRUCTIONS,\n",
    "#     tools=[WebSearchTool(search_context_size=\"large\")],\n",
    "#     model=\"claude-3-5-sonnet-20241022\",  # Would need Claude API setup\n",
    "#     model_settings=ModelSettings(temperature=0.6),\n",
    "# )\n",
    "\n",
    "# Comparative Deep Search Agent\n",
    "COMPARATIVE_INSTRUCTIONS = f\"\"\"You are a comparative research specialist. \n",
    "Compare and contrast findings from multiple AI perspectives (ChatGPT vs Claude approaches).\n",
    "\n",
    "Analysis framework:\n",
    "1. Identify convergent findings (high confidence)\n",
    "2. Highlight divergent perspectives (requires investigation)\n",
    "3. Evaluate evidence quality and source reliability\n",
    "4. Synthesize balanced conclusions\n",
    "5. Recommend areas for further research\n",
    "\n",
    "Provide meta-analysis of research quality and reliability.\"\"\"\n",
    "\n",
    "comparative_agent = Agent(\n",
    "    name=\"ComparativeAgent\",\n",
    "    instructions=COMPARATIVE_INSTRUCTIONS,\n",
    "    model=\"gpt-4o\",\n",
    "    output_type=ReportData,\n",
    ")\n",
    "\n",
    "print(\"✅ Agentic search agents initialized!\")\n",
    "print(f\"📊 Configuration: {MAX_STRATEGIC_SEARCHES} searches, {SEARCH_CONTEXT_SIZE} context, {REPORT_MIN_LENGTH}+ word reports\")\n",
    "print(\"🔥 Deep search agents ENABLED and ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900f18c7",
   "metadata": {},
   "source": [
    "## CELL 10: AGENTIC SEARCH EXECUTION AND DEEP SEARCH ENABLED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66203cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def perform_agentic_search(query: str):\n",
    "    \"\"\"\n",
    "    Enhanced agentic search pipeline.\n",
    "    \n",
    "    Note: This uses OpenAI Agents SDK which has its own built-in web search capabilities,\n",
    "    separate from the explorer search engines above.\n",
    "    \"\"\"\n",
    "    print(f\"🤖 Starting Agentic Search for: {query}\")\n",
    "    \n",
    "    with trace(\"Agentic Search Pipeline\"):\n",
    "        # Plan searches\n",
    "        print(\"📋 Planning strategic searches...\")\n",
    "        result = await Runner.run(planner_agent, f\"Query: {query}\")\n",
    "        search_plan = result.final_output\n",
    "        \n",
    "        # Execute searches\n",
    "        print(f\"🌐 Executing {len(search_plan.searches)} strategic searches...\")\n",
    "        search_tasks = []\n",
    "        for item in search_plan.searches:\n",
    "            input_text = f\"Search term: {item.query}\\nReason: {item.reason}\"\n",
    "            search_tasks.append(Runner.run(search_agent, input_text))\n",
    "        \n",
    "        search_results = await asyncio.gather(*search_tasks)\n",
    "        search_summaries = [result.final_output for result in search_results]\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        print(\"📝 Synthesizing comprehensive report...\")\n",
    "        input_text = f\"Original query: {query}\\nResearch findings: {search_summaries}\"\n",
    "        report_result = await Runner.run(writer_agent, input_text)\n",
    "        \n",
    "        print(\"✅ Agentic search completed\")\n",
    "        return report_result.final_output, search_plan, search_summaries\n",
    "\n",
    "# Helper functions for agentic search components\n",
    "async def plan_searches(query: str):\n",
    "    \"\"\"Plan strategic searches for the query\"\"\"\n",
    "    result = await Runner.run(planner_agent, f\"Query: {query}\")\n",
    "    return result.final_output\n",
    "\n",
    "async def perform_searches(search_plan):\n",
    "    \"\"\"Execute planned searches\"\"\"\n",
    "    search_tasks = []\n",
    "    for item in search_plan.searches:\n",
    "        input_text = f\"Search term: {item.query}\\nReason: {item.reason}\"\n",
    "        search_tasks.append(Runner.run(search_agent, input_text))\n",
    "    \n",
    "    search_results = await asyncio.gather(*search_tasks)\n",
    "    return [result.final_output for result in search_results]\n",
    "\n",
    "async def write_report(query: str, search_results):\n",
    "    \"\"\"Generate comprehensive report from search results\"\"\"\n",
    "    input_text = f\"Original query: {query}\\nResearch findings: {search_results}\"\n",
    "    report_result = await Runner.run(writer_agent, input_text)\n",
    "    return report_result.final_output\n",
    "\n",
    "print(\"✅ Agentic search execution functions loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# CELL 10B: DEEP SEARCH EXECUTION (NOW ENABLED!)\n",
    "# ============================================\n",
    "\n",
    "async def perform_deep_chatgpt_search(query: str):\n",
    "    \"\"\"\n",
    "    Perform deep iterative search using ChatGPT with progressive refinement\n",
    "    \"\"\"\n",
    "    print(f\"🧠 Starting Deep ChatGPT Search for: {query}\")\n",
    "    \n",
    "    with trace(\"Deep ChatGPT Search Pipeline\"):\n",
    "        search_history = []\n",
    "        refined_insights = []\n",
    "        \n",
    "        for iteration in range(DEEP_SEARCH_ITERATIONS):\n",
    "            print(f\"🔄 ChatGPT Iteration {iteration + 1}/{DEEP_SEARCH_ITERATIONS}\")\n",
    "            \n",
    "            # Build context from previous iterations\n",
    "            context = f\"Query: {query}\\n\"\n",
    "            if search_history:\n",
    "                context += f\"Previous findings: {search_history}\\n\"\n",
    "            context += f\"Focus for iteration {iteration + 1}: Identify gaps and explore new angles\"\n",
    "            \n",
    "            # Perform search with context\n",
    "            result = await Runner.run(deep_chatgpt_agent, context)\n",
    "            search_summary = result.final_output\n",
    "            \n",
    "            search_history.append({\n",
    "                \"iteration\": iteration + 1,\n",
    "                \"findings\": search_summary,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            print(f\"✓ Iteration {iteration + 1} completed\")\n",
    "            \n",
    "            # Brief pause between iterations\n",
    "            if iteration < DEEP_SEARCH_ITERATIONS - 1:\n",
    "                await asyncio.sleep(2)\n",
    "        \n",
    "        # Synthesize final report\n",
    "        print(\"📝 Synthesizing ChatGPT deep research findings...\")\n",
    "        synthesis_input = f\"Deep research query: {query}\\nIterative findings: {search_history}\\nCreate comprehensive final report.\"\n",
    "        final_result = await Runner.run(writer_agent, synthesis_input)\n",
    "        \n",
    "        return {\n",
    "            \"type\": \"deep_chatgpt\",\n",
    "            \"query\": query,\n",
    "            \"iterations\": DEEP_SEARCH_ITERATIONS,\n",
    "            \"search_history\": search_history,\n",
    "            \"final_report\": final_result.final_output,\n",
    "            \"completed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "async def perform_deep_claude_search(query: str):\n",
    "    \"\"\"\n",
    "    Perform deep systematic search using Claude methodology\n",
    "    Note: This requires Claude API integration - currently simulated with GPT\n",
    "    \"\"\"\n",
    "    print(f\"🎭 Starting Deep Claude Search for: {query}\")\n",
    "    \n",
    "    # Since we don't have Claude agent set up, we'll simulate with a Claude-style approach using GPT\n",
    "    claude_style_instructions = f\"\"\"You are simulating Claude's analytical approach. \n",
    "    Perform systematic research with emphasis on:\n",
    "    - Critical evaluation of sources\n",
    "    - Logical reasoning chains  \n",
    "    - Multiple perspective analysis\n",
    "    - Methodical gap identification\n",
    "    - Conservative confidence levels\n",
    "    \n",
    "    Query: {query}\n",
    "    Iterations: {DEEP_SEARCH_ITERATIONS}\"\"\"\n",
    "    \n",
    "    with trace(\"Deep Claude-Style Search Pipeline\"):\n",
    "        analysis_phases = []\n",
    "        \n",
    "        for phase in range(DEEP_SEARCH_ITERATIONS):\n",
    "            print(f\"🔍 Claude-style Analysis Phase {phase + 1}/{DEEP_SEARCH_ITERATIONS}\")\n",
    "            \n",
    "            phase_context = f\"{claude_style_instructions}\\nPhase {phase + 1}: \"\n",
    "            if phase == 0:\n",
    "                phase_context += \"Initial comprehensive search and source gathering\"\n",
    "            elif phase == 1:\n",
    "                phase_context += \"Critical evaluation and credibility assessment\"\n",
    "            elif phase == 2:\n",
    "                phase_context += \"Multi-angle analysis and perspective gathering\"\n",
    "            elif phase == 3:\n",
    "                phase_context += \"Gap identification and targeted investigation\"\n",
    "            else:\n",
    "                phase_context += \"Synthesis and final verification\"\n",
    "            \n",
    "            # Use search agent with Claude-style prompting\n",
    "            result = await Runner.run(search_agent, phase_context)\n",
    "            phase_findings = result.final_output\n",
    "            \n",
    "            analysis_phases.append({\n",
    "                \"phase\": phase + 1,\n",
    "                \"focus\": phase_context.split(\": \")[-1],\n",
    "                \"findings\": phase_findings,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            print(f\"✓ Phase {phase + 1} completed\")\n",
    "            await asyncio.sleep(1)\n",
    "        \n",
    "        # Final synthesis with Claude-style rigor\n",
    "        print(\"📊 Performing Claude-style systematic synthesis...\")\n",
    "        synthesis_prompt = f\"\"\"Synthesize research with Claude-style analytical rigor:\n",
    "        \n",
    "        Query: {query}\n",
    "        Research phases: {analysis_phases}\n",
    "        \n",
    "        Apply systematic evaluation:\n",
    "        1. Evidence quality assessment\n",
    "        2. Logical consistency check\n",
    "        3. Confidence level assignment\n",
    "        4. Alternative perspective consideration\n",
    "        5. Practical implication analysis\"\"\"\n",
    "        \n",
    "        final_result = await Runner.run(writer_agent, synthesis_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"type\": \"deep_claude_style\", \n",
    "            \"query\": query,\n",
    "            \"phases\": DEEP_SEARCH_ITERATIONS,\n",
    "            \"analysis_phases\": analysis_phases,\n",
    "            \"final_report\": final_result.final_output,\n",
    "            \"completed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "async def perform_comparative_deep_search(query: str):\n",
    "    \"\"\"\n",
    "    Perform both ChatGPT and Claude-style deep searches, then compare findings\n",
    "    \"\"\"\n",
    "    print(f\"⚖️ Starting Comparative Deep Search for: {query}\")\n",
    "    \n",
    "    with trace(\"Comparative Deep Search Pipeline\"):\n",
    "        # Run both deep searches in parallel\n",
    "        print(\"🔄 Running ChatGPT and Claude-style searches in parallel...\")\n",
    "        chatgpt_task = asyncio.create_task(perform_deep_chatgpt_search(query))\n",
    "        claude_task = asyncio.create_task(perform_deep_claude_search(query))\n",
    "        \n",
    "        chatgpt_results, claude_results = await asyncio.gather(chatgpt_task, claude_task)\n",
    "        \n",
    "        # Comparative analysis\n",
    "        print(\"📊 Performing comparative analysis...\")\n",
    "        comparison_prompt = f\"\"\"Perform comparative analysis of two AI research approaches:\n",
    "        \n",
    "        Query: {query}\n",
    "        \n",
    "        ChatGPT Approach Results:\n",
    "        {chatgpt_results['final_report'].markdown_report}\n",
    "        \n",
    "        Claude-Style Approach Results:  \n",
    "        {claude_results['final_report'].markdown_report}\n",
    "        \n",
    "        Compare and analyze:\n",
    "        1. Convergent findings (high confidence conclusions)\n",
    "        2. Divergent perspectives (areas of disagreement)\n",
    "        3. Methodology differences\n",
    "        4. Evidence quality and source coverage\n",
    "        5. Practical implications and recommendations\n",
    "        6. Areas requiring further investigation\n",
    "        \n",
    "        Provide meta-analysis of research quality and synthesized recommendations.\"\"\"\n",
    "        \n",
    "        comparative_result = await Runner.run(comparative_agent, comparison_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"type\": \"comparative_deep_search\",\n",
    "            \"query\": query,\n",
    "            \"chatgpt_results\": chatgpt_results,\n",
    "            \"claude_results\": claude_results,\n",
    "            \"comparative_analysis\": comparative_result.final_output,\n",
    "            \"total_iterations\": DEEP_SEARCH_ITERATIONS * 2,\n",
    "            \"completed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "print(\"✅ Deep search execution functions loaded!\")\n",
    "print(f\"🔧 Deep search configuration: {DEEP_SEARCH_ITERATIONS} iterations, refinement={'enabled' if DEEP_SEARCH_REFINEMENT else 'disabled'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d576d529",
   "metadata": {},
   "source": [
    "## CELL 11: UNIVERSAL JSON EVALUATOR SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3db68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_from_json(json_data: Dict, query: str) -> Dict:\n",
    "    \"\"\"Universal content extractor that handles any JSON structure\"\"\"\n",
    "    extracted = {\n",
    "        \"query\": query,\n",
    "        \"content_type\": \"unknown\",\n",
    "        \"text_content\": \"\",\n",
    "        \"structured_data\": {},\n",
    "        \"source_info\": {}\n",
    "    }\n",
    "    \n",
    "    # Detect content type and extract accordingly\n",
    "    if \"agentic_results\" in json_data and \"explorer_results\" in json_data:\n",
    "        # Combined results\n",
    "        extracted[\"content_type\"] = \"combined\"\n",
    "        \n",
    "        # Extract agentic content\n",
    "        if \"markdown_report\" in json_data.get(\"agentic_results\", {}):\n",
    "            extracted[\"text_content\"] += json_data[\"agentic_results\"][\"markdown_report\"]\n",
    "        \n",
    "        # Extract explorer summary\n",
    "        explorer_results = json_data.get(\"explorer_results\", [])\n",
    "        scraped_texts = []\n",
    "        for item in explorer_results:\n",
    "            if isinstance(item, dict) and \"text\" in item and not item.get(\"error\"):\n",
    "                scraped_texts.append(item[\"text\"][:500])  # First 500 chars\n",
    "        \n",
    "        if scraped_texts:\n",
    "            extracted[\"text_content\"] += f\"\\n\\n## Explorer Findings:\\n\" + \"\\n\".join(scraped_texts)\n",
    "        \n",
    "        extracted[\"structured_data\"] = {\n",
    "            \"agentic_summary\": json_data.get(\"agentic_results\", {}).get(\"short_summary\", \"\"),\n",
    "            \"explorer_count\": len(explorer_results),\n",
    "            \"sources_scraped\": len([item for item in explorer_results if \"url\" in item])\n",
    "        }\n",
    "    \n",
    "    elif \"report\" in json_data and \"markdown_report\" in json_data[\"report\"]:\n",
    "        # Agentic-only results\n",
    "        extracted[\"content_type\"] = \"agentic\"\n",
    "        extracted[\"text_content\"] = json_data[\"report\"][\"markdown_report\"]\n",
    "        extracted[\"structured_data\"] = {\n",
    "            \"summary\": json_data[\"report\"].get(\"short_summary\", \"\"),\n",
    "            \"follow_up_questions\": json_data[\"report\"].get(\"follow_up_questions\", [])\n",
    "        }\n",
    "    \n",
    "    elif isinstance(json_data, list) or \"data\" in json_data:\n",
    "        # Explorer-only results\n",
    "        extracted[\"content_type\"] = \"explorer\"\n",
    "        \n",
    "        # Handle list format or nested data\n",
    "        data_list = json_data if isinstance(json_data, list) else json_data.get(\"data\", [])\n",
    "        \n",
    "        explorer_texts = []\n",
    "        source_count = 0\n",
    "        \n",
    "        for item in data_list:\n",
    "            if isinstance(item, dict):\n",
    "                if \"text\" in item and not item.get(\"error\", False):\n",
    "                    explorer_texts.append(f\"Source: {item.get('url', 'Unknown')}\\n{item['text'][:800]}\")\n",
    "                    source_count += 1\n",
    "                elif \"data\" in item:\n",
    "                    # Handle nested structures like Google API results\n",
    "                    source_count += 1\n",
    "        \n",
    "        extracted[\"text_content\"] = \"\\n\\n---\\n\\n\".join(explorer_texts[:5])  # Limit to 5 sources\n",
    "        extracted[\"structured_data\"] = {\n",
    "            \"total_sources\": source_count,\n",
    "            \"scraped_sources\": len([item for item in data_list if isinstance(item, dict) and \"text\" in item])\n",
    "        }\n",
    "    \n",
    "    return extracted\n",
    "\n",
    "async def evaluate_with_gpt(content: Dict) -> DetailedEvaluation:\n",
    "    \"\"\"Enhanced GPT evaluation with retry logic and error handling\"\"\"\n",
    "    evaluation_prompt = f\"\"\"You are an expert research evaluator. Evaluate this research content:\n",
    "\n",
    "Query: {content['query']}\n",
    "Content Type: {content['content_type']}\n",
    "\n",
    "Research Content:\n",
    "{content['text_content'][:4000]}  # Limit for token constraints\n",
    "\n",
    "Additional Context:\n",
    "{json.dumps(content['structured_data'], indent=2)}\n",
    "\n",
    "Evaluate based on:\n",
    "1. Accuracy - Factual correctness and reliability\n",
    "2. Completeness - Coverage of the topic\n",
    "3. Relevance - Direct relationship to the query\n",
    "4. Clarity - Organization and readability  \n",
    "5. Depth - Level of insight and analysis\n",
    "\n",
    "Consider the content type when evaluating. Explorer results should be judged on breadth and source diversity, while agentic results should be judged on synthesis and insight quality.\"\"\"\n",
    "\n",
    "    max_retries = 3\n",
    "    retry_delay = 2\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"🤖 GPT-4 evaluation attempt {attempt + 1}/{max_retries}...\")\n",
    "            \n",
    "            response = openai_client.beta.chat.completions.parse(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a critical research evaluator. Provide detailed, constructive assessment.\"},\n",
    "                    {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "                ],\n",
    "                response_format=DetailedEvaluation,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.parsed\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            print(f\"⚠️ GPT-4 attempt {attempt + 1} failed: {error_str}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"🔄 Retrying GPT-4 evaluation in {retry_delay} seconds...\")\n",
    "                await asyncio.sleep(retry_delay)\n",
    "                retry_delay *= 2\n",
    "                continue\n",
    "            else:\n",
    "                print(\"🚫 GPT-4 persistently failing, using fallback evaluation...\")\n",
    "                return create_fallback_gpt_evaluation(content)\n",
    "    \n",
    "    return create_fallback_gpt_evaluation(content)\n",
    "\n",
    "def create_fallback_gpt_evaluation(content: Dict) -> DetailedEvaluation:\n",
    "    \"\"\"Create a fallback evaluation when GPT-4 API is unavailable\"\"\"\n",
    "    print(\"🔧 Generating fallback GPT-4 evaluation...\")\n",
    "    \n",
    "    text_length = len(content.get('text_content', ''))\n",
    "    \n",
    "    # Heuristic scoring\n",
    "    accuracy_score = 6.5\n",
    "    completeness_score = min(8.5, 5.0 + (text_length / 900))\n",
    "    relevance_score = 7.5\n",
    "    clarity_score = 7.0\n",
    "    depth_score = min(7.5, 4.0 + (text_length / 700))\n",
    "    \n",
    "    overall_score = (accuracy_score + completeness_score + relevance_score + clarity_score + depth_score) / 5\n",
    "    \n",
    "    return DetailedEvaluation(\n",
    "        criteria_scores=EvaluationCriteria(\n",
    "            accuracy_score=accuracy_score,\n",
    "            completeness_score=completeness_score,\n",
    "            relevance_score=relevance_score,\n",
    "            clarity_score=clarity_score,\n",
    "            depth_score=depth_score\n",
    "        ),\n",
    "        overall_score=round(overall_score, 1),\n",
    "        strengths=[\n",
    "            \"Comprehensive information gathering\",\n",
    "            \"Well-organized content structure\",\n",
    "            \"Good coverage of query topics\"\n",
    "        ],\n",
    "        weaknesses=[\n",
    "            \"Could not verify with GPT-4 API\",\n",
    "            \"Limited by automated evaluation\",\n",
    "            \"May need human expert review\"\n",
    "        ],\n",
    "        missing_aspects=[\n",
    "            \"Expert domain knowledge validation\",\n",
    "            \"Advanced fact-checking\",\n",
    "            \"Nuanced analysis capabilities\"\n",
    "        ],\n",
    "        recommendations=[\n",
    "            \"Re-evaluate when API is available\",\n",
    "            \"Consider manual expert review\",\n",
    "            \"Validate key claims independently\"\n",
    "        ],\n",
    "        confidence_level=\"Medium\"\n",
    "    )\n",
    "\n",
    "async def evaluate_with_claude(content: Dict) -> DetailedEvaluation:\n",
    "    \"\"\"Enhanced Claude evaluation with retry logic and fallback\"\"\"\n",
    "    evaluation_prompt = f\"\"\"Evaluate this research content as an expert evaluator:\n",
    "\n",
    "Query: {content['query']}\n",
    "Content Type: {content['content_type']}\n",
    "\n",
    "Research Content:\n",
    "{content['text_content'][:4000]}\n",
    "\n",
    "Additional Context:\n",
    "{json.dumps(content['structured_data'], indent=2)}\n",
    "\n",
    "Evaluate on accuracy, completeness, relevance, clarity, and depth (0-10 each).\n",
    "Adapt your evaluation criteria based on content type.\n",
    "\n",
    "Return in JSON format:\n",
    "{{\n",
    "    \"criteria_scores\": {{\n",
    "        \"accuracy_score\": <0-10>,\n",
    "        \"completeness_score\": <0-10>,\n",
    "        \"relevance_score\": <0-10>,\n",
    "        \"clarity_score\": <0-10>,\n",
    "        \"depth_score\": <0-10>\n",
    "    }},\n",
    "    \"overall_score\": <0-10>,\n",
    "    \"strengths\": [\"strength1\", \"strength2\", ...],\n",
    "    \"weaknesses\": [\"weakness1\", \"weakness2\", ...],\n",
    "    \"missing_aspects\": [\"aspect1\", \"aspect2\", ...],\n",
    "    \"recommendations\": [\"recommendation1\", \"recommendation2\", ...],\n",
    "    \"confidence_level\": \"High/Medium/Low\"\n",
    "}}\"\"\"\n",
    "\n",
    "    max_retries = 3\n",
    "    retry_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"🧠 Claude evaluation attempt {attempt + 1}/{max_retries}...\")\n",
    "            \n",
    "            response = claude_client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=2000,\n",
    "                temperature=0.3,\n",
    "                messages=[{\"role\": \"user\", \"content\": evaluation_prompt}]\n",
    "            )\n",
    "            \n",
    "            evaluation_json = json.loads(response.content[0].text)\n",
    "            return DetailedEvaluation(**evaluation_json)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            print(f\"⚠️ Claude attempt {attempt + 1} failed: {error_str}\")\n",
    "            \n",
    "            # Check if it's an overload error\n",
    "            if \"overloaded\" in error_str.lower() or \"529\" in error_str:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"🔄 Claude overloaded, retrying in {retry_delay} seconds...\")\n",
    "                    await asyncio.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Exponential backoff\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"🚫 Claude persistently overloaded, using fallback evaluation...\")\n",
    "                    return create_fallback_claude_evaluation(content)\n",
    "            else:\n",
    "                # For other errors, try fallback immediately\n",
    "                print(f\"🚫 Claude error: {error_str}, using fallback evaluation...\")\n",
    "                return create_fallback_claude_evaluation(content)\n",
    "    \n",
    "    # If all retries failed, use fallback\n",
    "    return create_fallback_claude_evaluation(content)\n",
    "\n",
    "def create_fallback_claude_evaluation(content: Dict) -> DetailedEvaluation:\n",
    "    \"\"\"Create a fallback evaluation when Claude API is unavailable\"\"\"\n",
    "    print(\"🔧 Generating fallback Claude evaluation...\")\n",
    "    \n",
    "    # Create reasonable fallback scores based on content analysis\n",
    "    text_length = len(content.get('text_content', ''))\n",
    "    \n",
    "    # Basic heuristic scoring\n",
    "    completeness_score = min(8.0, 4.0 + (text_length / 1000))  # Longer content = more complete\n",
    "    relevance_score = 7.0  # Assume reasonable relevance\n",
    "    clarity_score = 6.5    # Neutral clarity score\n",
    "    accuracy_score = 6.0   # Conservative accuracy score\n",
    "    depth_score = min(7.0, 3.0 + (text_length / 800))\n",
    "    \n",
    "    overall_score = (completeness_score + relevance_score + clarity_score + accuracy_score + depth_score) / 5\n",
    "    \n",
    "    return DetailedEvaluation(\n",
    "        criteria_scores=EvaluationCriteria(\n",
    "            accuracy_score=accuracy_score,\n",
    "            completeness_score=completeness_score,\n",
    "            relevance_score=relevance_score,\n",
    "            clarity_score=clarity_score,\n",
    "            depth_score=depth_score\n",
    "        ),\n",
    "        overall_score=round(overall_score, 1),\n",
    "        strengths=[\n",
    "            \"Content covers multiple aspects of the query\",\n",
    "            \"Information appears well-structured\",\n",
    "            \"Reasonable depth of analysis\"\n",
    "        ],\n",
    "        weaknesses=[\n",
    "            \"Could not verify accuracy with Claude API\",\n",
    "            \"May benefit from additional source validation\",\n",
    "            \"Evaluation limited by API availability\"\n",
    "        ],\n",
    "        missing_aspects=[\n",
    "            \"Real-time fact verification\",\n",
    "            \"Cross-reference validation\",\n",
    "            \"Expert domain analysis\"\n",
    "        ],\n",
    "        recommendations=[\n",
    "            \"Verify key facts with additional sources\",\n",
    "            \"Consider expert review for technical content\",\n",
    "            \"Re-evaluate when Claude API is available\"\n",
    "        ],\n",
    "        confidence_level=\"Medium\"\n",
    "    )\n",
    "\n",
    "def calculate_consensus(gpt_eval: DetailedEvaluation, claude_eval: DetailedEvaluation) -> Dict:\n",
    "    \"\"\"Calculate consensus between evaluations with fallback handling\"\"\"\n",
    "    gpt_scores = gpt_eval.criteria_scores.dict()\n",
    "    claude_scores = claude_eval.criteria_scores.dict()\n",
    "    \n",
    "    score_differences = {}\n",
    "    total_diff = 0\n",
    "    \n",
    "    for criterion, gpt_score in gpt_scores.items():\n",
    "        claude_score = claude_scores[criterion]\n",
    "        diff = abs(gpt_score - claude_score)\n",
    "        score_differences[criterion] = diff\n",
    "        total_diff += diff\n",
    "    \n",
    "    avg_difference = total_diff / len(score_differences)\n",
    "    consensus_score = max(0, 10 - avg_difference)\n",
    "    \n",
    "    # Combine recommendations\n",
    "    all_recommendations = list(set(gpt_eval.recommendations + claude_eval.recommendations))\n",
    "    \n",
    "    # Check if either evaluation was a fallback\n",
    "    gpt_fallback = gpt_eval.confidence_level == \"Medium\" and \"Could not verify with GPT-4 API\" in gpt_eval.weaknesses\n",
    "    claude_fallback = claude_eval.confidence_level == \"Medium\" and \"Could not verify accuracy with Claude API\" in claude_eval.weaknesses\n",
    "    \n",
    "    evaluation_notes = []\n",
    "    if gpt_fallback:\n",
    "        evaluation_notes.append(\"GPT-4 evaluation used fallback due to API issues\")\n",
    "    if claude_fallback:\n",
    "        evaluation_notes.append(\"Claude evaluation used fallback due to API overload\")\n",
    "    \n",
    "    return {\n",
    "        \"consensus_score\": round(consensus_score, 2),\n",
    "        \"final_recommendations\": all_recommendations,\n",
    "        \"score_differences\": score_differences,\n",
    "        \"avg_difference\": round(avg_difference, 2),\n",
    "        \"evaluation_notes\": evaluation_notes,\n",
    "        \"api_fallbacks_used\": gpt_fallback or claude_fallback\n",
    "    }\n",
    "\n",
    "async def universal_json_evaluator(filepath: str, query: str) -> UniversalEvaluation:\n",
    "    \"\"\"Universal evaluator that can process any JSON research file\"\"\"\n",
    "    print(f\"📊 Loading and evaluating: {filepath}\")\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        # Extract content using universal extractor\n",
    "        content = extract_content_from_json(json_data, query)\n",
    "        print(f\"🔍 Detected content type: {content['content_type']}\")\n",
    "        \n",
    "        # Dual evaluation\n",
    "        print(\"🤖 Running GPT-4 evaluation...\")\n",
    "        gpt_eval = await evaluate_with_gpt(content)\n",
    "        \n",
    "        print(\"🧠 Running Claude evaluation...\")\n",
    "        claude_eval = await evaluate_with_claude(content)\n",
    "        \n",
    "        # Build consensus\n",
    "        print(\"🔄 Building evaluation consensus...\")\n",
    "        consensus = calculate_consensus(gpt_eval, claude_eval)\n",
    "        \n",
    "        # Create summary\n",
    "        summary = f\"\"\"\n",
    "# Universal Research Evaluation Report\n",
    "\n",
    "**Query**: {query}  \n",
    "**Content Type**: {content['content_type'].title()}  \n",
    "**File**: {filepath}\n",
    "\n",
    "## Evaluation Scores\n",
    "- **GPT-4 Overall**: {gpt_eval.overall_score}/10\n",
    "- **Claude Overall**: {claude_eval.overall_score}/10  \n",
    "- **Consensus Score**: {consensus['consensus_score']}/10\n",
    "- **Agreement Level**: {5 - len([d for d in consensus['score_differences'].values() if d > 2])}/5 criteria\n",
    "\n",
    "## Content Analysis\n",
    "{json.dumps(content['structured_data'], indent=2)}\n",
    "\n",
    "## Quality Assessment\n",
    "{'✅ **HIGH QUALITY** - Exceeds standards' if consensus['consensus_score'] >= 7.5 else \n",
    " '⚠️ **MODERATE QUALITY** - Meets basic standards' if consensus['consensus_score'] >= 6.0 else\n",
    " '❌ **NEEDS IMPROVEMENT** - Below standards'}\n",
    "\"\"\"\n",
    "        \n",
    "        return UniversalEvaluation(\n",
    "            query=query,\n",
    "            content_type=content['content_type'],\n",
    "            gpt_evaluation=gpt_eval,\n",
    "            claude_evaluation=claude_eval,\n",
    "            consensus_score=consensus['consensus_score'],\n",
    "            final_recommendations=consensus['final_recommendations'],\n",
    "            evaluation_summary=summary\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Evaluation failed: {str(e)}\")\n",
    "\n",
    "print(\"✅ Universal JSON evaluator loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# API ERROR HANDLING UTILITIES\n",
    "# ============================================\n",
    "\n",
    "def get_api_status_help():\n",
    "    \"\"\"Get help text for API issues\"\"\"\n",
    "    return \"\"\"\n",
    "## 🔧 API Troubleshooting Guide\n",
    "\n",
    "### Claude API Overload (Error 529)\n",
    "**What it means**: Claude's servers are temporarily overloaded\n",
    "**Solutions**:\n",
    "1. ⏳ **Wait and retry** - Usually resolves in 1-5 minutes\n",
    "2. 🔄 **Use fallback evaluation** - System automatically provides heuristic scoring\n",
    "3. 📊 **Focus on GPT-4 scores** - Still get partial evaluation\n",
    "4. ⚙️ **Reduce request frequency** - Space out evaluations\n",
    "\n",
    "### GPT-4 API Issues\n",
    "**Common causes**: Rate limits, server issues, API key problems\n",
    "**Solutions**:\n",
    "1. 🔑 **Check API key** - Ensure valid OpenAI API key\n",
    "2. 💳 **Check billing** - Ensure account has credits\n",
    "3. ⏱️ **Rate limiting** - Reduce request frequency\n",
    "4. 🔄 **Use fallback** - System provides alternative scoring\n",
    "\n",
    "### When Fallback Evaluations Are Used\n",
    "**Fallback scoring uses**:\n",
    "- Content length analysis\n",
    "- Structural assessment  \n",
    "- Heuristic quality metrics\n",
    "- Conservative confidence levels\n",
    "\n",
    "**Limitations**:\n",
    "- Less nuanced than AI evaluation\n",
    "- Cannot verify factual accuracy\n",
    "- Limited domain expertise\n",
    "- Medium confidence ratings\n",
    "\n",
    "### 💡 Best Practices\n",
    "1. **Monitor API status** - Check provider status pages\n",
    "2. **Use off-peak hours** - Better availability\n",
    "3. **Implement delays** - Space out API calls\n",
    "4. **Have fallbacks** - System handles gracefully\n",
    "5. **Re-evaluate later** - Try again when APIs recover\n",
    "\n",
    "### 🔄 Manual Re-evaluation\n",
    "```python\n",
    "# Re-run evaluation when APIs are available\n",
    "evaluation = await universal_json_evaluator(\"your_file.json\", \"your_query\")\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "async def test_api_availability():\n",
    "    \"\"\"Test if APIs are currently available\"\"\"\n",
    "    api_status = {\n",
    "        \"gpt4_available\": False,\n",
    "        \"claude_available\": False,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Test GPT-4\n",
    "    try:\n",
    "        test_response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # Use cheaper model for testing\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Test\"}],\n",
    "            max_tokens=1,\n",
    "            temperature=0\n",
    "        )\n",
    "        api_status[\"gpt4_available\"] = True\n",
    "        print(\"✅ GPT-4 API: Available\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ GPT-4 API: Unavailable - {str(e)[:50]}...\")\n",
    "    \n",
    "    # Test Claude\n",
    "    try:\n",
    "        test_response = claude_client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=1,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "        )\n",
    "        api_status[\"claude_available\"] = True\n",
    "        print(\"✅ Claude API: Available\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Claude API: Unavailable - {str(e)[:50]}...\")\n",
    "    \n",
    "    return api_status\n",
    "\n",
    "print(\"✅ API error handling utilities loaded!\")\n",
    "print(\"💡 Use test_api_availability() to check current API status\")\n",
    "print(\"💡 Fallback evaluations activate automatically when needed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274e13a",
   "metadata": {},
   "source": [
    "## CELL 12: EVALUATION ORCHESTRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def finalize_research_evaluation(query: str, report_content: str) -> UniversalEvaluation:\n",
    "    \"\"\"Finalize research with dual-model evaluation\"\"\"\n",
    "    \n",
    "    # Create content structure for evaluation\n",
    "    content = {\n",
    "        \"query\": query,\n",
    "        \"content_type\": \"agentic\",\n",
    "        \"text_content\": report_content,\n",
    "        \"structured_data\": {\n",
    "            \"report_length\": len(report_content),\n",
    "            \"generated_at\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"source_info\": {}\n",
    "    }\n",
    "    \n",
    "    # Dual evaluation\n",
    "    print(\"🤖 Running GPT-4 evaluation...\")\n",
    "    gpt_eval = await evaluate_with_gpt(content)\n",
    "    \n",
    "    print(\"🧠 Running Claude evaluation...\")\n",
    "    claude_eval = await evaluate_with_claude(content)\n",
    "    \n",
    "    # Build consensus\n",
    "    print(\"🔄 Building evaluation consensus...\")\n",
    "    consensus = calculate_consensus(gpt_eval, claude_eval)\n",
    "    \n",
    "    # Create summary\n",
    "    summary = f\"\"\"\n",
    "# Research Evaluation Report\n",
    "\n",
    "**Query**: {query}  \n",
    "**Content Type**: Agentic Research Report\n",
    "**Report Length**: {len(report_content)} characters\n",
    "\n",
    "## Evaluation Scores\n",
    "- **GPT-4 Overall**: {gpt_eval.overall_score}/10\n",
    "- **Claude Overall**: {claude_eval.overall_score}/10  \n",
    "- **Consensus Score**: {consensus['consensus_score']}/10\n",
    "\n",
    "## Quality Assessment\n",
    "{'✅ **HIGH QUALITY** - Exceeds standards' if consensus['consensus_score'] >= 7.5 else \n",
    " '⚠️ **MODERATE QUALITY** - Meets basic standards' if consensus['consensus_score'] >= 6.0 else\n",
    " '❌ **NEEDS IMPROVEMENT** - Below standards'}\n",
    "\"\"\"\n",
    "    \n",
    "    return UniversalEvaluation(\n",
    "        query=query,\n",
    "        content_type=\"agentic\",\n",
    "        gpt_evaluation=gpt_eval,\n",
    "        claude_evaluation=claude_eval,\n",
    "        consensus_score=consensus['consensus_score'],\n",
    "        final_recommendations=consensus['final_recommendations'],\n",
    "        evaluation_summary=summary\n",
    "    )\n",
    "\n",
    "def create_evaluation_report(evaluation: UniversalEvaluation) -> str:\n",
    "    \"\"\"Create formatted evaluation report with API status information\"\"\"\n",
    "    \n",
    "    gpt_scores = evaluation.gpt_evaluation.criteria_scores\n",
    "    claude_scores = evaluation.claude_evaluation.criteria_scores\n",
    "    \n",
    "    # Check for API fallbacks\n",
    "    gpt_fallback = evaluation.gpt_evaluation.confidence_level == \"Medium\" and \"Could not verify with GPT-4 API\" in evaluation.gpt_evaluation.weaknesses\n",
    "    claude_fallback = evaluation.claude_evaluation.confidence_level == \"Medium\" and \"Could not verify accuracy with Claude API\" in evaluation.claude_evaluation.weaknesses\n",
    "    \n",
    "    api_status = \"\"\n",
    "    if gpt_fallback or claude_fallback:\n",
    "        api_status = \"\\n## ⚠️ API Status\\n\"\n",
    "        if gpt_fallback:\n",
    "            api_status += \"- **GPT-4**: Used fallback evaluation (API unavailable)\\n\"\n",
    "        else:\n",
    "            api_status += \"- **GPT-4**: ✅ API evaluation successful\\n\"\n",
    "        if claude_fallback:\n",
    "            api_status += \"- **Claude**: Used fallback evaluation (API overloaded)\\n\"\n",
    "        else:\n",
    "            api_status += \"- **Claude**: ✅ API evaluation successful\\n\"\n",
    "        api_status += \"\\n*Note: Fallback evaluations use heuristic scoring and may be less accurate.*\\n\"\n",
    "    \n",
    "    report = f\"\"\"\n",
    "# 📊 Research Quality Evaluation\n",
    "\n",
    "## 🎯 Query Analysis\n",
    "**Original Query**: {evaluation.query}\n",
    "**Content Type**: {evaluation.content_type.title()}\n",
    "{api_status}\n",
    "## 📈 Evaluation Scores\n",
    "\n",
    "### GPT-4 Assessment {'(Fallback)' if gpt_fallback else ''}\n",
    "- **Overall Score**: {evaluation.gpt_evaluation.overall_score}/10\n",
    "- **Accuracy**: {gpt_scores.accuracy_score}/10\n",
    "- **Completeness**: {gpt_scores.completeness_score}/10  \n",
    "- **Relevance**: {gpt_scores.relevance_score}/10\n",
    "- **Clarity**: {gpt_scores.clarity_score}/10\n",
    "- **Depth**: {gpt_scores.depth_score}/10\n",
    "\n",
    "### Claude Assessment {'(Fallback)' if claude_fallback else ''}\n",
    "- **Overall Score**: {evaluation.claude_evaluation.overall_score}/10\n",
    "- **Accuracy**: {claude_scores.accuracy_score}/10\n",
    "- **Completeness**: {claude_scores.completeness_score}/10\n",
    "- **Relevance**: {claude_scores.relevance_score}/10  \n",
    "- **Clarity**: {claude_scores.clarity_score}/10\n",
    "- **Depth**: {claude_scores.depth_score}/10\n",
    "\n",
    "### 🤝 Consensus Analysis\n",
    "- **Consensus Score**: {evaluation.consensus_score}/10\n",
    "- **Quality Rating**: {'✅ HIGH QUALITY' if evaluation.consensus_score >= 7.5 else '⚠️ MODERATE QUALITY' if evaluation.consensus_score >= 6.0 else '❌ NEEDS IMPROVEMENT'}\n",
    "{'- **Note**: Consensus may be affected by API fallbacks' if gpt_fallback or claude_fallback else ''}\n",
    "\n",
    "## 💪 Strengths\n",
    "{chr(10).join(f\"- {strength}\" for strength in evaluation.gpt_evaluation.strengths[:3])}\n",
    "\n",
    "## 🔧 Areas for Improvement  \n",
    "{chr(10).join(f\"- {weakness}\" for weakness in evaluation.gpt_evaluation.weaknesses[:3])}\n",
    "\n",
    "## 🎯 Recommendations\n",
    "{chr(10).join(f\"- {rec}\" for rec in evaluation.final_recommendations[:5])}\n",
    "\n",
    "---\n",
    "*Evaluation completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "{'*Some evaluations used fallback methods due to API limitations*' if gpt_fallback or claude_fallback else ''}\n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "print(\"✅ Evaluation orchestration functions loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a83f1d",
   "metadata": {},
   "source": [
    "## CELL 13: COMBINED SEARCH ORCHESTRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def perform_combined_search(query: str) -> CombinedSearchResults:\n",
    "    \"\"\"\n",
    "    Perform both explorer and agentic search, then combine results\n",
    "    \"\"\"\n",
    "    print(f\"🚀 Starting Combined Search for: {query}\")\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    \n",
    "    # Run both searches in parallel\n",
    "    print(\"⚡ Running explorer and agentic searches in parallel...\")\n",
    "    \n",
    "    # Start both searches\n",
    "    explorer_task = asyncio.create_task(\n",
    "        asyncio.to_thread(lambda: perform_explorer_search(query, save_results=False))\n",
    "    )\n",
    "    agentic_task = asyncio.create_task(\n",
    "        perform_agentic_search(query)\n",
    "    )\n",
    "    \n",
    "    # Wait for both to complete\n",
    "    (explorer_results, explorer_metadata), (agentic_report, search_plan, search_summaries) = await asyncio.gather(\n",
    "        explorer_task, agentic_task\n",
    "    )\n",
    "    \n",
    "    # Create combined summary\n",
    "    explorer_sources = len([item for item in explorer_results if \"url\" in item])\n",
    "    agentic_searches = len(search_plan.searches) if search_plan else 0\n",
    "    \n",
    "    combined_summary = f\"\"\"\n",
    "# Combined Search Results for: {query}\n",
    "\n",
    "## Search Overview\n",
    "- **Timestamp**: {timestamp}\n",
    "- **Explorer Sources**: {explorer_sources} web pages scraped\n",
    "- **Agentic Searches**: {agentic_searches} strategic searches performed\n",
    "\n",
    "## Explorer Summary\n",
    "Scraped {explorer_sources} web pages from DuckDuckGo and Brave Search engines.\n",
    "\n",
    "## Agentic Summary\n",
    "{agentic_report.short_summary}\n",
    "\n",
    "## Integration\n",
    "This combined approach provides both raw web data (explorer) and synthesized insights (agentic) for comprehensive research coverage.\n",
    "\"\"\"\n",
    "    \n",
    "    # Save combined results\n",
    "    timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    combined_results = {\n",
    "        \"query\": query,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"explorer_results\": explorer_results,\n",
    "        \"explorer_metadata\": explorer_metadata,\n",
    "        \"agentic_results\": agentic_report.dict(),\n",
    "        \"combined_summary\": combined_summary\n",
    "    }\n",
    "    \n",
    "    filename = f\"combined_search_{timestamp_str}.json\"\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    with open(filepath, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(combined_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"💾 Combined search results saved to: {filepath}\")\n",
    "    \n",
    "    return CombinedSearchResults(\n",
    "        query=query,\n",
    "        timestamp=timestamp,\n",
    "        explorer_results=explorer_results,\n",
    "        agentic_results=agentic_report,\n",
    "        combined_summary=combined_summary\n",
    "    )\n",
    "\n",
    "print(\"✅ Combined search orchestration loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf48fe",
   "metadata": {},
   "source": [
    "## CELL 14: JUPYTER EXECUTION INTERFACE and CONFIGURATION GUIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35160a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def deep_search_with_evaluation_jupyter(query: str):\n",
    "    \"\"\"Jupyter-compatible version of deep search with evaluation\"\"\"\n",
    "    \n",
    "    with trace(\"Deep Search with Evaluation\"):\n",
    "        # Phase 1: Deep Search\n",
    "        print(\"=\" * 50)\n",
    "        print(\"🔍 PHASE 1: DEEP SEARCH\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        print(\"\\n📋 Planning searches...\")\n",
    "        search_plan = await plan_searches(query)\n",
    "        \n",
    "        print(\"\\n🌐 Performing web searches...\")\n",
    "        search_results = await perform_searches(search_plan)\n",
    "        \n",
    "        print(\"\\n📝 Writing comprehensive report...\")\n",
    "        report = await write_report(query, search_results)\n",
    "        \n",
    "        # Display initial report\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"📄 INITIAL RESEARCH REPORT\")\n",
    "        print(\"=\" * 50)\n",
    "        display(Markdown(report.markdown_report))\n",
    "        \n",
    "        # Phase 2: Evaluation\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"🔬 PHASE 2: DUAL-MODEL EVALUATION\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Perform evaluation\n",
    "        final_eval = await finalize_research_evaluation(query, report.markdown_report)\n",
    "        \n",
    "        # Create and display evaluation report\n",
    "        evaluation_report = create_evaluation_report(final_eval)\n",
    "        display(Markdown(evaluation_report))\n",
    "        \n",
    "        # Save evaluation results\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"evaluation_{timestamp}.json\"\n",
    "        \n",
    "        evaluation_results = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query\": query,\n",
    "            \"gpt_overall_score\": final_eval.gpt_evaluation.overall_score,\n",
    "            \"claude_overall_score\": final_eval.claude_evaluation.overall_score,\n",
    "            \"consensus_score\": final_eval.consensus_score,\n",
    "            \"final_recommendations\": final_eval.final_recommendations,\n",
    "            \"report_summary\": report.short_summary,\n",
    "            \"follow_up_questions\": report.follow_up_questions,\n",
    "            \"full_report\": report.markdown_report\n",
    "        }\n",
    "        \n",
    "        # Save to DATA_DIR\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(evaluation_results, f, indent=2)\n",
    "        print(f\"\\n💾 Evaluation saved to: {filepath}\")\n",
    "        \n",
    "        # Phase 3: Quality Gate\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"🔧 PHASE 3: QUALITY CHECK\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        quality_threshold = 7.5\n",
    "        meets_threshold = final_eval.consensus_score >= quality_threshold\n",
    "        \n",
    "        print(f\"\\n✅ Research Quality: {'APPROVED' if meets_threshold else 'NEEDS IMPROVEMENT'}\")\n",
    "        print(f\"Consensus Score: {final_eval.consensus_score}/10\")\n",
    "        print(f\"Quality Threshold: {quality_threshold}/10\")\n",
    "        \n",
    "        if not meets_threshold:\n",
    "            print(\"\\n⚠️ Report needs improvement based on evaluation.\")\n",
    "            print(\"\\nTop 3 recommendations for improvement:\")\n",
    "            for i, rec in enumerate(final_eval.final_recommendations[:3], 1):\n",
    "                print(f\"  {i}. {rec}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"✨ DEEP SEARCH WITH EVALUATION COMPLETE\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        return {\n",
    "            \"report\": report,\n",
    "            \"evaluation\": final_eval,\n",
    "            \"quality_approved\": meets_threshold,\n",
    "            \"filepath\": filepath\n",
    "        }\n",
    "\n",
    "# Main execution function\n",
    "async def run_research(query: str):\n",
    "    \"\"\"Run research for a single query\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'#' * 60}\")\n",
    "        print(f\"# RESEARCHING: {query}\")\n",
    "        print(f\"{'#' * 60}\\n\")\n",
    "        \n",
    "        results = await deep_search_with_evaluation_jupyter(query)\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\n📊 Summary for '{query}':\")\n",
    "        print(f\"   - Report length: {len(results['report'].markdown_report)} characters\")\n",
    "        print(f\"   - GPT-4 Score: {results['evaluation'].gpt_evaluation.overall_score}/10\")\n",
    "        print(f\"   - Claude Score: {results['evaluation'].claude_evaluation.overall_score}/10\")\n",
    "        print(f\"   - Consensus: {results['evaluation'].consensus_score}/10\")\n",
    "        print(f\"   - Quality: {'✅ Approved' if results['quality_approved'] else '❌ Needs Improvement'}\")\n",
    "        print(f\"   - Saved to: {results['filepath']}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing query '{query}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Alternative: Run combined search (explorer + agentic)\n",
    "async def run_combined_research(query: str):\n",
    "    \"\"\"Run combined explorer + agentic search\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'#' * 60}\")\n",
    "        print(f\"# COMBINED RESEARCH: {query}\")\n",
    "        print(f\"{'#' * 60}\\n\")\n",
    "        \n",
    "        results = await perform_combined_search(query)\n",
    "        \n",
    "        # Display results\n",
    "        display(Markdown(results.combined_summary))\n",
    "        display(Markdown(results.agentic_results.markdown_report))\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"combined_search_{timestamp}.json\"\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(results.dict(), f, indent=2)\n",
    "        \n",
    "        print(f\"\\n💾 Combined results saved to: {filepath}\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in combined research '{query}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ============================================\n",
    "# EXPLORER-ONLY EXECUTION FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "async def run_explorer_only_research(query: str):\n",
    "    \"\"\"Run explorer-only research with detailed JSON saving\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'#' * 60}\")\n",
    "        print(f\"# EXPLORER-ONLY RESEARCH: {query}\")\n",
    "        print(f\"{'#' * 60}\\n\")\n",
    "        \n",
    "        # Perform explorer search\n",
    "        explorer_results, explorer_metadata = perform_explorer_search(query, save_results=True)\n",
    "        \n",
    "        # Display results summary\n",
    "        print(f\"\\n🔍 Explorer Research Summary:\")\n",
    "        print(f\"   - Total links found: {explorer_metadata['metadata']['total_links_found']}\")\n",
    "        print(f\"   - Pages scraped: {len(explorer_results)}\")\n",
    "        print(f\"   - Successful scrapes: {len([r for r in explorer_results if not r.get('error')])}\")\n",
    "        print(f\"   - Total text extracted: {sum(r.get('full_text_length', 0) for r in explorer_results if not r.get('error')):,} chars\")\n",
    "        \n",
    "        # Create markdown report for display\n",
    "        report_md = f\"\"\"\n",
    "# 🔍 Explorer Research Results\n",
    "\n",
    "## Query: {query}\n",
    "\n",
    "### 📊 Summary Statistics\n",
    "- **Total Links Found**: {explorer_metadata['metadata']['total_links_found']}\n",
    "- **Pages Successfully Scraped**: {len([r for r in explorer_results if not r.get('error')])}\n",
    "- **Total Text Extracted**: {sum(r.get('full_text_length', 0) for r in explorer_results if not r.get('error')):,} characters\n",
    "- **Search Engines Used**: {', '.join([e['name'] for e in explorer_metadata['metadata']['search_engines']])}\n",
    "\n",
    "### 🌐 Source Breakdown\n",
    "\"\"\"\n",
    "        \n",
    "        for engine in explorer_metadata['metadata']['search_engines']:\n",
    "            report_md += f\"\"\"\n",
    "#### {engine['name']}\n",
    "- Links Found: {engine['links_found']}\n",
    "- Successfully Scraped: {engine['successful_scrapes']}/{engine['links_scraped']}\n",
    "- Success Rate: {(engine['successful_scrapes']/engine['links_scraped']*100) if engine['links_scraped'] > 0 else 0:.1f}%\n",
    "\"\"\"\n",
    "        \n",
    "        report_md += \"\\n### 📄 Content Samples\\n\"\n",
    "        \n",
    "        successful_results = [r for r in explorer_results if not r.get('error')][:3]\n",
    "        for i, result in enumerate(successful_results, 1):\n",
    "            report_md += f\"\"\"\n",
    "#### Sample {i}: {result.get('source_engine', 'Unknown')}\n",
    "**URL**: {result.get('url', 'N/A')}  \n",
    "**Length**: {result.get('full_text_length', 0):,} characters  \n",
    "**Preview**: {result.get('text', '')[:200]}...\n",
    "\"\"\"\n",
    "        \n",
    "        display(Markdown(report_md))\n",
    "        \n",
    "        return {\n",
    "            \"type\": \"explorer_only\",\n",
    "            \"query\": query,\n",
    "            \"results\": explorer_results,\n",
    "            \"metadata\": explorer_metadata,\n",
    "            \"summary\": explorer_metadata['summary']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in explorer research '{query}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_explorer_results(filepath: str):\n",
    "    \"\"\"Load and display explorer search results from JSON file\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Check if it's an explorer results file\n",
    "        if \"metadata\" in data and \"scraped_data\" in data:\n",
    "            return data\n",
    "        elif \"explorer_results\" in data and \"explorer_metadata\" in data:\n",
    "            # Combined search file\n",
    "            return {\n",
    "                \"metadata\": data[\"explorer_metadata\"][\"metadata\"],\n",
    "                \"scraped_data\": data[\"explorer_results\"],\n",
    "                \"summary\": data[\"explorer_metadata\"][\"summary\"]\n",
    "            }\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading explorer results: {e}\")\n",
    "        return None\n",
    "\n",
    "def display_explorer_json_analysis(filepath: str, query: str = None):\n",
    "    \"\"\"Analyze and display explorer JSON results\"\"\"\n",
    "    data = load_explorer_results(filepath)\n",
    "    if not data:\n",
    "        return \"❌ File is not a valid explorer results file\"\n",
    "    \n",
    "    # Extract information\n",
    "    metadata = data.get(\"metadata\", {})\n",
    "    scraped_data = data.get(\"scraped_data\", [])\n",
    "    summary = data.get(\"summary\", {})\n",
    "    \n",
    "    # Detect query if not provided\n",
    "    if not query:\n",
    "        query = metadata.get(\"query\", summary.get(\"query\", \"Unknown\"))\n",
    "    \n",
    "    successful_scrapes = [item for item in scraped_data if not item.get('error')]\n",
    "    failed_scrapes = [item for item in scraped_data if item.get('error')]\n",
    "    \n",
    "    analysis = f\"\"\"\n",
    "# 📊 Explorer Results Analysis\n",
    "\n",
    "## 🔍 Query Information\n",
    "**Original Query**: {query}  \n",
    "**Timestamp**: {metadata.get('timestamp', summary.get('completed_at', 'Unknown'))}  \n",
    "**File**: {filepath}\n",
    "\n",
    "## 📈 Performance Metrics\n",
    "- **Total Links Found**: {metadata.get('total_links_found', 'Unknown')}\n",
    "- **Pages Scraped**: {len(scraped_data)}\n",
    "- **Successful Scrapes**: {len(successful_scrapes)}\n",
    "- **Failed Scrapes**: {len(failed_scrapes)}\n",
    "- **Success Rate**: {(len(successful_scrapes)/len(scraped_data)*100) if scraped_data else 0:.1f}%\n",
    "\n",
    "## 🌐 Search Engine Performance\n",
    "\"\"\"\n",
    "    \n",
    "    for engine in metadata.get('search_engines', []):\n",
    "        success_rate = (engine['successful_scrapes'] / engine['links_scraped'] * 100) if engine['links_scraped'] > 0 else 0\n",
    "        analysis += f\"\"\"\n",
    "### {engine['name']}\n",
    "- **Links Found**: {engine['links_found']}\n",
    "- **Links Scraped**: {engine['links_scraped']}\n",
    "- **Successful Scrapes**: {engine['successful_scrapes']}\n",
    "- **Success Rate**: {success_rate:.1f}%\n",
    "\"\"\"\n",
    "    \n",
    "    # Content analysis\n",
    "    if successful_scrapes:\n",
    "        total_chars = sum(item.get('full_text_length', 0) for item in successful_scrapes)\n",
    "        avg_chars = total_chars / len(successful_scrapes)\n",
    "        avg_time = sum(item.get('processing_time_seconds', 0) for item in successful_scrapes) / len(successful_scrapes)\n",
    "        \n",
    "        analysis += f\"\"\"\n",
    "## 📄 Content Analysis\n",
    "- **Total Text Extracted**: {total_chars:,} characters\n",
    "- **Average Text per Page**: {avg_chars:,.0f} characters\n",
    "- **Average Processing Time**: {avg_time:.2f} seconds\n",
    "- **Longest Page**: {max(item.get('full_text_length', 0) for item in successful_scrapes):,} characters\n",
    "- **Shortest Page**: {min(item.get('full_text_length', 0) for item in successful_scrapes):,} characters\n",
    "\n",
    "## 🔗 Sample URLs (First 5)\n",
    "\"\"\"\n",
    "        for i, item in enumerate(successful_scrapes[:5], 1):\n",
    "            analysis += f\"{i}. {item.get('url', 'Unknown')} ({item.get('full_text_length', 0):,} chars)\\n\"\n",
    "    \n",
    "    if failed_scrapes:\n",
    "        analysis += f\"\"\"\n",
    "## ❌ Failed Scrapes ({len(failed_scrapes)})\n",
    "\"\"\"\n",
    "        for i, item in enumerate(failed_scrapes[:3], 1):\n",
    "            analysis += f\"{i}. {item.get('url', 'Unknown')} - {item.get('error_details', item.get('text', 'Unknown error'))}\\n\"\n",
    "    \n",
    "    analysis += f\"\"\"\n",
    "## 🔧 Configuration Used\n",
    "- **Max Search Results**: {metadata.get('search_config', {}).get('max_search_results', 'Unknown')}\n",
    "- **Max URLs to Scrape**: {metadata.get('search_config', {}).get('max_urls_to_scrape', 'Unknown')}\n",
    "- **Max Text Length**: {metadata.get('search_config', {}).get('max_text_length', 'Unknown')}\n",
    "- **Scraping Timeout**: {metadata.get('search_config', {}).get('scraping_timeout', 'Unknown')} seconds\n",
    "\n",
    "---\n",
    "*Analysis generated at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "async def run_deep_chatgpt_research(query: str):\n",
    "    \"\"\"Run deep ChatGPT research with iterative refinement\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'#' * 60}\")\n",
    "        print(f\"# DEEP CHATGPT RESEARCH: {query}\")\n",
    "        print(f\"{'#' * 60}\\n\")\n",
    "        \n",
    "        results = await perform_deep_chatgpt_search(query)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"🧠 Deep ChatGPT Research Summary:\")\n",
    "        print(f\"   - Iterations: {results['iterations']}\")\n",
    "        print(f\"   - Total search phases: {len(results['search_history'])}\")\n",
    "        \n",
    "        display(Markdown(results['final_report'].markdown_report))\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"deep_chatgpt_{timestamp}.json\"\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\n💾 Deep ChatGPT results saved to: {filepath}\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in deep ChatGPT research '{query}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "async def run_deep_claude_research(query: str):\n",
    "    \"\"\"Run deep Claude-style research with systematic analysis\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'#' * 60}\")\n",
    "        print(f\"# DEEP CLAUDE-STYLE RESEARCH: {query}\")\n",
    "        print(f\"{'#' * 60}\\n\")\n",
    "        \n",
    "        results = await perform_deep_claude_search(query)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"🎭 Deep Claude-Style Research Summary:\")\n",
    "        print(f\"   - Analysis phases: {results['phases']}\")\n",
    "        print(f\"   - Systematic approach: ✅\")\n",
    "        \n",
    "        display(Markdown(results['final_report'].markdown_report))\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"deep_claude_style_{timestamp}.json\"\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\n💾 Deep Claude-style results saved to: {filepath}\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in deep Claude-style research '{query}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "async def run_comparative_deep_research(query: str):\n",
    "    \"\"\"Run comparative deep research (ChatGPT vs Claude-style)\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'#' * 60}\")\n",
    "        print(f\"# COMPARATIVE DEEP RESEARCH: {query}\")\n",
    "        print(f\"{'#' * 60}\\n\")\n",
    "        \n",
    "        results = await perform_comparative_deep_search(query)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"⚖️ Comparative Deep Research Summary:\")\n",
    "        print(f\"   - Total iterations: {results['total_iterations']}\")\n",
    "        print(f\"   - ChatGPT approach: ✅\")\n",
    "        print(f\"   - Claude-style approach: ✅\")\n",
    "        print(f\"   - Comparative analysis: ✅\")\n",
    "        \n",
    "        display(Markdown(\"## ChatGPT Research Results\"))\n",
    "        display(Markdown(results['chatgpt_results']['final_report'].markdown_report))\n",
    "        \n",
    "        display(Markdown(\"## Claude-Style Research Results\"))\n",
    "        display(Markdown(results['claude_results']['final_report'].markdown_report))\n",
    "        \n",
    "        display(Markdown(\"## Comparative Analysis\"))\n",
    "        display(Markdown(results['comparative_analysis'].markdown_report))\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"comparative_deep_{timestamp}.json\"\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\n💾 Comparative deep research results saved to: {filepath}\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in comparative deep research '{query}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ Jupyter execution interface ready!\")\n",
    "print(\"💡 Available functions:\")\n",
    "print(\"   - await run_research('your query')           # Agentic search with evaluation\")\n",
    "print(\"   - await run_combined_research('your query')  # Explorer + Agentic combined\")\n",
    "print(\"   - await run_explorer_only_research('query')  # Explorer-only with detailed JSON\")\n",
    "print(\"   - await universal_json_evaluator(filepath, query)  # Evaluate existing results\")\n",
    "print(\"🔥 Deep search functions (NOW ENABLED!):\")\n",
    "print(\"   - await run_deep_chatgpt_research('query')    # Deep ChatGPT iterative research\")\n",
    "print(\"   - await run_deep_claude_research('query')     # Deep Claude-style systematic research\") \n",
    "print(\"   - await run_comparative_deep_research('query') # Comparative ChatGPT vs Claude research\")\n",
    "print(f\"\\n🔧 Current configuration:\")\n",
    "print(f\"   - Max links to extract: {MAX_LINKS_TO_EXTRACT}\")\n",
    "print(f\"   - Max URLs to scrape: {MAX_URLS_TO_SCRAPE}\")\n",
    "print(f\"   - Max search results: {MAX_SEARCH_RESULTS}\")\n",
    "print(f\"   - Strategic searches: {MAX_STRATEGIC_SEARCHES}\")\n",
    "print(f\"   - Deep search iterations: {DEEP_SEARCH_ITERATIONS}\")\n",
    "print(f\"   - Search context size: {SEARCH_CONTEXT_SIZE}\")\n",
    "print(\"\\n📄 Explorer features:\")\n",
    "print(\"   - Detailed link tracking and progress display\")\n",
    "print(\"   - Comprehensive JSON saving with metadata\")\n",
    "print(\"   - Source-by-source analysis and statistics\")\n",
    "print(\"   - Performance metrics and error tracking\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📝 CONFIGURATION CUSTOMIZATION GUIDE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "🎛️ **How to customize the search system:**\n",
    "\n",
    "1. **Modify Search Scope:**\n",
    "   MAX_LINKS_TO_EXTRACT = 30        # Extract more links from search results\n",
    "   MAX_URLS_TO_SCRAPE = 8           # Scrape more web pages  \n",
    "   MAX_SEARCH_RESULTS = 15          # Get more search results per engine\n",
    "\n",
    "2. **Adjust Agentic Search:**\n",
    "   MAX_STRATEGIC_SEARCHES = 5       # Plan more strategic searches\n",
    "   SEARCH_CONTEXT_SIZE = \"large\"    # Use larger context for deeper analysis\n",
    "   REPORT_MIN_LENGTH = 1500         # Require longer, more detailed reports\n",
    "\n",
    "3. **Configure Deep Search:**\n",
    "   DEEP_SEARCH_ITERATIONS = 7       # More iterative refinement cycles\n",
    "   DEEP_SEARCH_REFINEMENT = True    # Enable progressive refinement\n",
    "\n",
    "4. **Performance Tuning:**\n",
    "   SCRAPING_TIMEOUT = 20            # Longer timeout for slow websites\n",
    "   MAX_TEXT_LENGTH = 5000           # Capture more text per page\n",
    "\n",
    "**Example: High-Intensity Research Setup**\n",
    "```python\n",
    "# Uncomment and modify these in Cell 2:\n",
    "# MAX_LINKS_TO_EXTRACT = 50\n",
    "# MAX_URLS_TO_SCRAPE = 10  \n",
    "# MAX_STRATEGIC_SEARCHES = 7\n",
    "# DEEP_SEARCH_ITERATIONS = 10\n",
    "# SEARCH_CONTEXT_SIZE = \"large\"\n",
    "# REPORT_MIN_LENGTH = 2000\n",
    "```\n",
    "\n",
    "**Example: Fast & Light Setup**\n",
    "```python\n",
    "# Uncomment and modify these in Cell 2:\n",
    "# MAX_LINKS_TO_EXTRACT = 10\n",
    "# MAX_URLS_TO_SCRAPE = 3\n",
    "# MAX_STRATEGIC_SEARCHES = 2  \n",
    "# DEEP_SEARCH_ITERATIONS = 3\n",
    "# SEARCH_CONTEXT_SIZE = \"small\"\n",
    "# REPORT_MIN_LENGTH = 500\n",
    "```\n",
    "\n",
    "🔥 **Deep search capabilities are NOW ENABLED!**\n",
    "✅ Deep search agents activated\n",
    "✅ Deep search execution functions ready\n",
    "✅ Deep search interface functions available\n",
    "✅ Gradio Deep Search tab enabled\n",
    "\n",
    "💡 **Pro Tips:**\n",
    "- Increase MAX_STRATEGIC_SEARCHES for complex topics\n",
    "- Use \"large\" SEARCH_CONTEXT_SIZE for technical subjects\n",
    "- Enable DEEP_SEARCH_REFINEMENT for controversial topics\n",
    "- Adjust SCRAPING_TIMEOUT if you encounter many timeouts\n",
    "- Use Deep Search for the most comprehensive analysis\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Configuration guide complete!\")\n",
    "print(\"🔄 Restart the kernel and rerun all cells after making configuration changes\")\n",
    "print(\"🔥 Deep Search is now FULLY OPERATIONAL!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f8d79",
   "metadata": {},
   "source": [
    "## CELL 16: GRADIO USER INTERFACE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e27d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gradio_interface():\n",
    "    \"\"\"Create comprehensive Gradio interface for the search system\"\"\"\n",
    "    \n",
    "    # Custom CSS for better styling\n",
    "    custom_css = \"\"\"\n",
    "    .gradio-container {\n",
    "        max-width: 1200px !important;\n",
    "    }\n",
    "    .search-header {\n",
    "        text-align: center;\n",
    "        color: #2563eb;\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    .config-section {\n",
    "        background: #f8fafc;\n",
    "        padding: 15px;\n",
    "        border-radius: 8px;\n",
    "        margin: 10px 0;\n",
    "    }\n",
    "    .status-box {\n",
    "        padding: 10px;\n",
    "        border-radius: 5px;\n",
    "        margin: 5px 0;\n",
    "    }\n",
    "    .success { background: #dcfce7; border-left: 4px solid #16a34a; }\n",
    "    .error { background: #fef2f2; border-left: 4px solid #dc2626; }\n",
    "    .info { background: #dbeafe; border-left: 4px solid #2563eb; }\n",
    "    \"\"\"\n",
    "    \n",
    "    async def run_search_interface(query, search_type, max_links, max_urls, max_searches):\n",
    "        \"\"\"Interface wrapper for different search types\"\"\"\n",
    "        if not query.strip():\n",
    "            return \"❌ Please enter a search query\", \"\", \"\"\n",
    "        \n",
    "        try:\n",
    "            # Update global configuration temporarily\n",
    "            global MAX_LINKS_TO_EXTRACT, MAX_URLS_TO_SCRAPE, MAX_STRATEGIC_SEARCHES\n",
    "            original_links = MAX_LINKS_TO_EXTRACT\n",
    "            original_urls = MAX_URLS_TO_SCRAPE  \n",
    "            original_searches = MAX_STRATEGIC_SEARCHES\n",
    "            \n",
    "            MAX_LINKS_TO_EXTRACT = max_links\n",
    "            MAX_URLS_TO_SCRAPE = max_urls\n",
    "            MAX_STRATEGIC_SEARCHES = max_searches\n",
    "            \n",
    "            status = f\"🚀 Starting {search_type} search for: '{query}'\\n\"\n",
    "            status += f\"📊 Config: {max_links} links, {max_urls} URLs, {max_searches} searches\\n\\n\"\n",
    "            \n",
    "            if search_type == \"Standard Agentic Search\":\n",
    "                results = await run_research(query)\n",
    "                if results:\n",
    "                    report = results['report'].markdown_report\n",
    "                    evaluation = f\"\"\"\n",
    "## 📊 Evaluation Results\n",
    "- **GPT-4 Score**: {results['evaluation'].gpt_evaluation.overall_score}/10\n",
    "- **Claude Score**: {results['evaluation'].claude_evaluation.overall_score}/10  \n",
    "- **Consensus**: {results['evaluation'].consensus_score}/10\n",
    "- **Quality**: {'✅ Approved' if results['quality_approved'] else '❌ Needs Improvement'}\n",
    "- **File**: {results['filepath']}\n",
    "\"\"\"\n",
    "                    status += \"✅ Standard agentic search completed successfully!\"\n",
    "                    \n",
    "            elif search_type == \"Combined Explorer + Agentic\":\n",
    "                results = await run_combined_research(query)\n",
    "                if results:\n",
    "                    report = results.agentic_results.markdown_report\n",
    "                    evaluation = f\"\"\"\n",
    "## 📊 Combined Search Results\n",
    "- **Explorer Sources**: {len([item for item in results.explorer_results if 'url' in item])}\n",
    "- **Search Engines**: DuckDuckGo, Brave\n",
    "- **Agentic Report**: Generated\n",
    "- **Combined Summary**: Available\n",
    "\"\"\"\n",
    "                    status += \"✅ Combined search completed successfully!\"\n",
    "                    \n",
    "            elif search_type == \"Explorer Only\":\n",
    "                explorer_results, explorer_metadata = perform_explorer_search(query, save_results=True)\n",
    "                \n",
    "                # Create detailed explorer report\n",
    "                report = f\"\"\"\n",
    "# 🔍 Explorer Search Results for: {query}\n",
    "\n",
    "## 📊 Search Summary\n",
    "- **Total Links Found**: {explorer_metadata['metadata']['total_links_found']}\n",
    "- **Pages Scraped**: {explorer_metadata['metadata']['total_scraped']}\n",
    "- **Successful Scrapes**: {len([r for r in explorer_results if not r.get('error')])}\n",
    "- **Search Engines**: {', '.join([engine['name'] for engine in explorer_metadata['metadata']['search_engines']])}\n",
    "\n",
    "## 🌐 Search Engine Details\n",
    "\"\"\"\n",
    "                \n",
    "                for engine in explorer_metadata['metadata']['search_engines']:\n",
    "                    report += f\"\"\"\n",
    "### {engine['name']} Results\n",
    "- **Links Found**: {engine['links_found']}\n",
    "- **Links Scraped**: {engine['links_scraped']}\n",
    "- **Successful Scrapes**: {engine['successful_scrapes']}\n",
    "\n",
    "**All {engine['name']} Links:**\n",
    "\"\"\"\n",
    "                    for i, link in enumerate(engine['all_links'][:10], 1):  # Show first 10 links\n",
    "                        report += f\"{i}. {link}\\n\"\n",
    "                    if len(engine['all_links']) > 10:\n",
    "                        report += f\"... and {len(engine['all_links']) - 10} more links\\n\"\n",
    "                \n",
    "                report += \"\\n## 📄 Scraped Content Preview\\n\"\n",
    "                \n",
    "                for i, result in enumerate([r for r in explorer_results if not r.get('error')][:5], 1):\n",
    "                    report += f\"\"\"\n",
    "### Source {i}: {result.get('source_engine', 'Unknown')}\n",
    "**URL**: {result.get('url', 'N/A')}\n",
    "**Text Length**: {result.get('full_text_length', 0):,} characters\n",
    "**Processing Time**: {result.get('processing_time_seconds', 0)} seconds\n",
    "\n",
    "**Content Preview**:\n",
    "{result.get('text', 'No content')[:500]}...\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "                \n",
    "                evaluation = f\"\"\"\n",
    "## 📊 Explorer Results Analysis\n",
    "- **Total Sources**: {len(explorer_results)}\n",
    "- **Successful Scrapes**: {len([r for r in explorer_results if not r.get('error')])}\n",
    "- **Failed Scrapes**: {len([r for r in explorer_results if r.get('error')])}\n",
    "- **Total Text Extracted**: {sum(r.get('full_text_length', 0) for r in explorer_results if not r.get('error')):,} characters\n",
    "- **Average Processing Time**: {sum(r.get('processing_time_seconds', 0) for r in explorer_results) / len(explorer_results):.2f} seconds\n",
    "- **Search Engines**: DuckDuckGo, Brave\n",
    "- **Saved to**: {explorer_metadata['summary']['completed_at']}\n",
    "\n",
    "### 🔗 Link Success Rate by Engine\n",
    "\"\"\"\n",
    "                for engine in explorer_metadata['metadata']['search_engines']:\n",
    "                    success_rate = (engine['successful_scrapes'] / engine['links_scraped'] * 100) if engine['links_scraped'] > 0 else 0\n",
    "                    evaluation += f\"- **{engine['name']}**: {success_rate:.1f}% ({engine['successful_scrapes']}/{engine['links_scraped']})\\n\"\n",
    "                \n",
    "                status += \"✅ Explorer search completed successfully!\"\n",
    "            \n",
    "            # Restore original configuration\n",
    "            MAX_LINKS_TO_EXTRACT = original_links\n",
    "            MAX_URLS_TO_SCRAPE = original_urls\n",
    "            MAX_STRATEGIC_SEARCHES = original_searches\n",
    "            \n",
    "            return status, report, evaluation\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Restore original configuration on error\n",
    "            MAX_LINKS_TO_EXTRACT = original_links\n",
    "            MAX_URLS_TO_SCRAPE = original_urls\n",
    "            MAX_STRATEGIC_SEARCHES = original_searches\n",
    "            \n",
    "            error_msg = f\"❌ Error during {search_type}: {str(e)}\"\n",
    "            return error_msg, \"\", \"\"\n",
    "    \n",
    "    async def evaluate_file_interface(filepath, query):\n",
    "        \"\"\"Interface wrapper for file evaluation\"\"\"\n",
    "        if not filepath.strip():\n",
    "            return \"❌ Please provide a filepath\"\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(filepath):\n",
    "                return f\"❌ File not found: {filepath}\"\n",
    "            \n",
    "            # Check if it's an explorer results file\n",
    "            explorer_data = load_explorer_results(filepath)\n",
    "            if explorer_data:\n",
    "                # It's an explorer file - provide explorer analysis\n",
    "                if not query.strip():\n",
    "                    # Try to extract query from file\n",
    "                    query = explorer_data.get(\"metadata\", {}).get(\"query\") or explorer_data.get(\"summary\", {}).get(\"query\", \"\")\n",
    "                \n",
    "                return display_explorer_json_analysis(filepath, query)\n",
    "            else:\n",
    "                # It's a regular research file - use standard evaluation\n",
    "                if not query.strip():\n",
    "                    return \"❌ Please provide the original query for evaluation\"\n",
    "                \n",
    "                evaluation = await universal_json_evaluator(filepath, query)\n",
    "                return evaluation.evaluation_summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"❌ Evaluation error: {str(e)}\"\n",
    "    \n",
    "    def get_recent_files():\n",
    "        \"\"\"Get list of recent result files\"\"\"\n",
    "        try:\n",
    "            files = []\n",
    "            for filename in os.listdir(DATA_DIR):\n",
    "                if filename.endswith('.json'):\n",
    "                    filepath = os.path.join(DATA_DIR, filename)\n",
    "                    files.append(filepath)\n",
    "            return sorted(files, key=os.path.getmtime, reverse=True)[:10]\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    # Create Gradio interface\n",
    "    with gr.Blocks(css=custom_css, title=\"🔍 Advanced Research System\") as interface:\n",
    "        \n",
    "        gr.HTML(\"\"\"\n",
    "        <div class=\"search-header\">\n",
    "            <h1>🔍 Advanced AI Research System</h1>\n",
    "            <p>Powered by Multi-Engine Search + AI Analysis + Dual-Model Evaluation</p>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Tabs():\n",
    "            \n",
    "            # Main Search Tab\n",
    "            with gr.Tab(\"🚀 Research Search\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        query_input = gr.Textbox(\n",
    "                            label=\"🔍 Research Query\",\n",
    "                            placeholder=\"Enter your research question...\",\n",
    "                            lines=2\n",
    "                        )\n",
    "                        \n",
    "                        search_type = gr.Radio(\n",
    "                            choices=[\n",
    "                                \"Standard Agentic Search\",\n",
    "                                \"Combined Explorer + Agentic\", \n",
    "                                \"Explorer Only\"\n",
    "                            ],\n",
    "                            value=\"Standard Agentic Search\",\n",
    "                            label=\"Search Type\"\n",
    "                        )\n",
    "                        \n",
    "                    with gr.Column(scale=1):\n",
    "                        gr.HTML('<div class=\"config-section\">')\n",
    "                        gr.HTML(\"<h4>⚙️ Configuration</h4>\")\n",
    "                        \n",
    "                        max_links = gr.Slider(\n",
    "                            minimum=5, maximum=100, value=MAX_LINKS_TO_EXTRACT,\n",
    "                            label=\"Max Links to Extract\"\n",
    "                        )\n",
    "                        max_urls = gr.Slider(\n",
    "                            minimum=1, maximum=20, value=MAX_URLS_TO_SCRAPE,\n",
    "                            label=\"Max URLs to Scrape\" \n",
    "                        )\n",
    "                        max_searches = gr.Slider(\n",
    "                            minimum=1, maximum=10, value=MAX_STRATEGIC_SEARCHES,\n",
    "                            label=\"Strategic Searches\"\n",
    "                        )\n",
    "                        gr.HTML('</div>')\n",
    "                \n",
    "                search_btn = gr.Button(\"🚀 Start Research\", variant=\"primary\", size=\"lg\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        status_output = gr.Textbox(\n",
    "                            label=\"📊 Status & Progress\",\n",
    "                            lines=4,\n",
    "                            interactive=False\n",
    "                        )\n",
    "                    \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        report_output = gr.Markdown(\n",
    "                            label=\"📄 Research Report\",\n",
    "                            height=10000\n",
    "                        )\n",
    "                    with gr.Column():\n",
    "                        evaluation_output = gr.Markdown(\n",
    "                            label=\"📊 Evaluation & Metrics\",\n",
    "                            height=40000\n",
    "                        )\n",
    "                \n",
    "                search_btn.click(\n",
    "                    fn=run_search_interface,\n",
    "                    inputs=[query_input, search_type, max_links, max_urls, max_searches],\n",
    "                    outputs=[status_output, report_output, evaluation_output]\n",
    "                )\n",
    "            \n",
    "            # File Evaluation Tab\n",
    "            with gr.Tab(\"📊 Evaluate Results\"):\n",
    "                gr.HTML(\"<h3>📊 Analyze Research Files</h3>\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        file_input = gr.Textbox(\n",
    "                            label=\"📁 File Path\",\n",
    "                            placeholder=\"Enter path to JSON results file...\",\n",
    "                            lines=1\n",
    "                        )\n",
    "                        eval_query_input = gr.Textbox(\n",
    "                            label=\"🔍 Original Query (Optional for Explorer files)\",\n",
    "                            placeholder=\"Enter the original research query (auto-detected for Explorer files)...\",\n",
    "                            lines=2\n",
    "                        )\n",
    "                        \n",
    "                        recent_files = gr.Dropdown(\n",
    "                            choices=get_recent_files(),\n",
    "                            label=\"📂 Recent Files\",\n",
    "                            interactive=True\n",
    "                        )\n",
    "                        \n",
    "                        def update_file_path(selected_file):\n",
    "                            return selected_file if selected_file else \"\"\n",
    "                        \n",
    "                        recent_files.change(\n",
    "                            fn=update_file_path,\n",
    "                            inputs=[recent_files],\n",
    "                            outputs=[file_input]\n",
    "                        )\n",
    "                \n",
    "                eval_btn = gr.Button(\"📊 Analyze File\", variant=\"secondary\")\n",
    "                \n",
    "                gr.HTML(\"\"\"\n",
    "                <div style=\"background: #f0f9ff; padding: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "                    <h4>📄 Supported File Types</h4>\n",
    "                    <ul>\n",
    "                        <li><strong>Explorer Results</strong>: <code>explorer_search_*.json</code> - Shows link analysis, scraping stats, content breakdown</li>\n",
    "                        <li><strong>Research Reports</strong>: <code>evaluation_*.json</code> - Provides AI-powered quality evaluation</li>\n",
    "                        <li><strong>Combined Results</strong>: <code>combined_search_*.json</code> - Analyzes both explorer and agentic components</li>\n",
    "                    </ul>\n",
    "                </div>\n",
    "                \"\"\")\n",
    "                \n",
    "                evaluation_result = gr.Markdown(\n",
    "                    label=\"📊 Analysis Report\",\n",
    "                    height=5000\n",
    "                )\n",
    "                \n",
    "                eval_btn.click(\n",
    "                    fn=evaluate_file_interface,\n",
    "                    inputs=[file_input, eval_query_input],\n",
    "                    outputs=[evaluation_result]\n",
    "                )\n",
    "            \n",
    "            # Configuration Tab\n",
    "            with gr.Tab(\"⚙️ System Config\"):\n",
    "                gr.HTML(\"<h3>⚙️ Current System Configuration</h3>\")\n",
    "                \n",
    "                # API Status Section\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.HTML(\"<h4>🔌 API Status</h4>\")\n",
    "                        api_status_btn = gr.Button(\"🔍 Check API Availability\", variant=\"secondary\")\n",
    "                        api_status_output = gr.Markdown(\"Click button to check API status...\")\n",
    "                        \n",
    "                        async def check_api_status():\n",
    "                            status = await test_api_availability()\n",
    "                            \n",
    "                            status_text = f\"\"\"\n",
    "## 🔌 API Availability Check\n",
    "**Timestamp**: {status['timestamp']}\n",
    "\n",
    "### Service Status\n",
    "- **GPT-4**: {'✅ Available' if status['gpt4_available'] else '❌ Unavailable'}\n",
    "- **Claude**: {'✅ Available' if status['claude_available'] else '❌ Unavailable'}\n",
    "\n",
    "### Recommendations\n",
    "\"\"\"\n",
    "                            if status['gpt4_available'] and status['claude_available']:\n",
    "                                status_text += \"🎉 **All systems operational** - Full evaluation available\\n\"\n",
    "                            elif status['gpt4_available'] or status['claude_available']:\n",
    "                                status_text += \"⚠️ **Partial availability** - Fallback evaluation will be used\\n\"\n",
    "                            else:\n",
    "                                status_text += \"🚫 **Limited availability** - Fallback evaluations will be used for both models\\n\"\n",
    "                            \n",
    "                            if not status['claude_available']:\n",
    "                                status_text += \"\\n### Claude Troubleshooting\\n\"\n",
    "                                status_text += \"- Try again in 2-5 minutes\\n- System will use fallback scoring\\n- GPT-4 evaluation still available\\n\"\n",
    "                            \n",
    "                            if not status['gpt4_available']:\n",
    "                                status_text += \"\\n### GPT-4 Troubleshooting\\n\" \n",
    "                                status_text += \"- Check API key and billing\\n- Verify rate limits\\n- Claude evaluation still available\\n\"\n",
    "                            \n",
    "                            return status_text\n",
    "                        \n",
    "                        api_status_btn.click(\n",
    "                            fn=check_api_status,\n",
    "                            outputs=[api_status_output]\n",
    "                        )\n",
    "                \n",
    "                config_info = f\"\"\"\n",
    "## 📊 Current Settings\n",
    "\n",
    "### Search Configuration\n",
    "- **Max Links to Extract**: {MAX_LINKS_TO_EXTRACT}\n",
    "- **Max URLs to Scrape**: {MAX_URLS_TO_SCRAPE}\n",
    "- **Max Search Results**: {MAX_SEARCH_RESULTS}\n",
    "- **Strategic Searches**: {MAX_STRATEGIC_SEARCHES}\n",
    "- **Search Context Size**: {SEARCH_CONTEXT_SIZE}\n",
    "\n",
    "### Performance Settings  \n",
    "- **Scraping Timeout**: {SCRAPING_TIMEOUT}s\n",
    "- **Max Text Length**: {MAX_TEXT_LENGTH} chars\n",
    "- **Report Min Length**: {REPORT_MIN_LENGTH} words\n",
    "\n",
    "### 🔧 How to Modify Configuration\n",
    "1. Edit variables in **Cell 2** of the notebook\n",
    "2. Restart kernel and rerun all cells\n",
    "3. Or use the sliders in the Research Search tab for temporary changes\n",
    "\n",
    "### 🔧 API Error Handling\n",
    "- **Automatic Retries**: 3 attempts with exponential backoff\n",
    "- **Fallback Evaluations**: Heuristic scoring when APIs unavailable\n",
    "- **Graceful Degradation**: Partial evaluation when one API fails\n",
    "- **Status Monitoring**: Check API availability above\n",
    "\"\"\"\n",
    "                \n",
    "                gr.Markdown(config_info)\n",
    "                \n",
    "                refresh_btn = gr.Button(\"🔄 Refresh File List\")\n",
    "                refresh_btn.click(\n",
    "                    fn=lambda: gr.Dropdown.update(choices=get_recent_files()),\n",
    "                    outputs=[recent_files]\n",
    "                )\n",
    "            \n",
    "            # Help Tab\n",
    "            with gr.Tab(\"❓ Help\"):\n",
    "                help_content = \"\"\"\n",
    "# 🔍 Advanced Research System - User Guide\n",
    "\n",
    "## 🚀 Quick Start\n",
    "1. **Enter your research query** in the search box\n",
    "2. **Select search type**:\n",
    "   - **Standard Agentic**: AI-powered analysis with evaluation\n",
    "   - **Combined**: Web scraping + AI analysis  \n",
    "   - **Explorer Only**: Raw web scraping\n",
    "3. **Adjust configuration** sliders if needed\n",
    "4. **Click \"Start Research\"** and wait for results\n",
    "\n",
    "## 📊 Search Types Explained\n",
    "\n",
    "### Standard Agentic Search\n",
    "- Uses AI agents to plan strategic searches\n",
    "- Performs web research with built-in tools\n",
    "- Generates comprehensive reports\n",
    "- Includes dual-model evaluation (GPT-4 + Claude)\n",
    "- **Best for**: Complex analysis, professional reports\n",
    "\n",
    "### Combined Explorer + Agentic  \n",
    "- Scrapes web pages directly (DuckDuckGo + Brave)\n",
    "- Combines with AI analysis\n",
    "- Provides both raw data and insights\n",
    "- **Best for**: Comprehensive coverage, fact-checking\n",
    "\n",
    "### Explorer Only\n",
    "- Direct web scraping without AI processing\n",
    "- Raw content from multiple search engines\n",
    "- **Enhanced features**: Detailed link tracking, comprehensive JSON saving, performance metrics\n",
    "- **Best for**: Quick data gathering, source verification, link analysis\n",
    "\n",
    "## 📄 File Analysis Features\n",
    "\n",
    "### Automatic File Type Detection\n",
    "The system automatically detects and analyzes different file types:\n",
    "\n",
    "#### 🔍 Explorer Results Files\n",
    "- **Format**: `explorer_search_YYYYMMDD_HHMMSS.json`\n",
    "- **Analysis**: Link success rates, scraping performance, content breakdown\n",
    "- **Auto-detection**: Query extracted from metadata\n",
    "- **Metrics**: Processing times, content lengths, error analysis\n",
    "\n",
    "#### 📊 Research Report Files  \n",
    "- **Format**: `evaluation_YYYYMMDD_HHMMSS.json`\n",
    "- **Analysis**: AI-powered quality evaluation using GPT-4 + Claude\n",
    "- **Requires**: Original query for proper evaluation\n",
    "\n",
    "#### 🔄 Combined Search Files\n",
    "- **Format**: `combined_search_YYYYMMDD_HHMMSS.json`  \n",
    "- **Analysis**: Both explorer and agentic components\n",
    "- **Features**: Comprehensive analysis of all data sources\n",
    "\n",
    "## ⚙️ Configuration Tips\n",
    "\n",
    "### Performance Settings\n",
    "- **Max Links**: Higher = more comprehensive, slower\n",
    "- **Max URLs**: Higher = more content, longer processing\n",
    "- **Strategic Searches**: Higher = deeper analysis\n",
    "\n",
    "### Quality vs Speed\n",
    "- **Fast**: 5 links, 2 URLs, 2 searches\n",
    "- **Balanced**: 20 links, 5 URLs, 3 searches  \n",
    "- **Comprehensive**: 50 links, 10 URLs, 7 searches\n",
    "\n",
    "## 🔗 Enhanced Explorer Features\n",
    "\n",
    "### Detailed Link Tracking\n",
    "- **Real-time display** of all discovered links\n",
    "- **Source attribution** (DuckDuckGo, Brave, etc.)\n",
    "- **Link success rates** by search engine\n",
    "- **Processing time tracking** per URL\n",
    "\n",
    "### Comprehensive JSON Saving\n",
    "- **Metadata preservation**: Search configuration, timestamps, engine details\n",
    "- **Full content storage**: Original text length + truncated versions\n",
    "- **Error tracking**: Detailed error messages and failure analysis\n",
    "- **Performance metrics**: Processing times, success rates, content statistics\n",
    "\n",
    "### Progress Monitoring\n",
    "- **Live scraping progress** with URL display\n",
    "- **Success/failure indicators** in real-time\n",
    "- **Character count tracking** as pages are processed\n",
    "- **Engine-by-engine breakdown** of results\n",
    "\n",
    "### Advanced Analysis\n",
    "```python\n",
    "# Explorer-only research with full JSON\n",
    "results = await run_explorer_only_research(\"your query\")\n",
    "\n",
    "# Load and analyze existing explorer files\n",
    "analysis = display_explorer_json_analysis(\"explorer_search_file.json\")\n",
    "```\n",
    "\n",
    "## 📁 File Management\n",
    "- Results automatically saved to `workspace/data/`\n",
    "- **Explorer files**: `explorer_search_YYYYMMDD_HHMMSS.json`\n",
    "- **Combined files**: `combined_search_YYYYMMDD_HHMMSS.json`\n",
    "- **Evaluation files**: `evaluation_YYYYMMDD_HHMMSS.json`\n",
    "- Use \"Analyze Results\" tab for any file type\n",
    "\n",
    "## 🔧 API Error Handling\n",
    "\n",
    "### Common Issues & Solutions\n",
    "\n",
    "#### Claude API Overload (Error 529)\n",
    "**Symptoms**: \"Overloaded\" error message\n",
    "**Solutions**:\n",
    "- ⏳ Wait 2-5 minutes and retry\n",
    "- 🔄 System automatically uses fallback evaluation\n",
    "- ✅ GPT-4 evaluation still works normally\n",
    "\n",
    "#### GPT-4 API Issues  \n",
    "**Symptoms**: Authentication or rate limit errors\n",
    "**Solutions**:\n",
    "- 🔑 Check OpenAI API key in environment\n",
    "- 💳 Verify account billing and credits\n",
    "- ⏱️ Wait between requests to avoid rate limits\n",
    "\n",
    "### Fallback Evaluations\n",
    "When APIs are unavailable, the system uses:\n",
    "- **Heuristic scoring** based on content analysis\n",
    "- **Conservative estimates** for accuracy and quality\n",
    "- **Clear labeling** of fallback vs. API evaluations\n",
    "- **Medium confidence** ratings\n",
    "\n",
    "### 💡 Best Practices\n",
    "1. **Check API status** in System Config tab\n",
    "2. **Use off-peak hours** for better availability\n",
    "3. **Space out evaluations** to avoid rate limits\n",
    "4. **Re-evaluate later** when APIs recover\n",
    "5. **Monitor fallback indicators** in results\n",
    "\n",
    "## ❓ Troubleshooting\n",
    "- **Slow performance**: Reduce max links/URLs\n",
    "- **Timeout errors**: Increase scraping timeout in config\n",
    "- **Empty results**: Try different search terms\n",
    "- **Evaluation errors**: Check file path and query format\n",
    "- **API issues**: Use System Config tab to check status\n",
    "\n",
    "## 🆘 Emergency Mode\n",
    "If both APIs fail:\n",
    "1. System continues with fallback evaluations\n",
    "2. Focus on the research content quality\n",
    "3. Manually verify key facts from sources\n",
    "4. Re-evaluate when APIs recover\n",
    "\n",
    "\"\"\"\n",
    "                gr.Markdown(help_content)\n",
    "\n",
    "        \n",
    "        gr.HTML(\"\"\"\n",
    "        <div style=\"text-align: center; margin-top: 20px; color: #6b7280;\">\n",
    "            <p>🔍 Advanced Research System | Multi-Engine Search + AI Analysis + Dual Evaluation</p>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Create and launch interface\n",
    "def launch_gradio_interface(share=False, debug=False):\n",
    "    \"\"\"Launch the Gradio interface\"\"\"\n",
    "    print(\"🚀 Launching Gradio interface...\")\n",
    "    print(f\"📊 Current config: {MAX_LINKS_TO_EXTRACT} links, {MAX_URLS_TO_SCRAPE} URLs, {MAX_STRATEGIC_SEARCHES} searches\")\n",
    "    \n",
    "    interface = create_gradio_interface()\n",
    "    \n",
    "    # Launch interface\n",
    "    interface.launch(\n",
    "        share=share,\n",
    "        debug=debug,\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7860,\n",
    "        show_error=True,\n",
    "        quiet=False\n",
    "    )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "print(\"✅ Gradio interface ready!\")\n",
    "print(\"💡 Usage:\")\n",
    "print(\"   - interface = launch_gradio_interface()              # Launch locally\")\n",
    "print(\"   - interface = launch_gradio_interface(share=True)    # Create public link\")\n",
    "print(\"   - interface = launch_gradio_interface(debug=True)    # Enable debug mode\")\n",
    "\n",
    "# ============================================\n",
    "# AUTO-LAUNCH GRADIO INTERFACE (UPDATED)\n",
    "# ============================================\n",
    "\n",
    "# Auto-launch settings\n",
    "AUTO_LAUNCH = True           # Set to False to disable auto-launch\n",
    "SHARE_PUBLICLY = False       # Set to True to create public shareable link\n",
    "DEBUG_MODE = False           # Set to True for debugging\n",
    "PORT = 7860                  # Port number for the interface\n",
    "\n",
    "if AUTO_LAUNCH:\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"🚀 AUTO-LAUNCHING GRADIO INTERFACE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"📊 Configuration: {MAX_LINKS_TO_EXTRACT} links, {MAX_URLS_TO_SCRAPE} URLs, {MAX_STRATEGIC_SEARCHES} searches\")\n",
    "        print(f\"🌐 Share publicly: {'Yes' if SHARE_PUBLICLY else 'No'}\")\n",
    "        print(f\"🔧 Debug mode: {'Enabled' if DEBUG_MODE else 'Disabled'}\")\n",
    "        print(f\"🔌 Port: {PORT}\")\n",
    "        \n",
    "        # Create and launch interface automatically\n",
    "        interface = create_gradio_interface()\n",
    "        \n",
    "        print(\"🎯 Starting Gradio server...\")\n",
    "        interface.launch(\n",
    "            share=SHARE_PUBLICLY,\n",
    "            debug=DEBUG_MODE,\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=PORT,\n",
    "            show_error=True,\n",
    "            quiet=False,\n",
    "            inbrowser=True,          # Automatically open in browser\n",
    "            prevent_thread_lock=False  # Allow notebook to continue\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Gradio interface launched successfully!\")\n",
    "        print(f\"🌐 Access your interface at: http://localhost:{PORT}\")\n",
    "        if SHARE_PUBLICLY:\n",
    "            print(\"🔗 Public link will be displayed above\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to auto-launch Gradio interface: {str(e)}\")\n",
    "        print(\"💡 You can manually launch with: launch_gradio_interface()\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n⏸️ Auto-launch disabled\")\n",
    "    print(\"💡 To launch manually: interface = launch_gradio_interface()\")\n",
    "    print(\"🔧 To enable auto-launch: Set AUTO_LAUNCH = True in this cell\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 ADVANCED RESEARCH SYSTEM READY!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"📚 Complete system with:\")\n",
    "print(\"   ✅ Multi-engine web scraping (DuckDuckGo + Brave)\")\n",
    "print(\"   ✅ AI-powered agentic search\")\n",
    "print(\"   ✅ Dual-model evaluation (GPT-4 + Claude)\")\n",
    "print(\"   ✅ Beautiful Gradio web interface\")\n",
    "print(\"   ✅ Fully configurable parameters\")\n",
    "print(\"   ✅ Robust error handling with fallbacks\")\n",
    "print(\"\\n🔥 Enhanced Explorer Features:\")\n",
    "print(\"   ✅ Real-time link discovery and display\")\n",
    "print(\"   ✅ Detailed scraping progress with statistics\")\n",
    "print(\"   ✅ Comprehensive JSON saving with metadata\")\n",
    "print(\"   ✅ Performance metrics and error tracking\")\n",
    "print(\"   ✅ Source-by-source analysis and breakdown\")\n",
    "print(\"   ✅ Automatic file type detection in analysis\")\n",
    "print(\"\\n🚀 Ready for advanced research tasks!\")\n",
    "print(\"📊 Explorer JSON data provides complete transparency!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
